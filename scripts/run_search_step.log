W0222 11:55:07.126639 540888 site-packages/torch/distributed/run.py:793] 
W0222 11:55:07.126639 540888 site-packages/torch/distributed/run.py:793] *****************************************
W0222 11:55:07.126639 540888 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 11:55:07.126639 540888 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank0]:[W222 11:55:30.762855006 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
[rank3]:[W222 11:55:33.286939859 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 11:55:33.290016958 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 11:55:33.295203262 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.43s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.46s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.47s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.81it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.28it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.19it/s]
Max step: 1, bias_value: 0.2
Max step: 1, bias_value: 0.2
Max step: 1, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 1, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [00:52<03:28, 52.16s/it] 20%|██        | 1/5 [00:55<03:40, 55.01s/it] 20%|██        | 1/5 [00:56<03:46, 56.67s/it] 20%|██        | 1/5 [00:56<03:46, 56.54s/it] 40%|████      | 2/5 [01:43<02:34, 51.44s/it] 40%|████      | 2/5 [01:45<02:37, 52.53s/it] 40%|████      | 2/5 [01:47<02:39, 53.25s/it] 40%|████      | 2/5 [01:47<02:39, 53.33s/it] 60%|██████    | 3/5 [02:36<01:45, 52.52s/it] 60%|██████    | 3/5 [02:40<01:46, 53.04s/it] 60%|██████    | 3/5 [02:40<01:46, 53.05s/it] 60%|██████    | 3/5 [02:40<01:47, 53.51s/it] 80%|████████  | 4/5 [03:31<00:53, 53.40s/it] 80%|████████  | 4/5 [03:36<00:54, 54.23s/it] 80%|████████  | 4/5 [03:37<00:54, 54.80s/it] 80%|████████  | 4/5 [03:39<00:55, 55.39s/it]100%|██████████| 5/5 [04:22<00:00, 52.45s/it]100%|██████████| 5/5 [04:22<00:00, 52.49s/it]
100%|██████████| 5/5 [04:30<00:00, 54.19s/it]100%|██████████| 5/5 [04:30<00:00, 54.10s/it]
100%|██████████| 5/5 [04:31<00:00, 54.65s/it]100%|██████████| 5/5 [04:31<00:00, 54.33s/it]
100%|██████████| 5/5 [04:32<00:00, 54.45s/it]100%|██████████| 5/5 [04:32<00:00, 54.42s/it]
[rank0]:[W222 12:01:44.144120409 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 12:01:51.113239 559619 site-packages/torch/distributed/run.py:793] 
W0222 12:01:51.113239 559619 site-packages/torch/distributed/run.py:793] *****************************************
W0222 12:01:51.113239 559619 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 12:01:51.113239 559619 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank2]:[W222 12:02:17.640692895 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 12:02:17.644066931 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 12:02:17.647277544 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 12:02:48.575249537 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.39s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.31s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_1/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_1/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_1/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_1/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:07<02:17,  7.25s/it]  5%|▌         | 1/20 [00:08<02:37,  8.31s/it]  5%|▌         | 1/20 [00:08<02:40,  8.45s/it]  5%|▌         | 1/20 [00:08<02:41,  8.49s/it] 10%|█         | 2/20 [00:12<01:50,  6.15s/it] 10%|█         | 2/20 [00:13<01:58,  6.57s/it] 10%|█         | 2/20 [00:13<01:58,  6.56s/it] 10%|█         | 2/20 [00:13<02:00,  6.67s/it] 15%|█▌        | 3/20 [00:17<01:32,  5.45s/it] 15%|█▌        | 3/20 [00:18<01:34,  5.59s/it] 15%|█▌        | 3/20 [00:18<01:34,  5.58s/it] 15%|█▌        | 3/20 [00:18<01:37,  5.72s/it] 20%|██        | 4/20 [00:22<01:27,  5.49s/it] 20%|██        | 4/20 [00:23<01:27,  5.47s/it] 20%|██        | 4/20 [00:23<01:30,  5.67s/it] 20%|██        | 4/20 [00:24<01:30,  5.67s/it] 25%|██▌       | 5/20 [00:27<01:20,  5.35s/it] 25%|██▌       | 5/20 [00:28<01:19,  5.32s/it] 25%|██▌       | 5/20 [00:28<01:21,  5.45s/it] 25%|██▌       | 5/20 [00:29<01:23,  5.58s/it] 30%|███       | 6/20 [00:33<01:15,  5.42s/it] 30%|███       | 6/20 [00:34<01:15,  5.40s/it] 30%|███       | 6/20 [00:34<01:17,  5.53s/it] 30%|███       | 6/20 [00:35<01:19,  5.68s/it] 35%|███▌      | 7/20 [00:38<01:09,  5.31s/it] 35%|███▌      | 7/20 [00:38<01:08,  5.25s/it] 35%|███▌      | 7/20 [00:39<01:10,  5.43s/it] 35%|███▌      | 7/20 [00:40<01:11,  5.53s/it] 40%|████      | 8/20 [00:43<01:03,  5.32s/it] 40%|████      | 8/20 [00:44<01:03,  5.28s/it] 40%|████      | 8/20 [00:45<01:05,  5.49s/it] 40%|████      | 8/20 [00:46<01:06,  5.50s/it] 45%|████▌     | 9/20 [00:48<00:57,  5.19s/it] 45%|████▌     | 9/20 [00:49<00:56,  5.16s/it] 45%|████▌     | 9/20 [00:50<00:58,  5.35s/it] 45%|████▌     | 9/20 [00:51<00:59,  5.40s/it] 50%|█████     | 10/20 [00:54<00:52,  5.20s/it] 50%|█████     | 10/20 [00:54<00:52,  5.22s/it] 50%|█████     | 10/20 [00:56<00:54,  5.45s/it] 50%|█████     | 10/20 [00:56<00:54,  5.48s/it] 55%|█████▌    | 11/20 [01:00<00:50,  5.59s/it] 55%|█████▌    | 11/20 [01:00<00:49,  5.54s/it] 55%|█████▌    | 11/20 [01:02<00:51,  5.77s/it] 55%|█████▌    | 11/20 [01:03<00:51,  5.76s/it] 60%|██████    | 12/20 [01:07<00:47,  5.96s/it] 60%|██████    | 12/20 [01:07<00:47,  5.88s/it] 60%|██████    | 12/20 [01:09<00:48,  6.04s/it] 60%|██████    | 12/20 [01:10<00:48,  6.07s/it] 65%|██████▌   | 13/20 [01:13<00:40,  5.80s/it] 65%|██████▌   | 13/20 [01:13<00:41,  5.90s/it] 65%|██████▌   | 13/20 [01:14<00:41,  5.89s/it] 65%|██████▌   | 13/20 [01:15<00:41,  5.89s/it] 70%|███████   | 14/20 [01:18<00:34,  5.82s/it] 70%|███████   | 14/20 [01:18<00:34,  5.78s/it] 70%|███████   | 14/20 [01:20<00:34,  5.82s/it] 70%|███████   | 14/20 [01:21<00:35,  5.91s/it] 75%|███████▌  | 15/20 [01:26<00:31,  6.39s/it] 75%|███████▌  | 15/20 [01:26<00:31,  6.38s/it] 75%|███████▌  | 15/20 [01:28<00:31,  6.36s/it] 75%|███████▌  | 15/20 [01:29<00:32,  6.43s/it] 80%|████████  | 16/20 [01:36<00:29,  7.30s/it] 80%|████████  | 16/20 [01:36<00:29,  7.42s/it] 80%|████████  | 16/20 [01:37<00:28,  7.17s/it] 80%|████████  | 16/20 [01:38<00:28,  7.23s/it] 85%|████████▌ | 17/20 [01:41<00:19,  6.61s/it] 85%|████████▌ | 17/20 [01:41<00:19,  6.66s/it] 85%|████████▌ | 17/20 [01:42<00:19,  6.52s/it] 85%|████████▌ | 17/20 [01:43<00:19,  6.60s/it] 90%|█████████ | 18/20 [01:46<00:12,  6.36s/it] 90%|█████████ | 18/20 [01:47<00:12,  6.43s/it] 90%|█████████ | 18/20 [01:48<00:12,  6.30s/it] 90%|█████████ | 18/20 [01:49<00:12,  6.37s/it] 95%|█████████▌| 19/20 [01:51<00:05,  5.90s/it] 95%|█████████▌| 19/20 [01:51<00:05,  5.93s/it] 95%|█████████▌| 19/20 [01:53<00:05,  5.94s/it] 95%|█████████▌| 19/20 [01:54<00:05,  5.95s/it]100%|██████████| 20/20 [01:57<00:00,  5.98s/it]100%|██████████| 20/20 [01:57<00:00,  5.89s/it]
100%|██████████| 20/20 [01:58<00:00,  6.02s/it]100%|██████████| 20/20 [01:58<00:00,  5.90s/it]
100%|██████████| 20/20 [01:59<00:00,  6.13s/it]100%|██████████| 20/20 [01:59<00:00,  5.98s/it]
100%|██████████| 20/20 [02:00<00:00,  6.02s/it]100%|██████████| 20/20 [02:00<00:00,  6.01s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 12:06:25,648 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_1/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 12:06:25,649 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 12:06:25,649 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 12:06:25,649 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 12:08:40,943 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 12:08:40,943 - RUN - INFO - Evaluation Results:
2025-02-22 12:08:40,946 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.47
 1  0        cross   0.41
 2  0        single  0.53
 3  1        all     0.475
 4  1        cross   0.38
 5  1        single  0.57
 6  2        all     0.505
 7  2        cross   0.4
 8  2        single  0.61
 9  3        all     0.52
10  3        cross   0.38
11  3        single  0.66
12  Average  all     0.4925
13  Average  cross   0.3925
14  Average  single  0.5925
--  -------  ------  ------
[rank0]:[W222 12:08:41.753101889 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 12:08:47.421343 573209 site-packages/torch/distributed/run.py:793] 
W0222 12:08:47.421343 573209 site-packages/torch/distributed/run.py:793] *****************************************
W0222 12:08:47.421343 573209 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 12:08:47.421343 573209 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank0]:[W222 12:09:15.344942529 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 12:09:15.346364468 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 12:09:15.348477153 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 12:09:15.353502311 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.62s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.61s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.61s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it]
Max step: 3, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 3, bias_value: 0.2Max step: 3, bias_value: 0.2Max step: 3, bias_value: 0.2


You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [01:17<05:09, 77.38s/it] 20%|██        | 1/5 [01:20<05:20, 80.24s/it] 20%|██        | 1/5 [01:20<05:22, 80.56s/it] 20%|██        | 1/5 [01:22<05:29, 82.27s/it] 40%|████      | 2/5 [02:30<03:45, 75.06s/it] 40%|████      | 2/5 [02:33<03:48, 76.25s/it] 40%|████      | 2/5 [02:35<03:51, 77.20s/it] 40%|████      | 2/5 [02:36<03:52, 77.41s/it] 60%|██████    | 3/5 [03:46<02:31, 75.57s/it] 60%|██████    | 3/5 [03:49<02:32, 76.27s/it] 60%|██████    | 3/5 [03:51<02:33, 76.65s/it] 60%|██████    | 3/5 [03:54<02:35, 77.79s/it] 80%|████████  | 4/5 [05:02<01:15, 75.68s/it] 80%|████████  | 4/5 [05:07<01:16, 76.51s/it] 80%|████████  | 4/5 [05:10<01:17, 77.86s/it] 80%|████████  | 4/5 [05:13<01:18, 78.19s/it]100%|██████████| 5/5 [06:22<00:00, 77.22s/it]100%|██████████| 5/5 [06:22<00:00, 76.55s/it]
100%|██████████| 5/5 [06:23<00:00, 76.14s/it]100%|██████████| 5/5 [06:23<00:00, 76.64s/it]
100%|██████████| 5/5 [06:25<00:00, 76.18s/it]100%|██████████| 5/5 [06:25<00:00, 77.18s/it]
100%|██████████| 5/5 [06:26<00:00, 77.29s/it]100%|██████████| 5/5 [06:26<00:00, 77.31s/it]
[rank0]:[W222 12:17:17.446843856 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 12:17:24.113010 593704 site-packages/torch/distributed/run.py:793] 
W0222 12:17:24.113010 593704 site-packages/torch/distributed/run.py:793] *****************************************
W0222 12:17:24.113010 593704 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 12:17:24.113010 593704 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W222 12:17:50.113770059 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 12:17:50.114236108 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 12:17:50.117081387 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 12:18:21.761350698 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.43s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_3/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_3/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_3/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_3/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:09,  6.83s/it]  5%|▌         | 1/20 [00:07<02:28,  7.82s/it]  5%|▌         | 1/20 [00:07<02:27,  7.76s/it]  5%|▌         | 1/20 [00:07<02:27,  7.78s/it] 10%|█         | 2/20 [00:11<01:41,  5.63s/it] 10%|█         | 2/20 [00:12<01:47,  5.97s/it] 10%|█         | 2/20 [00:12<01:49,  6.06s/it] 10%|█         | 2/20 [00:12<01:47,  6.00s/it] 15%|█▌        | 3/20 [00:15<01:23,  4.90s/it] 15%|█▌        | 3/20 [00:16<01:25,  5.02s/it] 15%|█▌        | 3/20 [00:16<01:25,  5.06s/it] 15%|█▌        | 3/20 [00:16<01:28,  5.18s/it] 20%|██        | 4/20 [00:21<01:22,  5.16s/it] 20%|██        | 4/20 [00:21<01:23,  5.19s/it] 20%|██        | 4/20 [00:21<01:23,  5.22s/it] 20%|██        | 4/20 [00:22<01:27,  5.44s/it] 25%|██▌       | 5/20 [00:25<01:12,  4.84s/it] 25%|██▌       | 5/20 [00:25<01:12,  4.81s/it] 25%|██▌       | 5/20 [00:25<01:12,  4.81s/it] 25%|██▌       | 5/20 [00:26<01:15,  5.03s/it] 30%|███       | 6/20 [00:30<01:06,  4.76s/it] 30%|███       | 6/20 [00:30<01:05,  4.70s/it] 30%|███       | 6/20 [00:30<01:06,  4.78s/it] 30%|███       | 6/20 [00:31<01:09,  4.99s/it] 35%|███▌      | 7/20 [00:35<01:03,  4.86s/it] 35%|███▌      | 7/20 [00:35<01:01,  4.71s/it] 35%|███▌      | 7/20 [00:35<01:02,  4.84s/it] 35%|███▌      | 7/20 [00:36<01:04,  4.97s/it] 40%|████      | 8/20 [00:40<00:59,  4.98s/it] 40%|████      | 8/20 [00:40<00:58,  4.88s/it] 40%|████      | 8/20 [00:40<00:59,  4.94s/it] 40%|████      | 8/20 [00:41<01:00,  5.04s/it] 45%|████▌     | 9/20 [00:45<00:54,  4.95s/it] 45%|████▌     | 9/20 [00:45<00:53,  4.88s/it] 45%|████▌     | 9/20 [00:45<00:54,  4.92s/it] 45%|████▌     | 9/20 [00:46<00:55,  5.02s/it] 50%|█████     | 10/20 [00:50<00:49,  4.98s/it] 50%|█████     | 10/20 [00:50<00:49,  4.93s/it] 50%|█████     | 10/20 [00:50<00:49,  4.97s/it] 50%|█████     | 10/20 [00:52<00:51,  5.10s/it] 55%|█████▌    | 11/20 [00:56<00:48,  5.37s/it] 55%|█████▌    | 11/20 [00:56<00:49,  5.47s/it] 55%|█████▌    | 11/20 [00:57<00:48,  5.40s/it] 55%|█████▌    | 11/20 [00:58<00:49,  5.50s/it] 60%|██████    | 12/20 [01:04<00:48,  6.10s/it] 60%|██████    | 12/20 [01:05<00:50,  6.26s/it] 60%|██████    | 12/20 [01:05<00:50,  6.28s/it] 60%|██████    | 12/20 [01:06<00:50,  6.31s/it] 65%|██████▌   | 13/20 [01:09<00:40,  5.72s/it] 65%|██████▌   | 13/20 [01:10<00:41,  5.99s/it] 65%|██████▌   | 13/20 [01:10<00:41,  5.98s/it] 65%|██████▌   | 13/20 [01:11<00:41,  5.90s/it] 70%|███████   | 14/20 [01:15<00:34,  5.68s/it] 70%|███████   | 14/20 [01:16<00:35,  5.92s/it] 70%|███████   | 14/20 [01:16<00:35,  5.93s/it] 70%|███████   | 14/20 [01:17<00:35,  5.96s/it] 75%|███████▌  | 15/20 [01:22<00:30,  6.16s/it] 75%|███████▌  | 15/20 [01:23<00:31,  6.31s/it] 75%|███████▌  | 15/20 [01:23<00:31,  6.33s/it] 75%|███████▌  | 15/20 [01:25<00:31,  6.39s/it] 80%|████████  | 16/20 [01:28<00:24,  6.04s/it] 80%|████████  | 16/20 [01:29<00:24,  6.13s/it] 80%|████████  | 16/20 [01:29<00:24,  6.19s/it] 80%|████████  | 16/20 [01:31<00:25,  6.27s/it] 85%|████████▌ | 17/20 [01:33<00:16,  5.66s/it] 85%|████████▌ | 17/20 [01:33<00:17,  5.77s/it] 85%|████████▌ | 17/20 [01:34<00:17,  5.80s/it] 85%|████████▌ | 17/20 [01:36<00:17,  5.95s/it] 90%|█████████ | 18/20 [01:38<00:11,  5.58s/it] 90%|█████████ | 18/20 [01:39<00:11,  5.71s/it] 90%|█████████ | 18/20 [01:39<00:11,  5.77s/it] 90%|█████████ | 18/20 [01:41<00:11,  5.78s/it] 95%|█████████▌| 19/20 [01:43<00:05,  5.36s/it] 95%|█████████▌| 19/20 [01:44<00:05,  5.42s/it] 95%|█████████▌| 19/20 [01:44<00:05,  5.46s/it] 95%|█████████▌| 19/20 [01:46<00:05,  5.54s/it]100%|██████████| 20/20 [01:49<00:00,  5.51s/it]100%|██████████| 20/20 [01:49<00:00,  5.46s/it]
100%|██████████| 20/20 [01:50<00:00,  5.58s/it]100%|██████████| 20/20 [01:50<00:00,  5.51s/it]
100%|██████████| 20/20 [01:50<00:00,  5.63s/it]100%|██████████| 20/20 [01:50<00:00,  5.53s/it]
100%|██████████| 20/20 [01:53<00:00,  5.75s/it]100%|██████████| 20/20 [01:53<00:00,  5.65s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 12:21:47,712 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_3/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 12:21:47,712 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 12:21:47,712 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 12:21:47,712 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 12:24:09,242 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 12:24:09,242 - RUN - INFO - Evaluation Results:
2025-02-22 12:24:09,244 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.5
 1  0        cross   0.41
 2  0        single  0.59
 3  1        all     0.485
 4  1        cross   0.37
 5  1        single  0.6
 6  2        all     0.535
 7  2        cross   0.41
 8  2        single  0.66
 9  3        all     0.54
10  3        cross   0.38
11  3        single  0.7
12  Average  all     0.515
13  Average  cross   0.3925
14  Average  single  0.6375
--  -------  ------  ------
[rank0]:[W222 12:24:09.064921040 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 12:24:15.601766 605474 site-packages/torch/distributed/run.py:793] 
W0222 12:24:15.601766 605474 site-packages/torch/distributed/run.py:793] *****************************************
W0222 12:24:15.601766 605474 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 12:24:15.601766 605474 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank1]:[W222 12:24:42.561049397 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 12:24:42.562436169 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 12:24:42.562765416 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 12:24:42.578755874 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.01s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.26s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Max step: 5, bias_value: 0.2
Max step: 5, bias_value: 0.2
Max step: 5, bias_value: 0.2
Max step: 5, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [01:32<06:09, 92.29s/it] 20%|██        | 1/5 [01:36<06:26, 96.51s/it] 20%|██        | 1/5 [01:38<06:33, 98.33s/it] 20%|██        | 1/5 [01:39<06:39, 99.81s/it] 40%|████      | 2/5 [03:02<04:33, 91.29s/it] 40%|████      | 2/5 [03:11<04:47, 95.86s/it] 40%|████      | 2/5 [03:12<04:46, 95.62s/it] 40%|████      | 2/5 [03:13<04:49, 96.65s/it] 60%|██████    | 3/5 [04:39<03:07, 93.58s/it] 60%|██████    | 3/5 [04:47<03:11, 95.92s/it] 60%|██████    | 3/5 [04:53<03:15, 97.89s/it] 60%|██████    | 3/5 [04:56<03:18, 99.46s/it] 80%|████████  | 4/5 [06:14<01:34, 94.19s/it] 80%|████████  | 4/5 [06:25<01:36, 96.47s/it] 80%|████████  | 4/5 [06:33<01:39, 99.07s/it] 80%|████████  | 4/5 [06:36<01:39, 99.63s/it]100%|██████████| 5/5 [07:48<00:00, 94.34s/it]100%|██████████| 5/5 [07:48<00:00, 93.78s/it]
100%|██████████| 5/5 [08:01<00:00, 96.37s/it]100%|██████████| 5/5 [08:01<00:00, 96.28s/it]
100%|██████████| 5/5 [08:11<00:00, 97.88s/it]100%|██████████| 5/5 [08:11<00:00, 98.26s/it]
100%|██████████| 5/5 [08:12<00:00, 98.95s/it]100%|██████████| 5/5 [08:12<00:00, 98.54s/it]
[rank0]:[W222 12:34:32.635884997 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 12:34:38.983510 629599 site-packages/torch/distributed/run.py:793] 
W0222 12:34:38.983510 629599 site-packages/torch/distributed/run.py:793] *****************************************
W0222 12:34:38.983510 629599 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 12:34:38.983510 629599 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank2]:[W222 12:35:11.814157020 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 12:35:11.816520420 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 12:35:11.821818411 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 12:35:42.977623710 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.52s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.56s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.59s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_5/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_5/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_5/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_5/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:06,  6.67s/it]  5%|▌         | 1/20 [00:07<02:19,  7.35s/it]  5%|▌         | 1/20 [00:07<02:21,  7.46s/it]  5%|▌         | 1/20 [00:07<02:19,  7.36s/it] 10%|█         | 2/20 [00:10<01:30,  5.02s/it] 10%|█         | 2/20 [00:11<01:34,  5.23s/it] 10%|█         | 2/20 [00:11<01:34,  5.27s/it] 10%|█         | 2/20 [00:11<01:36,  5.34s/it] 15%|█▌        | 3/20 [00:13<01:09,  4.10s/it] 15%|█▌        | 3/20 [00:14<01:12,  4.27s/it] 15%|█▌        | 3/20 [00:14<01:13,  4.31s/it] 15%|█▌        | 3/20 [00:14<01:15,  4.47s/it] 20%|██        | 4/20 [00:18<01:09,  4.35s/it] 20%|██        | 4/20 [00:18<01:09,  4.36s/it] 20%|██        | 4/20 [00:18<01:10,  4.43s/it] 20%|██        | 4/20 [00:19<01:12,  4.52s/it] 25%|██▌       | 5/20 [00:22<01:03,  4.23s/it] 25%|██▌       | 5/20 [00:22<01:02,  4.14s/it] 25%|██▌       | 5/20 [00:22<01:02,  4.18s/it] 25%|██▌       | 5/20 [00:23<01:05,  4.34s/it] 30%|███       | 6/20 [00:26<00:58,  4.17s/it] 30%|███       | 6/20 [00:26<00:59,  4.27s/it] 30%|███       | 6/20 [00:26<00:58,  4.18s/it] 30%|███       | 6/20 [00:27<01:00,  4.32s/it] 35%|███▌      | 7/20 [00:30<00:54,  4.16s/it] 35%|███▌      | 7/20 [00:31<00:56,  4.35s/it] 35%|███▌      | 7/20 [00:31<00:55,  4.29s/it] 35%|███▌      | 7/20 [00:32<00:56,  4.35s/it] 40%|████      | 8/20 [00:35<00:51,  4.30s/it] 40%|████      | 8/20 [00:35<00:52,  4.39s/it] 40%|████      | 8/20 [00:35<00:51,  4.32s/it] 40%|████      | 8/20 [00:36<00:52,  4.37s/it] 45%|████▌     | 9/20 [00:39<00:44,  4.09s/it] 45%|████▌     | 9/20 [00:39<00:45,  4.12s/it] 45%|████▌     | 9/20 [00:39<00:46,  4.22s/it] 45%|████▌     | 9/20 [00:40<00:45,  4.18s/it] 50%|█████     | 10/20 [00:43<00:43,  4.32s/it] 50%|█████     | 10/20 [00:44<00:44,  4.45s/it] 50%|█████     | 10/20 [00:44<00:44,  4.48s/it] 50%|█████     | 10/20 [00:45<00:45,  4.51s/it] 55%|█████▌    | 11/20 [00:48<00:40,  4.51s/it] 55%|█████▌    | 11/20 [00:49<00:41,  4.64s/it] 55%|█████▌    | 11/20 [00:49<00:42,  4.67s/it] 55%|█████▌    | 11/20 [00:50<00:42,  4.69s/it] 60%|██████    | 12/20 [00:54<00:39,  4.89s/it] 60%|██████    | 12/20 [00:55<00:39,  4.97s/it] 60%|██████    | 12/20 [00:55<00:39,  4.98s/it] 60%|██████    | 12/20 [00:56<00:40,  5.06s/it] 65%|██████▌   | 13/20 [00:59<00:33,  4.77s/it] 65%|██████▌   | 13/20 [00:59<00:33,  4.78s/it] 65%|██████▌   | 13/20 [01:00<00:34,  4.92s/it] 65%|██████▌   | 13/20 [01:00<00:34,  4.87s/it] 70%|███████   | 14/20 [01:03<00:27,  4.65s/it] 70%|███████   | 14/20 [01:04<00:27,  4.59s/it] 70%|███████   | 14/20 [01:04<00:29,  4.91s/it] 70%|███████   | 14/20 [01:05<00:28,  4.77s/it] 75%|███████▌  | 15/20 [01:10<00:26,  5.24s/it] 75%|███████▌  | 15/20 [01:10<00:25,  5.08s/it] 75%|███████▌  | 15/20 [01:11<00:27,  5.40s/it] 75%|███████▌  | 15/20 [01:11<00:26,  5.29s/it] 80%|████████  | 16/20 [01:15<00:20,  5.16s/it] 80%|████████  | 16/20 [01:15<00:20,  5.10s/it] 80%|████████  | 16/20 [01:16<00:21,  5.36s/it] 80%|████████  | 16/20 [01:16<00:20,  5.18s/it] 85%|████████▌ | 17/20 [01:19<00:14,  4.95s/it] 85%|████████▌ | 17/20 [01:19<00:14,  4.88s/it] 85%|████████▌ | 17/20 [01:21<00:15,  5.07s/it] 85%|████████▌ | 17/20 [01:21<00:15,  5.00s/it] 90%|█████████ | 18/20 [01:23<00:09,  4.67s/it] 90%|█████████ | 18/20 [01:23<00:09,  4.59s/it] 90%|█████████ | 18/20 [01:25<00:09,  4.81s/it] 90%|█████████ | 18/20 [01:25<00:09,  4.73s/it] 95%|█████████▌| 19/20 [01:27<00:04,  4.60s/it] 95%|█████████▌| 19/20 [01:28<00:04,  4.55s/it] 95%|█████████▌| 19/20 [01:29<00:04,  4.70s/it] 95%|█████████▌| 19/20 [01:30<00:04,  4.87s/it]100%|██████████| 20/20 [01:32<00:00,  4.63s/it]100%|██████████| 20/20 [01:32<00:00,  4.65s/it]
100%|██████████| 20/20 [01:33<00:00,  4.85s/it]100%|██████████| 20/20 [01:33<00:00,  4.67s/it]
100%|██████████| 20/20 [01:34<00:00,  4.79s/it]100%|██████████| 20/20 [01:34<00:00,  4.74s/it]
100%|██████████| 20/20 [01:35<00:00,  4.88s/it]100%|██████████| 20/20 [01:35<00:00,  4.78s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 12:38:51,214 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_5/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 12:38:51,214 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 12:38:51,214 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 12:38:51,214 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 12:41:06,258 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 12:41:06,259 - RUN - INFO - Evaluation Results:
2025-02-22 12:41:06,261 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.565
 1  0        cross   0.45
 2  0        single  0.68
 3  1        all     0.545
 4  1        cross   0.39
 5  1        single  0.7
 6  2        all     0.58
 7  2        cross   0.42
 8  2        single  0.74
 9  3        all     0.565
10  3        cross   0.4
11  3        single  0.73
12  Average  all     0.56375
13  Average  cross   0.415
14  Average  single  0.7125
--  -------  ------  -------
[rank0]:[W222 12:41:06.061024428 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 12:41:13.947687 642019 site-packages/torch/distributed/run.py:793] 
W0222 12:41:13.947687 642019 site-packages/torch/distributed/run.py:793] *****************************************
W0222 12:41:13.947687 642019 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 12:41:13.947687 642019 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank1]:[W222 12:41:41.683363780 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 12:41:41.683363922 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 12:41:41.687336654 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 12:41:41.702550190 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.58s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.62s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.59s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.23s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.22s/it]
Max step: 7, bias_value: 0.2
Max step: 7, bias_value: 0.2
Max step: 7, bias_value: 0.2
Max step: 7, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [01:44<06:57, 104.49s/it] 20%|██        | 1/5 [01:46<07:07, 106.99s/it] 20%|██        | 1/5 [01:48<07:14, 108.64s/it] 20%|██        | 1/5 [01:50<07:20, 110.09s/it] 40%|████      | 2/5 [03:29<05:14, 104.80s/it] 40%|████      | 2/5 [03:33<05:18, 106.15s/it] 40%|████      | 2/5 [03:34<05:20, 106.70s/it] 40%|████      | 2/5 [03:38<05:29, 109.70s/it] 60%|██████    | 3/5 [05:16<03:31, 105.94s/it] 60%|██████    | 3/5 [05:20<03:33, 106.67s/it] 60%|██████    | 3/5 [05:23<03:35, 107.58s/it] 60%|██████    | 3/5 [05:36<03:47, 113.50s/it] 80%|████████  | 4/5 [07:04<01:46, 106.66s/it] 80%|████████  | 4/5 [07:16<01:50, 110.30s/it] 80%|████████  | 4/5 [07:19<01:51, 111.21s/it] 80%|████████  | 4/5 [07:27<01:52, 112.44s/it]100%|██████████| 5/5 [08:53<00:00, 107.36s/it]100%|██████████| 5/5 [08:53<00:00, 106.64s/it]
100%|██████████| 5/5 [09:06<00:00, 110.24s/it]100%|██████████| 5/5 [09:06<00:00, 109.27s/it]
100%|██████████| 5/5 [09:15<00:00, 112.84s/it]100%|██████████| 5/5 [09:15<00:00, 111.11s/it]
100%|██████████| 5/5 [09:18<00:00, 111.84s/it]100%|██████████| 5/5 [09:18<00:00, 111.64s/it]
[rank0]:[W222 12:52:34.350095385 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 12:52:40.862998 669093 site-packages/torch/distributed/run.py:793] 
W0222 12:52:40.862998 669093 site-packages/torch/distributed/run.py:793] *****************************************
W0222 12:52:40.862998 669093 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 12:52:40.862998 669093 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W222 12:53:07.806840953 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 12:53:07.806841040 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 12:53:07.832930165 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 12:53:37.923185579 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.19s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.18s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_7/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_7/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_7/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_7/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:07,  6.73s/it]  5%|▌         | 1/20 [00:07<02:19,  7.35s/it]  5%|▌         | 1/20 [00:07<02:22,  7.49s/it]  5%|▌         | 1/20 [00:07<02:25,  7.66s/it] 10%|█         | 2/20 [00:10<01:31,  5.06s/it] 10%|█         | 2/20 [00:10<01:29,  5.00s/it] 10%|█         | 2/20 [00:11<01:33,  5.22s/it] 10%|█         | 2/20 [00:11<01:34,  5.28s/it] 15%|█▌        | 3/20 [00:13<01:06,  3.93s/it] 15%|█▌        | 3/20 [00:13<01:06,  3.90s/it] 15%|█▌        | 3/20 [00:13<01:07,  3.94s/it] 15%|█▌        | 3/20 [00:13<01:08,  4.01s/it] 20%|██        | 4/20 [00:17<01:03,  3.99s/it] 20%|██        | 4/20 [00:17<01:03,  3.98s/it] 20%|██        | 4/20 [00:17<01:06,  4.15s/it] 20%|██        | 4/20 [00:18<01:07,  4.19s/it] 25%|██▌       | 5/20 [00:20<00:54,  3.63s/it] 25%|██▌       | 5/20 [00:20<00:54,  3.62s/it] 25%|██▌       | 5/20 [00:20<00:57,  3.84s/it] 25%|██▌       | 5/20 [00:21<00:58,  3.89s/it] 30%|███       | 6/20 [00:24<00:51,  3.69s/it] 30%|███       | 6/20 [00:24<00:52,  3.73s/it] 30%|███       | 6/20 [00:24<00:52,  3.76s/it] 30%|███       | 6/20 [00:25<00:54,  3.89s/it] 35%|███▌      | 7/20 [00:27<00:45,  3.53s/it] 35%|███▌      | 7/20 [00:27<00:46,  3.56s/it] 35%|███▌      | 7/20 [00:27<00:46,  3.61s/it] 35%|███▌      | 7/20 [00:28<00:47,  3.68s/it] 40%|████      | 8/20 [00:30<00:41,  3.48s/it] 40%|████      | 8/20 [00:31<00:42,  3.57s/it] 40%|████      | 8/20 [00:31<00:43,  3.61s/it] 40%|████      | 8/20 [00:32<00:43,  3.62s/it] 45%|████▌     | 9/20 [00:34<00:37,  3.42s/it] 45%|████▌     | 9/20 [00:34<00:38,  3.49s/it] 45%|████▌     | 9/20 [00:34<00:38,  3.53s/it] 45%|████▌     | 9/20 [00:35<00:39,  3.58s/it] 50%|█████     | 10/20 [00:38<00:36,  3.63s/it] 50%|█████     | 10/20 [00:38<00:36,  3.67s/it] 50%|█████     | 10/20 [00:39<00:39,  3.95s/it] 50%|█████     | 10/20 [00:39<00:37,  3.74s/it] 55%|█████▌    | 11/20 [00:43<00:35,  3.92s/it] 55%|█████▌    | 11/20 [00:43<00:36,  4.04s/it] 55%|█████▌    | 11/20 [00:44<00:37,  4.16s/it] 55%|█████▌    | 11/20 [00:44<00:36,  4.00s/it] 60%|██████    | 12/20 [00:50<00:40,  5.09s/it] 60%|██████    | 12/20 [00:50<00:40,  5.07s/it] 60%|██████    | 12/20 [00:51<00:38,  4.84s/it] 60%|██████    | 12/20 [00:51<00:41,  5.15s/it] 65%|██████▌   | 13/20 [00:55<00:34,  4.87s/it] 65%|██████▌   | 13/20 [00:55<00:34,  4.89s/it] 65%|██████▌   | 13/20 [00:55<00:32,  4.69s/it] 65%|██████▌   | 13/20 [00:56<00:35,  5.04s/it] 70%|███████   | 14/20 [00:59<00:27,  4.63s/it] 70%|███████   | 14/20 [00:59<00:28,  4.73s/it] 70%|███████   | 14/20 [01:00<00:27,  4.66s/it] 70%|███████   | 14/20 [01:01<00:29,  4.98s/it] 75%|███████▌  | 15/20 [01:03<00:23,  4.68s/it] 75%|███████▌  | 15/20 [01:05<00:24,  4.92s/it] 75%|███████▌  | 15/20 [01:05<00:24,  4.80s/it] 75%|███████▌  | 15/20 [01:06<00:25,  5.16s/it] 80%|████████  | 16/20 [01:07<00:17,  4.46s/it] 80%|████████  | 16/20 [01:08<00:17,  4.49s/it] 80%|████████  | 16/20 [01:09<00:18,  4.55s/it] 80%|████████  | 16/20 [01:11<00:19,  4.84s/it] 85%|████████▌ | 17/20 [01:11<00:12,  4.22s/it] 85%|████████▌ | 17/20 [01:12<00:12,  4.21s/it] 85%|████████▌ | 17/20 [01:12<00:12,  4.27s/it] 85%|████████▌ | 17/20 [01:14<00:13,  4.44s/it] 90%|█████████ | 18/20 [01:15<00:08,  4.00s/it] 90%|█████████ | 18/20 [01:15<00:08,  4.01s/it] 90%|█████████ | 18/20 [01:16<00:08,  4.04s/it] 90%|█████████ | 18/20 [01:18<00:08,  4.19s/it] 95%|█████████▌| 19/20 [01:19<00:04,  4.14s/it] 95%|█████████▌| 19/20 [01:20<00:04,  4.14s/it] 95%|█████████▌| 19/20 [01:21<00:04,  4.34s/it] 95%|█████████▌| 19/20 [01:22<00:04,  4.24s/it]100%|██████████| 20/20 [01:24<00:00,  4.38s/it]100%|██████████| 20/20 [01:24<00:00,  4.22s/it]
100%|██████████| 20/20 [01:25<00:00,  4.54s/it]100%|██████████| 20/20 [01:25<00:00,  4.28s/it]
100%|██████████| 20/20 [01:26<00:00,  4.51s/it]100%|██████████| 20/20 [01:26<00:00,  4.31s/it]
100%|██████████| 20/20 [01:27<00:00,  4.44s/it]100%|██████████| 20/20 [01:27<00:00,  4.37s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 12:56:36,590 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_7/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 12:56:36,590 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 12:56:36,591 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 12:56:36,591 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 12:58:24,795 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 12:58:24,796 - RUN - INFO - Evaluation Results:
2025-02-22 12:58:24,798 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.645
 1  0        cross   0.52
 2  0        single  0.77
 3  1        all     0.58
 4  1        cross   0.4
 5  1        single  0.76
 6  2        all     0.61
 7  2        cross   0.44
 8  2        single  0.78
 9  3        all     0.595
10  3        cross   0.42
11  3        single  0.77
12  Average  all     0.6075
13  Average  cross   0.445
14  Average  single  0.77
--  -------  ------  ------
[rank0]:[W222 12:58:25.613240699 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 12:58:31.520229 680612 site-packages/torch/distributed/run.py:793] 
W0222 12:58:31.520229 680612 site-packages/torch/distributed/run.py:793] *****************************************
W0222 12:58:31.520229 680612 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 12:58:31.520229 680612 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank1]:[W222 12:58:58.428358994 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 12:58:58.459651742 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 12:58:58.461374662 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 12:58:58.473664984 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]
Max step: 9, bias_value: 0.2
Max step: 9, bias_value: 0.2Max step: 9, bias_value: 0.2

Max step: 9, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [01:57<07:48, 117.02s/it] 20%|██        | 1/5 [01:57<07:50, 117.72s/it] 20%|██        | 1/5 [01:57<07:51, 117.80s/it] 20%|██        | 1/5 [02:03<08:15, 123.78s/it] 40%|████      | 2/5 [03:49<05:43, 114.38s/it] 40%|████      | 2/5 [03:54<05:51, 117.25s/it] 40%|████      | 2/5 [03:55<05:49, 116.54s/it] 40%|████      | 2/5 [04:00<06:02, 120.80s/it] 60%|██████    | 3/5 [05:48<03:52, 116.30s/it] 60%|██████    | 3/5 [05:52<03:53, 116.76s/it] 60%|██████    | 3/5 [05:52<03:55, 117.56s/it] 60%|██████    | 3/5 [06:13<04:12, 126.34s/it] 80%|████████  | 4/5 [07:53<01:59, 119.06s/it] 80%|████████  | 4/5 [07:59<02:02, 122.05s/it] 80%|████████  | 4/5 [08:05<02:03, 123.32s/it] 80%|████████  | 4/5 [08:15<02:04, 124.72s/it]100%|██████████| 5/5 [09:57<00:00, 120.52s/it]100%|██████████| 5/5 [09:57<00:00, 119.41s/it]
100%|██████████| 5/5 [10:04<00:00, 123.12s/it]100%|██████████| 5/5 [10:04<00:00, 120.80s/it]
100%|██████████| 5/5 [10:08<00:00, 123.07s/it]100%|██████████| 5/5 [10:08<00:00, 121.65s/it]
100%|██████████| 5/5 [10:15<00:00, 122.95s/it]100%|██████████| 5/5 [10:15<00:00, 123.13s/it]
[rank0]:[W222 13:10:52.605588799 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 13:10:59.079112 712788 site-packages/torch/distributed/run.py:793] 
W0222 13:10:59.079112 712788 site-packages/torch/distributed/run.py:793] *****************************************
W0222 13:10:59.079112 712788 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 13:10:59.079112 712788 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W222 13:11:26.251989061 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 13:11:26.251990095 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 13:11:26.252796582 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 13:11:56.912350059 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.43s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.39s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.43s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_9/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_9/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_9/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_9/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:10,  6.88s/it]  5%|▌         | 1/20 [00:07<02:23,  7.55s/it]  5%|▌         | 1/20 [00:07<02:24,  7.60s/it]  5%|▌         | 1/20 [00:07<02:27,  7.78s/it] 10%|█         | 2/20 [00:10<01:29,  4.96s/it] 10%|█         | 2/20 [00:10<01:29,  5.00s/it] 10%|█         | 2/20 [00:10<01:30,  5.05s/it] 10%|█         | 2/20 [00:11<01:34,  5.25s/it] 15%|█▌        | 3/20 [00:12<01:05,  3.82s/it] 15%|█▌        | 3/20 [00:13<01:04,  3.78s/it] 15%|█▌        | 3/20 [00:13<01:06,  3.89s/it] 15%|█▌        | 3/20 [00:13<01:08,  4.00s/it] 20%|██        | 4/20 [00:17<01:02,  3.91s/it] 20%|██        | 4/20 [00:17<01:02,  3.94s/it] 20%|██        | 4/20 [00:17<01:03,  4.00s/it] 20%|██        | 4/20 [00:17<01:05,  4.08s/it] 25%|██▌       | 5/20 [00:20<00:54,  3.64s/it] 25%|██▌       | 5/20 [00:20<00:53,  3.58s/it] 25%|██▌       | 5/20 [00:20<00:53,  3.56s/it] 25%|██▌       | 5/20 [00:21<00:56,  3.76s/it] 30%|███       | 6/20 [00:23<00:48,  3.47s/it] 30%|███       | 6/20 [00:23<00:49,  3.54s/it] 30%|███       | 6/20 [00:23<00:49,  3.54s/it] 30%|███       | 6/20 [00:24<00:51,  3.65s/it] 35%|███▌      | 7/20 [00:26<00:44,  3.39s/it] 35%|███▌      | 7/20 [00:26<00:43,  3.35s/it] 35%|███▌      | 7/20 [00:26<00:43,  3.37s/it] 35%|███▌      | 7/20 [00:27<00:45,  3.48s/it] 40%|████      | 8/20 [00:29<00:40,  3.37s/it] 40%|████      | 8/20 [00:29<00:39,  3.31s/it] 40%|████      | 8/20 [00:29<00:39,  3.33s/it] 40%|████      | 8/20 [00:31<00:41,  3.44s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.27s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.22s/it] 45%|████▌     | 9/20 [00:33<00:35,  3.23s/it] 45%|████▌     | 9/20 [00:34<00:36,  3.33s/it] 50%|█████     | 10/20 [00:36<00:32,  3.27s/it] 50%|█████     | 10/20 [00:37<00:37,  3.71s/it] 50%|█████     | 10/20 [00:37<00:36,  3.70s/it] 50%|█████     | 10/20 [00:37<00:34,  3.43s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.56s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.88s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.82s/it] 55%|█████▌    | 11/20 [00:42<00:32,  3.66s/it] 60%|██████    | 12/20 [00:45<00:31,  3.94s/it] 60%|██████    | 12/20 [00:46<00:33,  4.15s/it] 60%|██████    | 12/20 [00:46<00:32,  4.09s/it] 60%|██████    | 12/20 [00:46<00:32,  4.05s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.10s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.19s/it] 65%|██████▌   | 13/20 [00:51<00:30,  4.34s/it] 65%|██████▌   | 13/20 [00:51<00:29,  4.16s/it] 70%|███████   | 14/20 [00:53<00:24,  4.07s/it] 70%|███████   | 14/20 [00:54<00:24,  4.05s/it] 70%|███████   | 14/20 [00:55<00:24,  4.14s/it] 70%|███████   | 14/20 [00:55<00:26,  4.38s/it] 75%|███████▌  | 15/20 [00:58<00:21,  4.35s/it] 75%|███████▌  | 15/20 [00:59<00:21,  4.23s/it] 75%|███████▌  | 15/20 [01:00<00:22,  4.44s/it] 75%|███████▌  | 15/20 [01:01<00:23,  4.65s/it] 80%|████████  | 16/20 [01:02<00:16,  4.05s/it] 80%|████████  | 16/20 [01:03<00:16,  4.20s/it] 80%|████████  | 16/20 [01:04<00:17,  4.30s/it] 80%|████████  | 16/20 [01:04<00:17,  4.38s/it] 85%|████████▌ | 17/20 [01:05<00:11,  3.89s/it] 85%|████████▌ | 17/20 [01:07<00:11,  4.00s/it] 85%|████████▌ | 17/20 [01:08<00:12,  4.08s/it] 85%|████████▌ | 17/20 [01:08<00:12,  4.08s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.74s/it] 90%|█████████ | 18/20 [01:10<00:07,  3.81s/it] 90%|█████████ | 18/20 [01:11<00:07,  3.85s/it] 90%|█████████ | 18/20 [01:11<00:07,  3.89s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.63s/it] 95%|█████████▌| 19/20 [01:14<00:03,  3.75s/it] 95%|█████████▌| 19/20 [01:15<00:03,  3.77s/it] 95%|█████████▌| 19/20 [01:15<00:03,  3.80s/it]100%|██████████| 20/20 [01:17<00:00,  3.92s/it]100%|██████████| 20/20 [01:17<00:00,  3.85s/it]
100%|██████████| 20/20 [01:18<00:00,  3.88s/it]100%|██████████| 20/20 [01:18<00:00,  3.91s/it]
100%|██████████| 20/20 [01:19<00:00,  3.92s/it]100%|██████████| 20/20 [01:19<00:00,  3.97s/it]
100%|██████████| 20/20 [01:19<00:00,  3.92s/it]100%|██████████| 20/20 [01:19<00:00,  3.98s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 13:14:48,447 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_9/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 13:14:48,447 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 13:14:48,447 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 13:14:48,447 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 13:16:58,371 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 13:16:58,372 - RUN - INFO - Evaluation Results:
2025-02-22 13:16:58,374 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.645
 1  0        single  0.78
 2  0        cross   0.51
 3  1        all     0.595
 4  1        single  0.79
 5  1        cross   0.4
 6  2        all     0.59
 7  2        single  0.78
 8  2        cross   0.4
 9  3        all     0.595
10  3        single  0.77
11  3        cross   0.42
12  Average  all     0.60625
13  Average  single  0.78
14  Average  cross   0.4325
--  -------  ------  -------
[rank0]:[W222 13:16:59.189516258 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 13:17:04.897028 724681 site-packages/torch/distributed/run.py:793] 
W0222 13:17:04.897028 724681 site-packages/torch/distributed/run.py:793] *****************************************
W0222 13:17:04.897028 724681 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 13:17:04.897028 724681 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank2]:[W222 13:17:32.480277179 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 13:17:32.480277031 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 13:17:32.481842272 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 13:17:32.483068142 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.28s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.24it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.29it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]
Max step: 11, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.25s/it]
Max step: 11, bias_value: 0.2
Max step: 11, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
  0%|          | 0/5 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Max step: 11, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [02:05<08:20, 125.02s/it] 20%|██        | 1/5 [02:05<08:22, 125.55s/it] 20%|██        | 1/5 [02:06<08:27, 126.89s/it] 20%|██        | 1/5 [02:16<09:04, 136.13s/it] 40%|████      | 2/5 [04:16<06:26, 128.91s/it] 40%|████      | 2/5 [04:06<06:08, 122.94s/it] 40%|████      | 2/5 [04:11<06:16, 125.62s/it] 40%|████      | 2/5 [04:15<06:18, 126.21s/it] 60%|██████    | 3/5 [06:14<04:08, 124.22s/it] 60%|██████    | 3/5 [06:16<04:12, 126.23s/it] 60%|██████    | 3/5 [06:20<04:11, 125.69s/it] 60%|██████    | 3/5 [06:37<04:29, 134.58s/it] 80%|████████  | 4/5 [08:25<02:07, 127.11s/it] 80%|████████  | 4/5 [08:46<02:12, 132.19s/it] 80%|████████  | 4/5 [08:44<02:12, 132.77s/it] 80%|████████  | 4/5 [08:44<02:14, 134.69s/it]100%|██████████| 5/5 [10:36<00:00, 128.45s/it]100%|██████████| 5/5 [10:36<00:00, 127.31s/it]
100%|██████████| 5/5 [10:55<00:00, 130.87s/it]100%|██████████| 5/5 [10:55<00:00, 131.01s/it]
100%|██████████| 5/5 [10:55<00:00, 132.41s/it]100%|██████████| 5/5 [10:55<00:00, 131.17s/it]
100%|██████████| 5/5 [10:56<00:00, 133.75s/it]100%|██████████| 5/5 [10:56<00:00, 131.31s/it]
[rank0]:[W222 13:29:46.201637544 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 13:29:52.985947 756442 site-packages/torch/distributed/run.py:793] 
W0222 13:29:52.985947 756442 site-packages/torch/distributed/run.py:793] *****************************************
W0222 13:29:52.985947 756442 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 13:29:52.985947 756442 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank2]:[W222 13:30:22.763346550 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 13:30:22.763346592 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 13:30:22.763347039 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 13:30:53.751492270 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.40s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.39s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.31it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_11/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_11/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonMax step: 50, bias_value: 0.2

load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_11/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_11/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:07,  6.69s/it]  5%|▌         | 1/20 [00:07<02:13,  7.00s/it]  5%|▌         | 1/20 [00:07<02:17,  7.21s/it]  5%|▌         | 1/20 [00:07<02:18,  7.27s/it] 10%|█         | 2/20 [00:10<01:27,  4.86s/it] 10%|█         | 2/20 [00:10<01:26,  4.82s/it] 10%|█         | 2/20 [00:10<01:28,  4.92s/it] 10%|█         | 2/20 [00:10<01:31,  5.08s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.78s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.70s/it] 15%|█▌        | 3/20 [00:13<01:04,  3.81s/it] 15%|█▌        | 3/20 [00:13<01:06,  3.89s/it] 20%|██        | 4/20 [00:16<01:01,  3.87s/it] 20%|██        | 4/20 [00:16<01:02,  3.89s/it] 20%|██        | 4/20 [00:16<01:01,  3.85s/it] 20%|██        | 4/20 [00:17<01:05,  4.07s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.54s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.50s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.61s/it] 25%|██▌       | 5/20 [00:20<00:56,  3.75s/it] 30%|███       | 6/20 [00:23<00:48,  3.48s/it] 30%|███       | 6/20 [00:23<00:48,  3.50s/it] 30%|███       | 6/20 [00:23<00:49,  3.53s/it] 30%|███       | 6/20 [00:24<00:51,  3.66s/it] 35%|███▌      | 7/20 [00:25<00:42,  3.24s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.19s/it] 35%|███▌      | 7/20 [00:25<00:42,  3.25s/it] 35%|███▌      | 7/20 [00:26<00:43,  3.32s/it] 40%|████      | 8/20 [00:28<00:38,  3.18s/it] 40%|████      | 8/20 [00:29<00:39,  3.33s/it] 40%|████      | 8/20 [00:29<00:40,  3.37s/it] 40%|████      | 8/20 [00:30<00:40,  3.35s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.12s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.24s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.24s/it] 45%|████▌     | 9/20 [00:33<00:35,  3.25s/it] 50%|█████     | 10/20 [00:36<00:33,  3.33s/it] 50%|█████     | 10/20 [00:36<00:36,  3.61s/it] 50%|█████     | 10/20 [00:37<00:36,  3.68s/it] 50%|█████     | 10/20 [00:37<00:34,  3.40s/it] 55%|█████▌    | 11/20 [00:40<00:31,  3.55s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.77s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.81s/it] 55%|█████▌    | 11/20 [00:41<00:32,  3.63s/it] 60%|██████    | 12/20 [00:44<00:31,  3.94s/it] 60%|██████    | 12/20 [00:45<00:32,  4.03s/it] 60%|██████    | 12/20 [00:45<00:32,  4.09s/it] 60%|██████    | 12/20 [00:46<00:32,  4.02s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.10s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.13s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.28s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.15s/it] 70%|███████   | 14/20 [00:53<00:23,  3.99s/it] 70%|███████   | 14/20 [00:53<00:24,  4.11s/it] 70%|███████   | 14/20 [00:54<00:25,  4.20s/it] 70%|███████   | 14/20 [00:55<00:26,  4.36s/it] 75%|███████▌  | 15/20 [00:57<00:20,  4.12s/it] 75%|███████▌  | 15/20 [00:58<00:21,  4.26s/it] 75%|███████▌  | 15/20 [00:59<00:21,  4.35s/it] 75%|███████▌  | 15/20 [01:00<00:22,  4.53s/it] 80%|████████  | 16/20 [01:00<00:15,  3.79s/it] 80%|████████  | 16/20 [01:01<00:15,  3.90s/it] 80%|████████  | 16/20 [01:02<00:16,  4.02s/it] 80%|████████  | 16/20 [01:03<00:16,  4.12s/it] 85%|████████▌ | 17/20 [01:04<00:11,  3.75s/it] 85%|████████▌ | 17/20 [01:04<00:11,  3.81s/it] 85%|████████▌ | 17/20 [01:06<00:11,  3.90s/it] 85%|████████▌ | 17/20 [01:06<00:11,  3.91s/it] 90%|█████████ | 18/20 [01:07<00:07,  3.62s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.66s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.71s/it] 90%|█████████ | 18/20 [01:10<00:07,  3.75s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.45s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.55s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.59s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.64s/it]100%|██████████| 20/20 [01:15<00:00,  3.77s/it]100%|██████████| 20/20 [01:15<00:00,  3.77s/it]
100%|██████████| 20/20 [01:15<00:00,  3.71s/it]100%|██████████| 20/20 [01:15<00:00,  3.78s/it]
100%|██████████| 20/20 [01:17<00:00,  3.80s/it]100%|██████████| 20/20 [01:17<00:00,  3.87s/it]
100%|██████████| 20/20 [01:17<00:00,  3.81s/it]100%|██████████| 20/20 [01:17<00:00,  3.88s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 13:33:47,694 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_11/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 13:33:47,694 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 13:33:47,694 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 13:33:47,694 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 13:36:11,905 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 13:36:11,906 - RUN - INFO - Evaluation Results:
2025-02-22 13:36:11,908 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.645
 1  0        cross   0.51
 2  0        single  0.78
 3  1        all     0.6
 4  1        cross   0.4
 5  1        single  0.8
 6  2        all     0.585
 7  2        cross   0.39
 8  2        single  0.78
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.60625
13  Average  cross   0.4275
14  Average  single  0.785
--  -------  ------  -------
[rank0]:[W222 13:36:12.929288551 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 13:36:18.688037 769884 site-packages/torch/distributed/run.py:793] 
W0222 13:36:18.688037 769884 site-packages/torch/distributed/run.py:793] *****************************************
W0222 13:36:18.688037 769884 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 13:36:18.688037 769884 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank0]:[W222 13:36:47.767368673 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 13:36:47.770602798 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 13:36:47.770684954 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 13:36:47.776483492 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.59s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.59s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.55s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]
Max step: 13, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 13, bias_value: 0.2Max step: 13, bias_value: 0.2

Max step: 13, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [02:13<08:54, 133.51s/it] 20%|██        | 1/5 [02:15<09:00, 135.08s/it] 20%|██        | 1/5 [02:15<09:01, 135.35s/it] 20%|██        | 1/5 [02:27<09:49, 147.42s/it] 40%|████      | 2/5 [04:21<06:31, 130.47s/it] 40%|████      | 2/5 [04:25<06:37, 132.39s/it] 40%|████      | 2/5 [04:35<06:55, 138.40s/it] 40%|████      | 2/5 [04:36<06:50, 136.87s/it] 60%|██████    | 3/5 [06:31<04:19, 129.65s/it] 60%|██████    | 3/5 [06:36<04:24, 132.16s/it] 60%|██████    | 3/5 [06:55<04:35, 137.55s/it] 60%|██████    | 3/5 [07:03<04:45, 142.74s/it] 80%|████████  | 4/5 [08:50<02:13, 133.09s/it] 80%|████████  | 4/5 [09:11<02:21, 141.32s/it] 80%|████████  | 4/5 [09:23<02:21, 141.37s/it] 80%|████████  | 4/5 [09:29<02:23, 143.94s/it]100%|██████████| 5/5 [11:11<00:00, 135.88s/it]100%|██████████| 5/5 [11:11<00:00, 134.24s/it]
100%|██████████| 5/5 [11:33<00:00, 141.71s/it]100%|██████████| 5/5 [11:33<00:00, 138.76s/it]
100%|██████████| 5/5 [11:39<00:00, 139.59s/it]100%|██████████| 5/5 [11:39<00:00, 139.90s/it]
100%|██████████| 5/5 [11:51<00:00, 143.45s/it]100%|██████████| 5/5 [11:51<00:00, 142.31s/it]
[rank0]:[W222 13:50:14.062311211 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 13:50:21.749879 803715 site-packages/torch/distributed/run.py:793] 
W0222 13:50:21.749879 803715 site-packages/torch/distributed/run.py:793] *****************************************
W0222 13:50:21.749879 803715 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 13:50:21.749879 803715 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank2]:[W222 13:50:49.117079389 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 13:50:49.117079094 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 13:50:49.117587574 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 13:51:20.193814900 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.40s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.25s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.42s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_13/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2Max step: 50, bias_value: 0.2

load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_13/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_13/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_13/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:00,  6.34s/it]  5%|▌         | 1/20 [00:07<02:14,  7.06s/it]  5%|▌         | 1/20 [00:07<02:16,  7.16s/it]  5%|▌         | 1/20 [00:07<02:16,  7.16s/it] 10%|█         | 2/20 [00:09<01:24,  4.69s/it] 10%|█         | 2/20 [00:10<01:26,  4.79s/it] 10%|█         | 2/20 [00:10<01:27,  4.87s/it] 10%|█         | 2/20 [00:10<01:29,  4.95s/it] 15%|█▌        | 3/20 [00:12<01:01,  3.60s/it] 15%|█▌        | 3/20 [00:12<01:00,  3.58s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.68s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.70s/it] 20%|██        | 4/20 [00:16<00:59,  3.75s/it] 20%|██        | 4/20 [00:16<00:57,  3.61s/it] 20%|██        | 4/20 [00:16<00:59,  3.73s/it] 20%|██        | 4/20 [00:16<01:01,  3.84s/it] 25%|██▌       | 5/20 [00:19<00:51,  3.44s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.51s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.63s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.68s/it] 30%|███       | 6/20 [00:22<00:48,  3.47s/it] 30%|███       | 6/20 [00:22<00:48,  3.44s/it] 30%|███       | 6/20 [00:22<00:48,  3.49s/it] 30%|███       | 6/20 [00:23<00:50,  3.62s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.20s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.17s/it] 35%|███▌      | 7/20 [00:25<00:42,  3.28s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.28s/it] 40%|████      | 8/20 [00:28<00:38,  3.21s/it] 40%|████      | 8/20 [00:28<00:39,  3.28s/it] 40%|████      | 8/20 [00:29<00:39,  3.32s/it] 40%|████      | 8/20 [00:29<00:39,  3.31s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.14s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.16s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.23s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.22s/it] 50%|█████     | 10/20 [00:35<00:32,  3.22s/it] 50%|█████     | 10/20 [00:36<00:36,  3.62s/it] 50%|█████     | 10/20 [00:36<00:34,  3.43s/it] 50%|█████     | 10/20 [00:36<00:36,  3.67s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.46s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.75s/it] 55%|█████▌    | 11/20 [00:40<00:34,  3.79s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.67s/it] 60%|██████    | 12/20 [00:43<00:30,  3.85s/it] 60%|██████    | 12/20 [00:45<00:32,  4.01s/it] 60%|██████    | 12/20 [00:45<00:32,  4.10s/it] 60%|██████    | 12/20 [00:45<00:32,  4.02s/it] 65%|██████▌   | 13/20 [00:48<00:28,  4.03s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.10s/it] 65%|██████▌   | 13/20 [00:50<00:28,  4.14s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.28s/it] 70%|███████   | 14/20 [00:52<00:24,  4.02s/it] 70%|███████   | 14/20 [00:53<00:23,  3.98s/it] 70%|███████   | 14/20 [00:54<00:24,  4.13s/it] 70%|███████   | 14/20 [00:54<00:26,  4.36s/it] 75%|███████▌  | 15/20 [00:56<00:20,  4.09s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.85s/it] 75%|███████▌  | 15/20 [00:58<00:20,  4.11s/it] 80%|████████  | 16/20 [00:59<00:14,  3.66s/it] 75%|███████▌  | 15/20 [00:59<00:21,  4.36s/it] 80%|████████  | 16/20 [01:00<00:14,  3.74s/it] 80%|████████  | 16/20 [01:01<00:15,  3.87s/it] 80%|████████  | 16/20 [01:02<00:16,  4.06s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.61s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.67s/it] 85%|████████▌ | 17/20 [01:05<00:11,  3.75s/it] 85%|████████▌ | 17/20 [01:06<00:11,  3.88s/it] 90%|█████████ | 18/20 [01:06<00:07,  3.54s/it] 90%|█████████ | 18/20 [01:06<00:07,  3.59s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.63s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.42s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.72s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.50s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.55s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.56s/it]100%|██████████| 20/20 [01:13<00:00,  3.76s/it]100%|██████████| 20/20 [01:13<00:00,  3.69s/it]
100%|██████████| 20/20 [01:14<00:00,  3.74s/it]100%|██████████| 20/20 [01:14<00:00,  3.73s/it]
100%|██████████| 20/20 [01:16<00:00,  3.82s/it]100%|██████████| 20/20 [01:16<00:00,  3.81s/it]
100%|██████████| 20/20 [01:16<00:00,  3.79s/it]100%|██████████| 20/20 [01:16<00:00,  3.84s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 13:54:11,347 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_13/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 13:54:11,347 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 13:54:11,347 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 13:54:11,348 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 13:56:29,089 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 13:56:29,089 - RUN - INFO - Evaluation Results:
2025-02-22 13:56:29,092 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.65
 1  0        single  0.78
 2  0        cross   0.52
 3  1        all     0.605
 4  1        single  0.8
 5  1        cross   0.41
 6  2        all     0.585
 7  2        single  0.78
 8  2        cross   0.39
 9  3        all     0.595
10  3        single  0.78
11  3        cross   0.41
12  Average  all     0.60875
13  Average  single  0.785
14  Average  cross   0.4325
--  -------  ------  -------
[rank0]:[W222 13:56:29.908582967 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 13:56:35.967550 816175 site-packages/torch/distributed/run.py:793] 
W0222 13:56:35.967550 816175 site-packages/torch/distributed/run.py:793] *****************************************
W0222 13:56:35.967550 816175 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 13:56:35.967550 816175 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank1]:[W222 13:57:09.625454241 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 13:57:09.625661326 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 13:57:09.643604388 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 13:57:09.643971938 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.23s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.23s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.22s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Max step: 15, bias_value: 0.2
Max step: 15, bias_value: 0.2
Max step: 15, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 15, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [02:19<09:19, 139.95s/it] 20%|██        | 1/5 [02:21<09:27, 141.82s/it] 20%|██        | 1/5 [02:23<09:35, 143.80s/it] 20%|██        | 1/5 [02:37<10:29, 157.42s/it] 40%|████      | 2/5 [04:33<06:49, 136.45s/it] 40%|████      | 2/5 [04:36<06:52, 137.50s/it] 40%|████      | 2/5 [04:49<07:07, 142.61s/it] 40%|████      | 2/5 [04:53<07:20, 146.99s/it] 60%|██████    | 3/5 [06:47<04:29, 134.50s/it] 60%|██████    | 3/5 [06:54<04:36, 138.27s/it] 60%|██████    | 3/5 [07:17<04:49, 145.00s/it] 60%|██████    | 3/5 [07:30<05:03, 151.60s/it] 80%|████████  | 4/5 [09:13<02:19, 139.14s/it] 80%|████████  | 4/5 [09:37<02:27, 147.94s/it] 80%|████████  | 4/5 [09:56<02:29, 149.55s/it] 80%|████████  | 4/5 [10:02<02:33, 153.06s/it]100%|██████████| 5/5 [11:44<00:00, 143.25s/it]100%|██████████| 5/5 [11:44<00:00, 140.81s/it]
100%|██████████| 5/5 [12:07<00:00, 148.96s/it]100%|██████████| 5/5 [12:07<00:00, 145.58s/it]
100%|██████████| 5/5 [12:17<00:00, 146.63s/it]100%|██████████| 5/5 [12:17<00:00, 147.59s/it]
100%|██████████| 5/5 [12:33<00:00, 152.14s/it]100%|██████████| 5/5 [12:33<00:00, 150.69s/it]
[rank0]:[W222 14:11:14.489870597 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 14:11:21.055207 852642 site-packages/torch/distributed/run.py:793] 
W0222 14:11:21.055207 852642 site-packages/torch/distributed/run.py:793] *****************************************
W0222 14:11:21.055207 852642 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 14:11:21.055207 852642 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank3]:[W222 14:11:47.537712768 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 14:11:47.537712722 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 14:11:47.538629137 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 14:12:18.190576843 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.66s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.60s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_15/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_15/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0: load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_15/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov

Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_15/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Rank 0:  Model Class: LlavaQwenForCausalLM
  5%|▌         | 1/20 [00:06<01:55,  6.07s/it] 10%|█         | 2/20 [00:09<01:17,  4.29s/it]  0%|          | 0/20 [00:00<?, ?it/s] 15%|█▌        | 3/20 [00:11<00:56,  3.31s/it]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
 20%|██        | 4/20 [00:15<00:59,  3.72s/it]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:05<01:48,  5.69s/it]  5%|▌         | 1/20 [00:06<02:03,  6.49s/it]  5%|▌         | 1/20 [00:06<02:04,  6.53s/it] 25%|██▌       | 5/20 [00:18<00:52,  3.50s/it] 10%|█         | 2/20 [00:09<01:17,  4.32s/it] 10%|█         | 2/20 [00:09<01:20,  4.46s/it] 10%|█         | 2/20 [00:09<01:23,  4.64s/it] 30%|███       | 6/20 [00:22<00:49,  3.51s/it] 15%|█▌        | 3/20 [00:11<00:57,  3.41s/it] 15%|█▌        | 3/20 [00:11<00:58,  3.45s/it] 15%|█▌        | 3/20 [00:12<01:01,  3.59s/it] 35%|███▌      | 7/20 [00:24<00:41,  3.18s/it] 20%|██        | 4/20 [00:15<00:56,  3.56s/it] 20%|██        | 4/20 [00:15<00:55,  3.45s/it] 20%|██        | 4/20 [00:16<00:59,  3.72s/it] 40%|████      | 8/20 [00:28<00:39,  3.29s/it] 25%|██▌       | 5/20 [00:18<00:50,  3.37s/it] 25%|██▌       | 5/20 [00:18<00:52,  3.51s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.59s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.17s/it] 30%|███       | 6/20 [00:21<00:47,  3.39s/it] 30%|███       | 6/20 [00:21<00:47,  3.42s/it] 30%|███       | 6/20 [00:23<00:50,  3.58s/it] 50%|█████     | 10/20 [00:34<00:33,  3.33s/it] 35%|███▌      | 7/20 [00:24<00:41,  3.19s/it] 35%|███▌      | 7/20 [00:24<00:41,  3.17s/it] 35%|███▌      | 7/20 [00:25<00:42,  3.26s/it] 55%|█████▌    | 11/20 [00:38<00:31,  3.55s/it] 40%|████      | 8/20 [00:27<00:38,  3.17s/it] 40%|████      | 8/20 [00:28<00:39,  3.31s/it] 40%|████      | 8/20 [00:29<00:39,  3.30s/it] 45%|████▌     | 9/20 [00:30<00:34,  3.10s/it] 45%|████▌     | 9/20 [00:31<00:35,  3.22s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.22s/it] 60%|██████    | 12/20 [00:43<00:31,  3.99s/it] 50%|█████     | 10/20 [00:35<00:35,  3.60s/it] 50%|█████     | 10/20 [00:35<00:33,  3.35s/it] 50%|█████     | 10/20 [00:35<00:36,  3.68s/it] 65%|██████▌   | 13/20 [00:48<00:29,  4.16s/it] 55%|█████▌    | 11/20 [00:39<00:33,  3.74s/it] 55%|█████▌    | 11/20 [00:39<00:34,  3.78s/it] 55%|█████▌    | 11/20 [00:39<00:32,  3.63s/it] 70%|███████   | 14/20 [00:52<00:24,  4.15s/it] 60%|██████    | 12/20 [00:44<00:32,  4.01s/it] 60%|██████    | 12/20 [00:44<00:32,  4.07s/it] 60%|██████    | 12/20 [00:44<00:31,  3.99s/it] 75%|███████▌  | 15/20 [00:56<00:21,  4.22s/it] 80%|████████  | 16/20 [00:59<00:14,  3.74s/it] 65%|██████▌   | 13/20 [00:48<00:28,  4.09s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.25s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.13s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.61s/it] 70%|███████   | 14/20 [00:52<00:23,  4.00s/it] 70%|███████   | 14/20 [00:53<00:25,  4.18s/it] 70%|███████   | 14/20 [00:53<00:25,  4.32s/it] 90%|█████████ | 18/20 [01:06<00:07,  3.52s/it] 75%|███████▌  | 15/20 [00:55<00:19,  3.92s/it] 75%|███████▌  | 15/20 [00:57<00:21,  4.22s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.40s/it] 75%|███████▌  | 15/20 [00:58<00:21,  4.39s/it] 80%|████████  | 16/20 [00:59<00:15,  3.86s/it] 80%|████████  | 16/20 [01:01<00:16,  4.03s/it] 80%|████████  | 16/20 [01:01<00:16,  4.00s/it]100%|██████████| 20/20 [01:14<00:00,  3.81s/it]100%|██████████| 20/20 [01:14<00:00,  3.70s/it]
 85%|████████▌ | 17/20 [01:02<00:11,  3.67s/it] 85%|████████▌ | 17/20 [01:04<00:11,  3.72s/it] 85%|████████▌ | 17/20 [01:04<00:11,  3.79s/it] 90%|█████████ | 18/20 [01:06<00:07,  3.57s/it] 90%|█████████ | 18/20 [01:07<00:07,  3.62s/it] 90%|█████████ | 18/20 [01:07<00:07,  3.63s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.51s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.48s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.53s/it]100%|██████████| 20/20 [01:13<00:00,  3.69s/it]100%|██████████| 20/20 [01:13<00:00,  3.68s/it]
100%|██████████| 20/20 [01:15<00:00,  3.70s/it]100%|██████████| 20/20 [01:15<00:00,  3.76s/it]
100%|██████████| 20/20 [01:15<00:00,  3.73s/it]100%|██████████| 20/20 [01:15<00:00,  3.77s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 14:14:57,279 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_15/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 14:14:57,279 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 14:14:57,280 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 14:14:57,280 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 14:16:57,349 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 14:16:57,350 - RUN - INFO - Evaluation Results:
2025-02-22 14:16:57,352 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.65
 1  0        single  0.77
 2  0        cross   0.53
 3  1        all     0.605
 4  1        single  0.8
 5  1        cross   0.41
 6  2        all     0.585
 7  2        single  0.78
 8  2        cross   0.39
 9  3        all     0.595
10  3        single  0.78
11  3        cross   0.41
12  Average  all     0.60875
13  Average  single  0.7825
14  Average  cross   0.435
--  -------  ------  -------
[rank0]:[W222 14:16:58.166244981 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 14:17:03.842956 861463 site-packages/torch/distributed/run.py:793] 
W0222 14:17:03.842956 861463 site-packages/torch/distributed/run.py:793] *****************************************
W0222 14:17:03.842956 861463 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 14:17:03.842956 861463 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank2]:[W222 14:17:29.102797059 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 14:17:29.109165485 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 14:17:29.111857875 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 14:17:29.117017264 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]
Max step: 17, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.31s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]
  0%|          | 0/5 [00:00<?, ?it/s]Max step: 17, bias_value: 0.2
Max step: 17, bias_value: 0.2
Max step: 17, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [02:28<09:54, 148.62s/it] 20%|██        | 1/5 [02:22<09:29, 142.49s/it] 20%|██        | 1/5 [02:27<09:50, 147.74s/it] 20%|██        | 1/5 [02:41<10:46, 161.62s/it] 40%|████      | 2/5 [05:06<07:41, 153.77s/it] 40%|████      | 2/5 [04:42<07:02, 140.97s/it] 40%|████      | 2/5 [04:47<07:09, 143.01s/it] 40%|████      | 2/5 [05:02<07:27, 149.17s/it] 60%|██████    | 3/5 [07:00<04:39, 139.58s/it] 60%|██████    | 3/5 [07:15<04:50, 145.10s/it] 60%|██████    | 3/5 [07:56<05:22, 161.28s/it] 60%|██████    | 3/5 [07:32<04:59, 149.62s/it] 80%|████████  | 4/5 [09:27<02:22, 142.71s/it] 80%|████████  | 4/5 [10:29<02:38, 158.28s/it] 80%|████████  | 4/5 [10:07<02:35, 155.84s/it] 80%|████████  | 4/5 [10:25<02:39, 159.13s/it]100%|██████████| 5/5 [12:08<00:00, 149.13s/it]100%|██████████| 5/5 [12:08<00:00, 145.67s/it]
100%|██████████| 5/5 [12:53<00:00, 152.91s/it]100%|██████████| 5/5 [12:53<00:00, 154.65s/it]
100%|██████████| 5/5 [12:41<00:00, 155.37s/it]100%|██████████| 5/5 [12:41<00:00, 152.37s/it]
100%|██████████| 5/5 [13:08<00:00, 160.27s/it]100%|██████████| 5/5 [13:08<00:00, 157.65s/it]
[rank0]:[W222 14:32:06.289145483 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 14:32:12.899769 891132 site-packages/torch/distributed/run.py:793] 
W0222 14:32:12.899769 891132 site-packages/torch/distributed/run.py:793] *****************************************
W0222 14:32:12.899769 891132 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 14:32:12.899769 891132 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W222 14:32:41.755950603 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 14:32:41.755950491 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 14:32:41.756713942 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 14:33:12.949259817 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.42s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.41s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_17/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_17/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_17/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_17/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:03,  6.52s/it]  5%|▌         | 1/20 [00:06<02:12,  6.99s/it]  5%|▌         | 1/20 [00:07<02:15,  7.13s/it]  5%|▌         | 1/20 [00:07<02:16,  7.17s/it] 10%|█         | 2/20 [00:09<01:24,  4.67s/it] 10%|█         | 2/20 [00:10<01:24,  4.69s/it] 10%|█         | 2/20 [00:10<01:25,  4.73s/it] 10%|█         | 2/20 [00:10<01:27,  4.84s/it] 15%|█▌        | 3/20 [00:12<01:00,  3.58s/it] 15%|█▌        | 3/20 [00:12<00:59,  3.52s/it] 15%|█▌        | 3/20 [00:12<01:01,  3.61s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.67s/it] 20%|██        | 4/20 [00:15<00:58,  3.67s/it] 20%|██        | 4/20 [00:16<00:57,  3.59s/it] 20%|██        | 4/20 [00:16<00:58,  3.64s/it] 20%|██        | 4/20 [00:16<01:00,  3.80s/it] 25%|██▌       | 5/20 [00:19<00:51,  3.43s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.56s/it] 25%|██▌       | 5/20 [00:19<00:51,  3.45s/it] 25%|██▌       | 5/20 [00:20<00:54,  3.64s/it] 30%|███       | 6/20 [00:22<00:48,  3.44s/it] 30%|███       | 6/20 [00:22<00:48,  3.47s/it] 30%|███       | 6/20 [00:22<00:48,  3.45s/it] 30%|███       | 6/20 [00:23<00:50,  3.60s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.21s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.19s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.18s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.26s/it] 40%|████      | 8/20 [00:28<00:38,  3.18s/it] 40%|████      | 8/20 [00:28<00:39,  3.27s/it] 40%|████      | 8/20 [00:28<00:39,  3.32s/it] 40%|████      | 8/20 [00:29<00:39,  3.29s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.11s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.18s/it] 45%|████▌     | 9/20 [00:31<00:35,  3.22s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.19s/it] 50%|█████     | 10/20 [00:34<00:32,  3.23s/it] 50%|█████     | 10/20 [00:36<00:35,  3.59s/it] 50%|█████     | 10/20 [00:36<00:34,  3.42s/it] 50%|█████     | 10/20 [00:36<00:36,  3.66s/it] 55%|█████▌    | 11/20 [00:38<00:31,  3.45s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.76s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.59s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.76s/it] 60%|██████    | 12/20 [00:43<00:30,  3.86s/it] 60%|██████    | 12/20 [00:44<00:32,  4.01s/it] 60%|██████    | 12/20 [00:45<00:32,  4.06s/it] 60%|██████    | 12/20 [00:45<00:31,  3.95s/it] 65%|██████▌   | 13/20 [00:48<00:28,  4.08s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.11s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.11s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.26s/it] 70%|███████   | 14/20 [00:52<00:24,  4.05s/it] 70%|███████   | 14/20 [00:53<00:24,  4.04s/it] 70%|███████   | 14/20 [00:53<00:24,  4.12s/it] 70%|███████   | 14/20 [00:54<00:25,  4.33s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.99s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.86s/it] 75%|███████▌  | 15/20 [00:57<00:20,  4.10s/it] 75%|███████▌  | 15/20 [00:58<00:21,  4.31s/it] 80%|████████  | 16/20 [00:58<00:14,  3.56s/it] 80%|████████  | 16/20 [00:59<00:14,  3.71s/it] 80%|████████  | 16/20 [01:01<00:15,  3.84s/it] 80%|████████  | 16/20 [01:01<00:15,  3.96s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.47s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.57s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.64s/it] 85%|████████▌ | 17/20 [01:04<00:11,  3.68s/it] 90%|█████████ | 18/20 [01:05<00:06,  3.43s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.47s/it] 90%|█████████ | 18/20 [01:07<00:07,  3.54s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.58s/it] 95%|█████████▌| 19/20 [01:08<00:03,  3.33s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.39s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.47s/it] 95%|█████████▌| 19/20 [01:13<00:04,  4.08s/it]100%|██████████| 20/20 [01:15<00:00,  4.54s/it]100%|██████████| 20/20 [01:15<00:00,  3.79s/it]
100%|██████████| 20/20 [01:17<00:00,  4.77s/it]100%|██████████| 20/20 [01:17<00:00,  3.88s/it]
100%|██████████| 20/20 [01:17<00:00,  4.52s/it]100%|██████████| 20/20 [01:17<00:00,  3.89s/it]
100%|██████████| 20/20 [01:19<00:00,  4.79s/it]100%|██████████| 20/20 [01:19<00:00,  4.00s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 14:36:03,711 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_17/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 14:36:03,712 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 14:36:03,712 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 14:36:03,712 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 14:38:20,669 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 14:38:20,669 - RUN - INFO - Evaluation Results:
2025-02-22 14:38:20,671 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.65
 1  0        cross   0.53
 2  0        single  0.77
 3  1        all     0.605
 4  1        cross   0.41
 5  1        single  0.8
 6  2        all     0.585
 7  2        cross   0.39
 8  2        single  0.78
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.60875
13  Average  cross   0.435
14  Average  single  0.7825
--  -------  ------  -------
[rank0]:[W222 14:38:21.509703224 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 14:38:27.042193 900898 site-packages/torch/distributed/run.py:793] 
W0222 14:38:27.042193 900898 site-packages/torch/distributed/run.py:793] *****************************************
W0222 14:38:27.042193 900898 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 14:38:27.042193 900898 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank3]:[W222 14:38:53.688417053 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 14:38:53.691031685 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 14:38:53.697564395 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 14:38:53.706346518 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.19s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]
Max step: 19, bias_value: 0.2
Max step: 19, bias_value: 0.2
Max step: 19, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 19, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [02:27<09:50, 147.58s/it] 20%|██        | 1/5 [02:37<10:31, 157.90s/it] 20%|██        | 1/5 [02:41<10:46, 161.52s/it] 20%|██        | 1/5 [02:50<11:23, 170.96s/it] 40%|████      | 2/5 [04:51<07:17, 145.70s/it] 40%|████      | 2/5 [05:06<07:37, 152.56s/it] 40%|████      | 2/5 [05:17<07:50, 156.81s/it] 40%|████      | 2/5 [05:29<08:15, 165.31s/it] 60%|██████    | 3/5 [07:13<04:48, 144.00s/it] 60%|██████    | 3/5 [07:42<05:08, 154.09s/it] 60%|██████    | 3/5 [07:54<05:13, 156.71s/it] 60%|██████    | 3/5 [08:32<05:47, 173.56s/it] 80%|████████  | 4/5 [09:48<02:28, 148.01s/it] 80%|████████  | 4/5 [10:44<02:45, 165.18s/it] 80%|████████  | 4/5 [11:00<02:48, 168.25s/it] 80%|████████  | 4/5 [11:15<02:49, 169.17s/it]100%|██████████| 5/5 [12:37<00:00, 155.67s/it]100%|██████████| 5/5 [12:37<00:00, 151.47s/it]
100%|██████████| 5/5 [13:26<00:00, 163.94s/it]100%|██████████| 5/5 [13:26<00:00, 161.31s/it]
100%|██████████| 5/5 [13:48<00:00, 168.15s/it]100%|██████████| 5/5 [13:48<00:00, 165.67s/it]
100%|██████████| 5/5 [13:48<00:00, 163.39s/it]100%|██████████| 5/5 [13:48<00:00, 165.69s/it]
[rank0]:[W222 14:54:14.340650168 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 14:54:21.164559 934495 site-packages/torch/distributed/run.py:793] 
W0222 14:54:21.164559 934495 site-packages/torch/distributed/run.py:793] *****************************************
W0222 14:54:21.164559 934495 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 14:54:21.164559 934495 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank2]:[W222 14:54:48.388251485 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 14:54:48.388251551 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 14:54:48.388251502 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 14:55:18.653129386 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.18s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.24s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.25s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_19/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2Max step: 50, bias_value: 0.2

load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_19/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_19/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_19/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:02,  6.47s/it]  5%|▌         | 1/20 [00:07<02:15,  7.14s/it]  5%|▌         | 1/20 [00:07<02:18,  7.27s/it]  5%|▌         | 1/20 [00:07<02:20,  7.42s/it] 10%|█         | 2/20 [00:09<01:24,  4.67s/it] 10%|█         | 2/20 [00:10<01:26,  4.79s/it] 10%|█         | 2/20 [00:10<01:27,  4.84s/it] 10%|█         | 2/20 [00:10<01:27,  4.88s/it] 15%|█▌        | 3/20 [00:12<01:01,  3.62s/it] 15%|█▌        | 3/20 [00:12<01:01,  3.60s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.67s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.74s/it] 20%|██        | 4/20 [00:16<00:58,  3.68s/it] 20%|██        | 4/20 [00:16<00:58,  3.66s/it] 20%|██        | 4/20 [00:16<00:59,  3.70s/it] 20%|██        | 4/20 [00:16<01:01,  3.86s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.48s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.62s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.49s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.70s/it] 30%|███       | 6/20 [00:22<00:48,  3.48s/it] 30%|███       | 6/20 [00:22<00:48,  3.48s/it] 30%|███       | 6/20 [00:23<00:48,  3.49s/it] 30%|███       | 6/20 [00:23<00:51,  3.67s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.18s/it] 35%|███▌      | 7/20 [00:25<00:42,  3.25s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.23s/it] 35%|███▌      | 7/20 [00:26<00:43,  3.35s/it] 40%|████      | 8/20 [00:29<00:39,  3.25s/it] 40%|████      | 8/20 [00:29<00:39,  3.31s/it] 40%|████      | 8/20 [00:29<00:40,  3.41s/it] 40%|████      | 8/20 [00:30<00:40,  3.41s/it] 45%|████▌     | 9/20 [00:32<00:34,  3.17s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.21s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.31s/it] 45%|████▌     | 9/20 [00:33<00:36,  3.30s/it] 50%|█████     | 10/20 [00:35<00:32,  3.26s/it] 50%|█████     | 10/20 [00:36<00:36,  3.69s/it] 50%|█████     | 10/20 [00:36<00:34,  3.45s/it] 50%|█████     | 10/20 [00:37<00:37,  3.75s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.49s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.85s/it] 55%|█████▌    | 11/20 [00:41<00:33,  3.68s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.83s/it] 60%|██████    | 12/20 [00:44<00:31,  3.89s/it] 60%|██████    | 12/20 [00:45<00:33,  4.13s/it] 60%|██████    | 12/20 [00:45<00:32,  4.02s/it] 60%|██████    | 12/20 [00:45<00:33,  4.13s/it] 65%|██████▌   | 13/20 [00:48<00:28,  4.11s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.24s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.18s/it] 65%|██████▌   | 13/20 [00:50<00:30,  4.33s/it] 70%|███████   | 14/20 [00:52<00:24,  4.09s/it] 70%|███████   | 14/20 [00:54<00:24,  4.11s/it] 70%|███████   | 14/20 [00:54<00:25,  4.17s/it] 70%|███████   | 14/20 [00:55<00:26,  4.46s/it] 75%|███████▌  | 15/20 [00:56<00:20,  4.02s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.90s/it] 75%|███████▌  | 15/20 [00:58<00:20,  4.14s/it] 80%|████████  | 16/20 [00:59<00:14,  3.63s/it] 75%|███████▌  | 15/20 [00:59<00:21,  4.40s/it] 80%|████████  | 16/20 [01:01<00:15,  3.80s/it] 80%|████████  | 16/20 [01:02<00:15,  3.93s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.54s/it] 80%|████████  | 16/20 [01:03<00:16,  4.09s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.61s/it] 85%|████████▌ | 17/20 [01:05<00:11,  3.74s/it] 85%|████████▌ | 17/20 [01:06<00:11,  3.80s/it] 90%|█████████ | 18/20 [01:06<00:07,  3.53s/it] 90%|█████████ | 18/20 [01:07<00:07,  3.52s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.63s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.40s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.70s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.48s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.57s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.56s/it]100%|██████████| 20/20 [01:14<00:00,  3.80s/it]100%|██████████| 20/20 [01:14<00:00,  3.71s/it]
100%|██████████| 20/20 [01:15<00:00,  3.72s/it]100%|██████████| 20/20 [01:15<00:00,  3.77s/it]
100%|██████████| 20/20 [01:16<00:00,  3.80s/it]100%|██████████| 20/20 [01:16<00:00,  3.83s/it]
100%|██████████| 20/20 [01:17<00:00,  3.76s/it]100%|██████████| 20/20 [01:17<00:00,  3.86s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 14:58:05,828 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_19/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 14:58:05,828 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 14:58:05,828 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 14:58:05,829 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 14:59:52,518 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 14:59:52,518 - RUN - INFO - Evaluation Results:
2025-02-22 14:59:52,521 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.65
 1  0        cross   0.53
 2  0        single  0.77
 3  1        all     0.605
 4  1        cross   0.41
 5  1        single  0.8
 6  2        all     0.585
 7  2        cross   0.39
 8  2        single  0.78
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.60875
13  Average  cross   0.435
14  Average  single  0.7825
--  -------  ------  -------
[rank0]:[W222 14:59:53.330455987 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 14:59:58.911853 943446 site-packages/torch/distributed/run.py:793] 
W0222 14:59:58.911853 943446 site-packages/torch/distributed/run.py:793] *****************************************
W0222 14:59:58.911853 943446 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 14:59:58.911853 943446 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank3]:[W222 15:00:27.747309062 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 15:00:27.751582465 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 15:00:27.752705981 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 15:00:27.753861712 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.37s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Max step: 21, bias_value: 0.2
Max step: 21, bias_value: 0.2Max step: 21, bias_value: 0.2Max step: 21, bias_value: 0.2


Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [02:31<10:04, 151.12s/it] 20%|██        | 1/5 [02:46<11:04, 166.15s/it] 20%|██        | 1/5 [02:50<11:20, 170.06s/it] 20%|██        | 1/5 [02:56<11:44, 176.10s/it] 40%|████      | 2/5 [05:02<07:33, 151.25s/it] 40%|████      | 2/5 [05:22<08:00, 160.29s/it] 40%|████      | 2/5 [05:26<08:03, 161.22s/it] 40%|████      | 2/5 [05:45<08:40, 173.35s/it] 60%|██████    | 3/5 [07:28<04:58, 149.08s/it] 60%|██████    | 3/5 [08:08<05:25, 162.95s/it] 60%|██████    | 3/5 [08:12<05:25, 162.99s/it] 60%|██████    | 3/5 [08:58<06:04, 182.12s/it] 80%|████████  | 4/5 [10:10<02:33, 154.00s/it] 80%|████████  | 4/5 [11:20<02:54, 174.60s/it] 80%|████████  | 4/5 [11:30<02:56, 176.95s/it] 80%|████████  | 4/5 [11:49<02:57, 177.64s/it]100%|██████████| 5/5 [13:11<00:00, 163.86s/it]100%|██████████| 5/5 [13:11<00:00, 158.37s/it]
100%|██████████| 5/5 [14:10<00:00, 172.96s/it]100%|██████████| 5/5 [14:10<00:00, 170.19s/it]
100%|██████████| 5/5 [14:28<00:00, 177.34s/it]100%|██████████| 5/5 [14:28<00:00, 173.67s/it]
100%|██████████| 5/5 [14:29<00:00, 171.36s/it]100%|██████████| 5/5 [14:29<00:00, 173.85s/it]
[rank0]:[W222 15:16:29.969191089 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 15:16:36.596539 974009 site-packages/torch/distributed/run.py:793] 
W0222 15:16:36.596539 974009 site-packages/torch/distributed/run.py:793] *****************************************
W0222 15:16:36.596539 974009 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 15:16:36.596539 974009 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank3]:[W222 15:17:03.167935739 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 15:17:03.168975754 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 15:17:03.170109838 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 15:17:32.121824956 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.43s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.46s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.41s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_21/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_21/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_21/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_21/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:02,  6.45s/it]  5%|▌         | 1/20 [00:07<02:14,  7.10s/it]  5%|▌         | 1/20 [00:07<02:13,  7.02s/it]  5%|▌         | 1/20 [00:07<02:14,  7.08s/it] 10%|█         | 2/20 [00:09<01:24,  4.69s/it] 10%|█         | 2/20 [00:10<01:24,  4.71s/it] 10%|█         | 2/20 [00:10<01:25,  4.74s/it] 10%|█         | 2/20 [00:10<01:26,  4.80s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.73s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.68s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.76s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.76s/it] 20%|██        | 4/20 [00:16<00:58,  3.67s/it] 20%|██        | 4/20 [00:16<01:00,  3.78s/it] 20%|██        | 4/20 [00:16<01:00,  3.77s/it] 20%|██        | 4/20 [00:16<01:01,  3.86s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.48s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.54s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.62s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.69s/it] 30%|███       | 6/20 [00:22<00:48,  3.47s/it] 30%|███       | 6/20 [00:22<00:49,  3.50s/it] 30%|███       | 6/20 [00:23<00:49,  3.52s/it] 30%|███       | 6/20 [00:23<00:51,  3.64s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.19s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.20s/it] 35%|███▌      | 7/20 [00:25<00:42,  3.27s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.30s/it] 40%|████      | 8/20 [00:28<00:39,  3.26s/it] 40%|████      | 8/20 [00:29<00:40,  3.37s/it] 40%|████      | 8/20 [00:29<00:40,  3.39s/it] 40%|████      | 8/20 [00:30<00:41,  3.43s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.17s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.23s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.29s/it] 45%|████▌     | 9/20 [00:33<00:36,  3.31s/it] 50%|█████     | 10/20 [00:35<00:33,  3.31s/it] 50%|█████     | 10/20 [00:36<00:36,  3.68s/it] 50%|█████     | 10/20 [00:37<00:36,  3.69s/it] 50%|█████     | 10/20 [00:37<00:35,  3.55s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.53s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.77s/it] 55%|█████▌    | 11/20 [00:41<00:33,  3.67s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.81s/it] 60%|██████    | 12/20 [00:44<00:31,  3.93s/it] 60%|██████    | 12/20 [00:45<00:32,  4.02s/it] 60%|██████    | 12/20 [00:46<00:32,  4.03s/it] 60%|██████    | 12/20 [00:46<00:33,  4.17s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.10s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.12s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.17s/it] 65%|██████▌   | 13/20 [00:50<00:30,  4.36s/it] 70%|███████   | 14/20 [00:53<00:24,  4.07s/it] 70%|███████   | 14/20 [00:53<00:24,  4.00s/it] 70%|███████   | 14/20 [00:54<00:24,  4.15s/it] 70%|███████   | 14/20 [00:55<00:26,  4.44s/it] 75%|███████▌  | 15/20 [00:56<00:18,  3.75s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.96s/it] 75%|███████▌  | 15/20 [00:58<00:19,  3.95s/it] 80%|████████  | 16/20 [00:59<00:14,  3.57s/it] 75%|███████▌  | 15/20 [00:59<00:22,  4.42s/it] 80%|████████  | 16/20 [00:59<00:14,  3.65s/it] 80%|████████  | 16/20 [01:01<00:15,  3.76s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.49s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.50s/it] 80%|████████  | 16/20 [01:03<00:16,  4.07s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.60s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.47s/it] 85%|████████▌ | 17/20 [01:06<00:11,  3.77s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.42s/it] 90%|█████████ | 18/20 [01:07<00:07,  3.51s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.36s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.38s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.65s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.48s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.51s/it]100%|██████████| 20/20 [01:13<00:00,  3.59s/it]100%|██████████| 20/20 [01:13<00:00,  3.68s/it]
100%|██████████| 20/20 [01:13<00:00,  3.71s/it]100%|██████████| 20/20 [01:13<00:00,  3.70s/it]
100%|██████████| 20/20 [01:15<00:00,  3.69s/it]100%|██████████| 20/20 [01:15<00:00,  3.78s/it]
100%|██████████| 20/20 [01:17<00:00,  3.80s/it]100%|██████████| 20/20 [01:17<00:00,  3.87s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 15:20:24,171 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_21/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 15:20:24,171 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 15:20:24,172 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 15:20:24,172 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 15:22:15,695 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 15:22:15,695 - RUN - INFO - Evaluation Results:
2025-02-22 15:22:15,697 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.65
 1  0        cross   0.54
 2  0        single  0.76
 3  1        all     0.6
 4  1        cross   0.41
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.39
 8  2        single  0.77
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.60625
13  Average  cross   0.4375
14  Average  single  0.775
--  -------  ------  -------
[rank0]:[W222 15:22:16.491809481 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 15:22:22.173595 983695 site-packages/torch/distributed/run.py:793] 
W0222 15:22:22.173595 983695 site-packages/torch/distributed/run.py:793] *****************************************
W0222 15:22:22.173595 983695 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 15:22:22.173595 983695 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank2]:[W222 15:22:51.146203805 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 15:22:51.147370552 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 15:22:51.158580572 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 15:22:51.172048382 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.45s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.41s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.45s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]
Max step: 23, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 23, bias_value: 0.2
Max step: 23, bias_value: 0.2
Max step: 23, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [02:37<10:30, 157.68s/it] 20%|██        | 1/5 [02:51<11:26, 171.65s/it] 20%|██        | 1/5 [02:59<11:56, 179.22s/it] 20%|██        | 1/5 [03:02<12:10, 182.52s/it] 40%|████      | 2/5 [05:14<07:51, 157.33s/it] 40%|████      | 2/5 [05:36<08:22, 167.54s/it] 40%|████      | 2/5 [05:39<08:22, 167.52s/it] 40%|████      | 2/5 [05:56<08:54, 178.10s/it] 60%|██████    | 3/5 [07:47<05:10, 155.32s/it] 60%|██████    | 3/5 [08:36<05:43, 171.62s/it] 60%|██████    | 3/5 [08:36<05:47, 173.50s/it] 60%|██████    | 3/5 [09:15<06:15, 187.76s/it] 80%|████████  | 4/5 [10:32<02:38, 158.89s/it] 80%|████████  | 4/5 [11:57<03:04, 184.25s/it] 80%|████████  | 4/5 [12:05<03:06, 186.66s/it] 80%|████████  | 4/5 [12:15<03:04, 184.58s/it]100%|██████████| 5/5 [13:42<00:00, 170.33s/it]100%|██████████| 5/5 [13:42<00:00, 164.53s/it]
100%|██████████| 5/5 [14:55<00:00, 181.95s/it]100%|██████████| 5/5 [14:55<00:00, 179.10s/it]
100%|██████████| 5/5 [15:01<00:00, 177.89s/it]100%|██████████| 5/5 [15:01<00:00, 180.31s/it]
100%|██████████| 5/5 [15:08<00:00, 185.20s/it]100%|██████████| 5/5 [15:08<00:00, 181.68s/it]
[rank0]:[W222 15:39:33.441716684 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 15:39:40.373188 1015224 site-packages/torch/distributed/run.py:793] 
W0222 15:39:40.373188 1015224 site-packages/torch/distributed/run.py:793] *****************************************
W0222 15:39:40.373188 1015224 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 15:39:40.373188 1015224 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank2]:[W222 15:40:08.434528818 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 15:40:08.434970965 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 15:40:08.435281291 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 15:40:38.939404555 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.22s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.23s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.25s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_23/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_23/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_23/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_23/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json


You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:03,  6.49s/it]  5%|▌         | 1/20 [00:06<02:11,  6.92s/it]  5%|▌         | 1/20 [00:07<02:16,  7.17s/it]  5%|▌         | 1/20 [00:07<02:18,  7.26s/it] 10%|█         | 2/20 [00:10<01:25,  4.76s/it] 10%|█         | 2/20 [00:10<01:25,  4.76s/it] 10%|█         | 2/20 [00:10<01:27,  4.84s/it] 10%|█         | 2/20 [00:10<01:25,  4.77s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.74s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.69s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.78s/it] 15%|█▌        | 3/20 [00:12<01:05,  3.83s/it] 20%|██        | 4/20 [00:16<01:00,  3.76s/it] 20%|██        | 4/20 [00:16<00:59,  3.75s/it] 20%|██        | 4/20 [00:16<00:59,  3.72s/it] 20%|██        | 4/20 [00:16<01:01,  3.85s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.50s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.61s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.54s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.71s/it] 30%|███       | 6/20 [00:22<00:48,  3.43s/it] 30%|███       | 6/20 [00:22<00:47,  3.42s/it] 30%|███       | 6/20 [00:23<00:48,  3.48s/it] 30%|███       | 6/20 [00:23<00:49,  3.55s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.13s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.21s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.19s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.26s/it] 40%|████      | 8/20 [00:29<00:39,  3.32s/it] 40%|████      | 8/20 [00:29<00:39,  3.25s/it] 40%|████      | 8/20 [00:29<00:40,  3.39s/it] 40%|████      | 8/20 [00:29<00:40,  3.41s/it] 45%|████▌     | 9/20 [00:31<00:35,  3.20s/it] 45%|████▌     | 9/20 [00:32<00:34,  3.17s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.28s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.28s/it] 50%|█████     | 10/20 [00:35<00:32,  3.28s/it] 50%|█████     | 10/20 [00:36<00:34,  3.42s/it] 50%|█████     | 10/20 [00:36<00:36,  3.64s/it] 50%|█████     | 10/20 [00:37<00:37,  3.76s/it] 55%|█████▌    | 11/20 [00:39<00:32,  3.58s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.58s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.78s/it] 55%|█████▌    | 11/20 [00:41<00:35,  3.93s/it] 60%|██████    | 12/20 [00:44<00:31,  3.93s/it] 60%|██████    | 12/20 [00:45<00:31,  3.94s/it] 60%|██████    | 12/20 [00:45<00:32,  4.08s/it] 60%|██████    | 12/20 [00:46<00:33,  4.23s/it] 65%|██████▌   | 13/20 [00:48<00:28,  4.10s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.09s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.18s/it] 65%|██████▌   | 13/20 [00:51<00:30,  4.39s/it] 70%|███████   | 14/20 [00:52<00:24,  4.07s/it] 70%|███████   | 14/20 [00:53<00:24,  4.06s/it] 70%|███████   | 14/20 [00:54<00:24,  4.13s/it] 70%|███████   | 14/20 [00:55<00:26,  4.44s/it] 75%|███████▌  | 15/20 [00:56<00:20,  4.03s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.80s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.92s/it] 80%|████████  | 16/20 [00:59<00:14,  3.60s/it] 75%|███████▌  | 15/20 [01:00<00:22,  4.41s/it] 80%|████████  | 16/20 [01:00<00:15,  3.75s/it] 80%|████████  | 16/20 [01:00<00:14,  3.74s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.48s/it] 80%|████████  | 16/20 [01:03<00:16,  4.05s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.58s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.61s/it] 90%|█████████ | 18/20 [01:05<00:06,  3.40s/it] 85%|████████▌ | 17/20 [01:06<00:11,  3.76s/it] 90%|█████████ | 18/20 [01:07<00:06,  3.45s/it] 90%|█████████ | 18/20 [01:07<00:06,  3.47s/it] 95%|█████████▌| 19/20 [01:08<00:03,  3.33s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.63s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.41s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.44s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.49s/it]100%|██████████| 20/20 [01:13<00:00,  3.72s/it]100%|██████████| 20/20 [01:13<00:00,  3.68s/it]
100%|██████████| 20/20 [01:14<00:00,  3.67s/it]100%|██████████| 20/20 [01:14<00:00,  3.73s/it]
100%|██████████| 20/20 [01:14<00:00,  3.68s/it]100%|██████████| 20/20 [01:14<00:00,  3.74s/it]
100%|██████████| 20/20 [01:17<00:00,  3.79s/it]100%|██████████| 20/20 [01:17<00:00,  3.87s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 15:43:27,768 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_23/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 15:43:27,768 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 15:43:27,768 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 15:43:27,768 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 15:45:40,884 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 15:45:40,884 - RUN - INFO - Evaluation Results:
2025-02-22 15:45:40,886 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.65
 1  0        cross   0.54
 2  0        single  0.76
 3  1        all     0.6
 4  1        cross   0.41
 5  1        single  0.79
 6  2        all     0.575
 7  2        cross   0.39
 8  2        single  0.76
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.605
13  Average  cross   0.4375
14  Average  single  0.7725
--  -------  ------  ------
[rank0]:[W222 15:45:41.685375316 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 15:45:47.356796 1024678 site-packages/torch/distributed/run.py:793] 
W0222 15:45:47.356796 1024678 site-packages/torch/distributed/run.py:793] *****************************************
W0222 15:45:47.356796 1024678 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 15:45:47.356796 1024678 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank3]:[W222 15:46:14.970959526 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 15:46:14.970976897 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 15:46:14.971161865 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 15:46:14.973683946 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.53s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.46s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]
Max step: 25, bias_value: 0.2
Max step: 25, bias_value: 0.2Max step: 25, bias_value: 0.2Max step: 25, bias_value: 0.2


Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [02:43<10:53, 163.25s/it] 20%|██        | 1/5 [02:59<11:56, 179.06s/it] 20%|██        | 1/5 [03:06<12:25, 186.38s/it] 20%|██        | 1/5 [03:12<12:48, 192.07s/it] 40%|████      | 2/5 [05:29<08:14, 164.91s/it] 40%|████      | 2/5 [05:53<08:48, 176.32s/it] 40%|████      | 2/5 [05:53<08:42, 174.22s/it] 40%|████      | 2/5 [06:06<09:08, 182.71s/it] 60%|██████    | 3/5 [08:05<05:21, 160.71s/it] 60%|██████    | 3/5 [08:52<05:52, 176.30s/it] 60%|██████    | 3/5 [08:55<05:58, 179.08s/it] 60%|██████    | 3/5 [09:32<06:26, 193.33s/it] 80%|████████  | 4/5 [10:58<02:45, 165.88s/it] 80%|████████  | 4/5 [12:28<03:12, 192.18s/it] 80%|████████  | 4/5 [12:34<03:14, 194.14s/it] 80%|████████  | 4/5 [12:39<03:10, 190.71s/it]100%|██████████| 5/5 [14:18<00:00, 178.14s/it]100%|██████████| 5/5 [14:18<00:00, 171.75s/it]
100%|██████████| 5/5 [15:27<00:00, 182.75s/it]100%|██████████| 5/5 [15:27<00:00, 185.56s/it]
100%|██████████| 5/5 [15:31<00:00, 188.94s/it]100%|██████████| 5/5 [15:31<00:00, 186.26s/it]
100%|██████████| 5/5 [15:41<00:00, 191.60s/it]100%|██████████| 5/5 [15:41<00:00, 188.23s/it]
[rank0]:[W222 16:03:29.385840895 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 16:03:36.118019 1057203 site-packages/torch/distributed/run.py:793] 
W0222 16:03:36.118019 1057203 site-packages/torch/distributed/run.py:793] *****************************************
W0222 16:03:36.118019 1057203 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 16:03:36.118019 1057203 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank3]:[W222 16:04:02.027250405 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 16:04:02.028131760 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 16:04:02.030936337 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 16:04:33.447241334 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.60s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.56s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.61s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_25/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2Max step: 50, bias_value: 0.2

Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_25/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_25/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_25/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:01,  6.39s/it]  5%|▌         | 1/20 [00:06<02:12,  6.96s/it]  5%|▌         | 1/20 [00:06<02:09,  6.79s/it]  5%|▌         | 1/20 [00:07<02:15,  7.13s/it] 10%|█         | 2/20 [00:09<01:22,  4.61s/it] 10%|█         | 2/20 [00:10<01:23,  4.66s/it] 10%|█         | 2/20 [00:09<01:23,  4.62s/it] 10%|█         | 2/20 [00:10<01:26,  4.81s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.68s/it] 15%|█▌        | 3/20 [00:12<01:01,  3.59s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.73s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.74s/it] 20%|██        | 4/20 [00:16<00:59,  3.74s/it] 20%|██        | 4/20 [00:16<00:58,  3.66s/it] 20%|██        | 4/20 [00:16<00:59,  3.70s/it] 20%|██        | 4/20 [00:16<01:00,  3.80s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.48s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.50s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.62s/it] 25%|██▌       | 5/20 [00:20<00:54,  3.65s/it] 30%|███       | 6/20 [00:22<00:47,  3.40s/it] 30%|███       | 6/20 [00:22<00:47,  3.42s/it] 30%|███       | 6/20 [00:22<00:47,  3.41s/it] 30%|███       | 6/20 [00:23<00:49,  3.51s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.18s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.18s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.13s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.20s/it] 40%|████      | 8/20 [00:28<00:39,  3.26s/it] 40%|████      | 8/20 [00:29<00:40,  3.38s/it] 40%|████      | 8/20 [00:28<00:39,  3.33s/it] 40%|████      | 8/20 [00:29<00:40,  3.35s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.17s/it] 45%|████▌     | 9/20 [00:31<00:35,  3.23s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.27s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.24s/it] 50%|█████     | 10/20 [00:35<00:32,  3.28s/it] 50%|█████     | 10/20 [00:36<00:33,  3.35s/it] 50%|█████     | 10/20 [00:36<00:36,  3.67s/it] 50%|█████     | 10/20 [00:36<00:36,  3.69s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.50s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.56s/it] 55%|█████▌    | 11/20 [00:40<00:34,  3.79s/it] 55%|█████▌    | 11/20 [00:40<00:34,  3.80s/it] 60%|██████    | 12/20 [00:44<00:31,  3.92s/it] 60%|██████    | 12/20 [00:44<00:31,  3.91s/it] 60%|██████    | 12/20 [00:45<00:32,  4.05s/it] 60%|██████    | 12/20 [00:45<00:32,  4.08s/it] 65%|██████▌   | 13/20 [00:48<00:28,  4.07s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.04s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.15s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.26s/it] 70%|███████   | 14/20 [00:52<00:24,  4.05s/it] 70%|███████   | 14/20 [00:53<00:24,  4.02s/it] 70%|███████   | 14/20 [00:53<00:24,  4.06s/it] 70%|███████   | 14/20 [00:54<00:25,  4.33s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.93s/it] 75%|███████▌  | 15/20 [00:56<00:18,  3.78s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.89s/it] 75%|███████▌  | 15/20 [00:58<00:21,  4.24s/it] 80%|████████  | 16/20 [00:58<00:14,  3.55s/it] 80%|████████  | 16/20 [01:00<00:14,  3.68s/it] 80%|████████  | 16/20 [01:00<00:14,  3.71s/it] 80%|████████  | 16/20 [01:01<00:15,  3.92s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.45s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.53s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.59s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.65s/it] 90%|█████████ | 18/20 [01:05<00:06,  3.38s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.38s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.45s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.52s/it] 95%|█████████▌| 19/20 [01:08<00:03,  3.28s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.35s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.41s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.40s/it]100%|██████████| 20/20 [01:12<00:00,  3.68s/it]100%|██████████| 20/20 [01:12<00:00,  3.65s/it]
100%|██████████| 20/20 [01:13<00:00,  3.61s/it]100%|██████████| 20/20 [01:13<00:00,  3.69s/it]
100%|██████████| 20/20 [01:13<00:00,  3.62s/it]100%|██████████| 20/20 [01:13<00:00,  3.70s/it]
100%|██████████| 20/20 [01:15<00:00,  3.65s/it]100%|██████████| 20/20 [01:15<00:00,  3.77s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 16:07:24,314 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_25/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 16:07:24,314 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 16:07:24,314 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 16:07:24,314 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 16:09:55,239 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 16:09:55,240 - RUN - INFO - Evaluation Results:
2025-02-22 16:09:55,242 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.65
 1  0        single  0.76
 2  0        cross   0.54
 3  1        all     0.6
 4  1        single  0.79
 5  1        cross   0.41
 6  2        all     0.575
 7  2        single  0.76
 8  2        cross   0.39
 9  3        all     0.595
10  3        single  0.78
11  3        cross   0.41
12  Average  all     0.605
13  Average  single  0.7725
14  Average  cross   0.4375
--  -------  ------  ------
[rank0]:[W222 16:09:55.029860984 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 16:10:02.101354 1066867 site-packages/torch/distributed/run.py:793] 
W0222 16:10:02.101354 1066867 site-packages/torch/distributed/run.py:793] *****************************************
W0222 16:10:02.101354 1066867 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 16:10:02.101354 1066867 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank0]:[W222 16:10:29.829822232 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 16:10:29.833462442 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 16:10:29.836730938 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 16:10:29.858483782 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.70it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.13s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.14s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.54it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]
Max step: 27, bias_value: 0.2
Max step: 27, bias_value: 0.2Max step: 27, bias_value: 0.2Max step: 27, bias_value: 0.2


Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [02:48<11:13, 168.36s/it] 20%|██        | 1/5 [03:05<12:20, 185.18s/it] 20%|██        | 1/5 [03:18<13:14, 198.65s/it] 20%|██        | 1/5 [03:19<13:19, 199.82s/it] 40%|████      | 2/5 [05:40<08:32, 170.71s/it] 40%|████      | 2/5 [06:07<09:10, 183.49s/it] 40%|████      | 2/5 [06:12<09:12, 184.04s/it] 40%|████      | 2/5 [06:22<09:29, 189.91s/it] 60%|██████    | 3/5 [08:21<05:32, 166.39s/it] 60%|██████    | 3/5 [09:18<06:13, 186.95s/it] 60%|██████    | 3/5 [09:19<06:10, 185.17s/it] 60%|██████    | 3/5 [09:56<06:41, 200.93s/it] 80%|████████  | 4/5 [11:17<02:50, 170.04s/it] 80%|████████  | 4/5 [12:57<03:19, 199.57s/it] 80%|████████  | 4/5 [13:12<03:18, 198.95s/it] 80%|████████  | 4/5 [13:13<03:24, 204.55s/it]100%|██████████| 5/5 [14:46<00:00, 184.04s/it]100%|██████████| 5/5 [14:46<00:00, 177.29s/it]
100%|██████████| 5/5 [16:03<00:00, 188.79s/it]100%|██████████| 5/5 [16:03<00:00, 192.63s/it]
100%|██████████| 5/5 [16:06<00:00, 195.78s/it]100%|██████████| 5/5 [16:06<00:00, 193.31s/it]
100%|██████████| 5/5 [16:26<00:00, 200.35s/it]100%|██████████| 5/5 [16:26<00:00, 197.30s/it]
[rank0]:[W222 16:28:28.833668827 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 16:28:35.332813 1099650 site-packages/torch/distributed/run.py:793] 
W0222 16:28:35.332813 1099650 site-packages/torch/distributed/run.py:793] *****************************************
W0222 16:28:35.332813 1099650 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 16:28:35.332813 1099650 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W222 16:29:01.722970475 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 16:29:01.723398966 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 16:29:01.736321523 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 16:29:31.141446689 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.40s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.39s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_27/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_27/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_27/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_27/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:04,  6.56s/it]  5%|▌         | 1/20 [00:07<02:16,  7.20s/it]  5%|▌         | 1/20 [00:07<02:17,  7.25s/it]  5%|▌         | 1/20 [00:07<02:19,  7.33s/it] 10%|█         | 2/20 [00:09<01:24,  4.69s/it] 10%|█         | 2/20 [00:10<01:26,  4.79s/it] 10%|█         | 2/20 [00:10<01:26,  4.81s/it] 10%|█         | 2/20 [00:10<01:27,  4.86s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.72s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.69s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.79s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.78s/it] 20%|██        | 4/20 [00:16<01:00,  3.78s/it] 20%|██        | 4/20 [00:16<00:59,  3.72s/it] 20%|██        | 4/20 [00:16<01:00,  3.78s/it] 20%|██        | 4/20 [00:16<01:01,  3.85s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.64s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.53s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.54s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.67s/it] 30%|███       | 6/20 [00:22<00:48,  3.45s/it] 30%|███       | 6/20 [00:22<00:47,  3.42s/it] 30%|███       | 6/20 [00:23<00:48,  3.49s/it] 30%|███       | 6/20 [00:23<00:49,  3.51s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.14s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.23s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.22s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.19s/it] 40%|████      | 8/20 [00:29<00:40,  3.39s/it] 40%|████      | 8/20 [00:29<00:40,  3.34s/it] 40%|████      | 8/20 [00:29<00:39,  3.29s/it] 40%|████      | 8/20 [00:29<00:40,  3.36s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.23s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.30s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.22s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.25s/it] 50%|█████     | 10/20 [00:35<00:32,  3.29s/it] 50%|█████     | 10/20 [00:36<00:34,  3.40s/it] 50%|█████     | 10/20 [00:37<00:37,  3.76s/it] 50%|█████     | 10/20 [00:37<00:38,  3.85s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.50s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.57s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.89s/it] 55%|█████▌    | 11/20 [00:41<00:35,  3.93s/it] 60%|██████    | 12/20 [00:44<00:31,  3.91s/it] 60%|██████    | 12/20 [00:45<00:31,  3.92s/it] 60%|██████    | 12/20 [00:46<00:33,  4.15s/it] 60%|██████    | 12/20 [00:46<00:33,  4.21s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.10s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.06s/it] 65%|██████▌   | 13/20 [00:50<00:30,  4.34s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.24s/it] 70%|███████   | 14/20 [00:53<00:24,  4.10s/it] 70%|███████   | 14/20 [00:53<00:24,  4.06s/it] 70%|███████   | 14/20 [00:54<00:24,  4.14s/it] 70%|███████   | 14/20 [00:55<00:26,  4.43s/it] 75%|███████▌  | 15/20 [00:57<00:20,  4.06s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.90s/it] 75%|███████▌  | 15/20 [00:58<00:19,  3.91s/it] 75%|███████▌  | 15/20 [00:59<00:21,  4.34s/it] 80%|████████  | 16/20 [00:59<00:14,  3.61s/it] 80%|████████  | 16/20 [01:00<00:14,  3.70s/it] 80%|████████  | 16/20 [01:01<00:15,  3.83s/it] 80%|████████  | 16/20 [01:02<00:15,  3.99s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.53s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.55s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.62s/it] 85%|████████▌ | 17/20 [01:05<00:11,  3.71s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.48s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.41s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.51s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.56s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.37s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.38s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.46s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.45s/it]100%|██████████| 20/20 [01:14<00:00,  3.74s/it]100%|██████████| 20/20 [01:14<00:00,  3.70s/it]
100%|██████████| 20/20 [01:14<00:00,  3.63s/it]100%|██████████| 20/20 [01:14<00:00,  3.71s/it]
100%|██████████| 20/20 [01:15<00:00,  3.66s/it]100%|██████████| 20/20 [01:15<00:00,  3.78s/it]
100%|██████████| 20/20 [01:16<00:00,  3.73s/it]100%|██████████| 20/20 [01:16<00:00,  3.84s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 16:32:21,884 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_27/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 16:32:21,884 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 16:32:21,884 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 16:32:21,884 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 16:34:43,569 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 16:34:43,569 - RUN - INFO - Evaluation Results:
2025-02-22 16:34:43,572 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        single  0.76
 2  0        cross   0.55
 3  1        all     0.605
 4  1        single  0.79
 5  1        cross   0.42
 6  2        all     0.58
 7  2        single  0.76
 8  2        cross   0.4
 9  3        all     0.595
10  3        single  0.78
11  3        cross   0.41
12  Average  all     0.60875
13  Average  single  0.7725
14  Average  cross   0.445
--  -------  ------  -------
[rank0]:[W222 16:34:44.424180066 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 16:34:50.083159 1113425 site-packages/torch/distributed/run.py:793] 
W0222 16:34:50.083159 1113425 site-packages/torch/distributed/run.py:793] *****************************************
W0222 16:34:50.083159 1113425 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 16:34:50.083159 1113425 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank1]:[W222 16:35:16.757884792 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 16:35:16.758060530 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 16:35:16.773273937 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 16:35:16.780185211 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.45it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.35it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.37it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]
Max step: 29, bias_value: 0.2
Max step: 29, bias_value: 0.2
Max step: 29, bias_value: 0.2
Max step: 29, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:01<12:05, 181.49s/it] 20%|██        | 1/5 [03:11<12:46, 191.61s/it] 20%|██        | 1/5 [03:25<13:43, 205.90s/it] 20%|██        | 1/5 [03:30<14:01, 210.38s/it] 40%|████      | 2/5 [05:57<08:55, 178.34s/it] 40%|████      | 2/5 [06:18<09:26, 188.96s/it] 40%|████      | 2/5 [06:31<09:41, 193.67s/it] 40%|████      | 2/5 [06:31<09:38, 192.95s/it] 60%|██████    | 3/5 [08:49<05:51, 175.61s/it] 60%|██████    | 3/5 [09:33<06:23, 191.63s/it] 60%|██████    | 3/5 [09:45<06:27, 193.71s/it] 60%|██████    | 3/5 [10:12<06:52, 206.23s/it] 80%|████████  | 4/5 [11:55<02:59, 179.53s/it] 80%|████████  | 4/5 [13:18<03:24, 204.79s/it] 80%|████████  | 4/5 [13:36<03:25, 205.34s/it] 80%|████████  | 4/5 [13:49<03:33, 213.44s/it]100%|██████████| 5/5 [15:30<00:00, 192.41s/it]100%|██████████| 5/5 [15:30<00:00, 186.15s/it]
100%|██████████| 5/5 [16:31<00:00, 194.70s/it]100%|██████████| 5/5 [16:31<00:00, 198.40s/it]
100%|██████████| 5/5 [16:32<00:00, 200.79s/it]100%|██████████| 5/5 [16:32<00:00, 198.44s/it]
100%|██████████| 5/5 [17:06<00:00, 207.47s/it]100%|██████████| 5/5 [17:06<00:00, 205.26s/it]
[rank0]:[W222 16:53:54.181509628 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 16:54:00.833033 1150831 site-packages/torch/distributed/run.py:793] 
W0222 16:54:00.833033 1150831 site-packages/torch/distributed/run.py:793] *****************************************
W0222 16:54:00.833033 1150831 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 16:54:00.833033 1150831 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W222 16:54:29.677425428 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 16:54:29.687463941 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 16:54:29.691461847 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 16:54:59.890784318 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.48s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.54s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.59s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_29/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_29/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_29/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_29/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:00,  6.35s/it]  5%|▌         | 1/20 [00:07<02:16,  7.18s/it]  5%|▌         | 1/20 [00:07<02:16,  7.20s/it]  5%|▌         | 1/20 [00:07<02:17,  7.24s/it] 10%|█         | 2/20 [00:09<01:23,  4.66s/it] 10%|█         | 2/20 [00:10<01:25,  4.77s/it] 10%|█         | 2/20 [00:10<01:25,  4.77s/it] 10%|█         | 2/20 [00:10<01:27,  4.88s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.70s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.73s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.71s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.76s/it] 20%|██        | 4/20 [00:16<00:58,  3.65s/it] 20%|██        | 4/20 [00:16<01:00,  3.77s/it] 20%|██        | 4/20 [00:16<01:00,  3.79s/it] 20%|██        | 4/20 [00:16<01:02,  3.89s/it] 25%|██▌       | 5/20 [00:19<00:51,  3.43s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.51s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.64s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.72s/it] 30%|███       | 6/20 [00:22<00:47,  3.38s/it] 30%|███       | 6/20 [00:22<00:48,  3.43s/it] 30%|███       | 6/20 [00:22<00:48,  3.46s/it] 30%|███       | 6/20 [00:23<00:49,  3.55s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.09s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.21s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.16s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.25s/it] 40%|████      | 8/20 [00:28<00:38,  3.19s/it] 40%|████      | 8/20 [00:29<00:40,  3.38s/it] 40%|████      | 8/20 [00:29<00:40,  3.37s/it] 40%|████      | 8/20 [00:29<00:40,  3.39s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.12s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.27s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.23s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.28s/it] 50%|█████     | 10/20 [00:35<00:32,  3.28s/it] 50%|█████     | 10/20 [00:36<00:36,  3.60s/it] 50%|█████     | 10/20 [00:36<00:36,  3.70s/it] 50%|█████     | 10/20 [00:36<00:34,  3.48s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.50s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.72s/it] 55%|█████▌    | 11/20 [00:40<00:34,  3.81s/it] 55%|█████▌    | 11/20 [00:41<00:33,  3.72s/it] 60%|██████    | 12/20 [00:44<00:31,  3.97s/it] 60%|██████    | 12/20 [00:44<00:31,  3.92s/it] 60%|██████    | 12/20 [00:45<00:32,  4.12s/it] 60%|██████    | 12/20 [00:46<00:32,  4.07s/it] 65%|██████▌   | 13/20 [00:48<00:28,  4.07s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.12s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.19s/it] 65%|██████▌   | 13/20 [00:50<00:30,  4.35s/it] 70%|███████   | 14/20 [00:52<00:23,  3.95s/it] 70%|███████   | 14/20 [00:53<00:24,  4.09s/it] 70%|███████   | 14/20 [00:54<00:25,  4.17s/it] 70%|███████   | 14/20 [00:55<00:26,  4.40s/it] 75%|███████▌  | 15/20 [00:55<00:18,  3.71s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.95s/it] 75%|███████▌  | 15/20 [00:58<00:19,  3.96s/it] 80%|████████  | 16/20 [00:59<00:14,  3.59s/it] 75%|███████▌  | 15/20 [00:59<00:21,  4.37s/it] 80%|████████  | 16/20 [00:59<00:14,  3.56s/it] 80%|████████  | 16/20 [01:01<00:15,  3.77s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.49s/it] 80%|████████  | 16/20 [01:02<00:16,  4.04s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.48s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.60s/it] 90%|█████████ | 18/20 [01:05<00:06,  3.36s/it] 85%|████████▌ | 17/20 [01:05<00:11,  3.73s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.42s/it] 90%|█████████ | 18/20 [01:07<00:06,  3.49s/it] 95%|█████████▌| 19/20 [01:08<00:03,  3.33s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.58s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.34s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.45s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.45s/it]100%|██████████| 20/20 [01:12<00:00,  3.57s/it]100%|██████████| 20/20 [01:12<00:00,  3.64s/it]
100%|██████████| 20/20 [01:13<00:00,  3.74s/it]100%|██████████| 20/20 [01:13<00:00,  3.69s/it]
100%|██████████| 20/20 [01:15<00:00,  3.65s/it]100%|██████████| 20/20 [01:15<00:00,  3.77s/it]
100%|██████████| 20/20 [01:16<00:00,  3.75s/it]100%|██████████| 20/20 [01:16<00:00,  3.83s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 16:57:48,882 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_29/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 16:57:48,883 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 16:57:48,883 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 16:57:48,883 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 16:59:45,157 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 16:59:45,157 - RUN - INFO - Evaluation Results:
2025-02-22 16:59:45,159 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.60875
13  Average  cross   0.445
14  Average  single  0.7725
--  -------  ------  -------
[rank0]:[W222 16:59:45.961656696 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 16:59:51.604702 1159743 site-packages/torch/distributed/run.py:793] 
W0222 16:59:51.604702 1159743 site-packages/torch/distributed/run.py:793] *****************************************
W0222 16:59:51.604702 1159743 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 16:59:51.604702 1159743 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank2]:[W222 17:00:21.560052729 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 17:00:21.562729403 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 17:00:21.570117845 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 17:00:21.591816093 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Max step: 31, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]Max step: 31, bias_value: 0.2
Max step: 31, bias_value: 0.2Max step: 31, bias_value: 0.2

You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:18<13:15, 198.93s/it] 20%|██        | 1/5 [03:08<12:35, 188.83s/it] 20%|██        | 1/5 [03:33<14:13, 213.36s/it] 20%|██        | 1/5 [03:37<14:30, 217.51s/it] 40%|████      | 2/5 [06:10<09:14, 184.80s/it] 40%|████      | 2/5 [06:33<09:48, 196.21s/it] 40%|████      | 2/5 [06:42<09:54, 198.25s/it] 40%|████      | 2/5 [06:44<10:00, 200.26s/it] 60%|██████    | 3/5 [09:09<06:03, 181.83s/it] 60%|██████    | 3/5 [09:55<06:37, 198.93s/it] 60%|██████    | 3/5 [10:03<06:39, 199.62s/it] 60%|██████    | 3/5 [10:33<07:06, 213.40s/it] 80%|████████  | 4/5 [12:19<03:05, 185.27s/it] 80%|████████  | 4/5 [13:49<03:32, 212.94s/it] 80%|████████  | 4/5 [14:03<03:32, 212.18s/it] 80%|████████  | 4/5 [14:24<03:43, 223.69s/it]100%|██████████| 5/5 [16:01<00:00, 198.43s/it]100%|██████████| 5/5 [16:01<00:00, 192.28s/it]
100%|██████████| 5/5 [17:13<00:00, 209.54s/it]100%|██████████| 5/5 [17:13<00:00, 206.66s/it]
100%|██████████| 5/5 [17:03<00:00, 200.38s/it]100%|██████████| 5/5 [17:03<00:00, 204.65s/it]
100%|██████████| 5/5 [17:47<00:00, 216.25s/it]100%|██████████| 5/5 [17:47<00:00, 213.44s/it]
[rank0]:[W222 17:19:24.311711839 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 17:19:31.124415 1196381 site-packages/torch/distributed/run.py:793] 
W0222 17:19:31.124415 1196381 site-packages/torch/distributed/run.py:793] *****************************************
W0222 17:19:31.124415 1196381 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 17:19:31.124415 1196381 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W222 17:19:59.901319956 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 17:19:59.901320060 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 17:19:59.902780363 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 17:20:29.222033139 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.48s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.30s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.40s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_31/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_31/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_31/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_31/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:01,  6.39s/it]  5%|▌         | 1/20 [00:06<02:05,  6.62s/it]  5%|▌         | 1/20 [00:07<02:14,  7.07s/it]  5%|▌         | 1/20 [00:07<02:14,  7.07s/it] 10%|█         | 2/20 [00:09<01:23,  4.64s/it] 10%|█         | 2/20 [00:10<01:24,  4.68s/it] 10%|█         | 2/20 [00:09<01:21,  4.55s/it] 10%|█         | 2/20 [00:10<01:25,  4.76s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.65s/it] 15%|█▌        | 3/20 [00:12<01:00,  3.57s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.71s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.73s/it] 20%|██        | 4/20 [00:16<00:57,  3.62s/it] 20%|██        | 4/20 [00:16<00:59,  3.73s/it] 20%|██        | 4/20 [00:15<00:58,  3.68s/it] 20%|██        | 4/20 [00:16<01:01,  3.82s/it] 25%|██▌       | 5/20 [00:19<00:51,  3.44s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.48s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.61s/it] 25%|██▌       | 5/20 [00:20<00:54,  3.66s/it] 30%|███       | 6/20 [00:22<00:47,  3.40s/it] 30%|███       | 6/20 [00:22<00:47,  3.41s/it] 30%|███       | 6/20 [00:22<00:47,  3.41s/it] 30%|███       | 6/20 [00:23<00:49,  3.51s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.11s/it] 35%|███▌      | 7/20 [00:24<00:40,  3.14s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.21s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.19s/it] 40%|████      | 8/20 [00:28<00:38,  3.22s/it] 40%|████      | 8/20 [00:28<00:39,  3.31s/it] 40%|████      | 8/20 [00:29<00:40,  3.35s/it] 40%|████      | 8/20 [00:29<00:40,  3.34s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.14s/it] 45%|████▌     | 9/20 [00:31<00:35,  3.20s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.26s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.24s/it] 50%|█████     | 10/20 [00:34<00:32,  3.25s/it] 50%|█████     | 10/20 [00:36<00:33,  3.38s/it] 50%|█████     | 10/20 [00:36<00:36,  3.65s/it] 50%|█████     | 10/20 [00:36<00:36,  3.70s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.50s/it] 55%|█████▌    | 11/20 [00:40<00:31,  3.55s/it] 55%|█████▌    | 11/20 [00:40<00:34,  3.78s/it] 55%|█████▌    | 11/20 [00:40<00:34,  3.80s/it] 60%|██████    | 12/20 [00:43<00:31,  3.90s/it] 60%|██████    | 12/20 [00:44<00:32,  4.01s/it] 60%|██████    | 12/20 [00:44<00:31,  3.90s/it] 60%|██████    | 12/20 [00:45<00:32,  4.11s/it] 65%|██████▌   | 13/20 [00:48<00:28,  4.07s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.11s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.03s/it] 65%|██████▌   | 13/20 [00:50<00:30,  4.30s/it] 70%|███████   | 14/20 [00:52<00:24,  4.04s/it] 70%|███████   | 14/20 [00:52<00:24,  4.00s/it] 70%|███████   | 14/20 [00:53<00:24,  4.04s/it] 70%|███████   | 14/20 [00:54<00:26,  4.36s/it] 75%|███████▌  | 15/20 [00:56<00:18,  3.74s/it] 75%|███████▌  | 15/20 [00:55<00:19,  3.92s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.87s/it] 80%|████████  | 16/20 [00:58<00:14,  3.52s/it] 75%|███████▌  | 15/20 [00:58<00:21,  4.29s/it] 80%|████████  | 16/20 [00:59<00:14,  3.62s/it] 80%|████████  | 16/20 [00:59<00:14,  3.70s/it] 85%|████████▌ | 17/20 [01:01<00:10,  3.44s/it] 80%|████████  | 16/20 [01:02<00:15,  3.94s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.51s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.55s/it] 85%|████████▌ | 17/20 [01:05<00:10,  3.66s/it] 90%|█████████ | 18/20 [01:04<00:06,  3.37s/it] 90%|█████████ | 18/20 [01:05<00:06,  3.39s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.43s/it] 95%|█████████▌| 19/20 [01:08<00:03,  3.28s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.53s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.36s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.38s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.42s/it]100%|██████████| 20/20 [01:12<00:00,  3.63s/it]100%|██████████| 20/20 [01:12<00:00,  3.62s/it]
100%|██████████| 20/20 [01:13<00:00,  3.58s/it]100%|██████████| 20/20 [01:13<00:00,  3.66s/it]
100%|██████████| 20/20 [01:13<00:00,  3.61s/it]100%|██████████| 20/20 [01:13<00:00,  3.69s/it]
100%|██████████| 20/20 [01:15<00:00,  3.68s/it]100%|██████████| 20/20 [01:15<00:00,  3.79s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 17:23:16,782 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_31/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 17:23:16,782 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 17:23:16,782 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 17:23:16,783 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 17:25:40,850 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 17:25:40,852 - RUN - INFO - Evaluation Results:
2025-02-22 17:25:40,854 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        single  0.76
 2  0        cross   0.55
 3  1        all     0.605
 4  1        single  0.79
 5  1        cross   0.42
 6  2        all     0.58
 7  2        single  0.76
 8  2        cross   0.4
 9  3        all     0.595
10  3        single  0.78
11  3        cross   0.41
12  Average  all     0.60875
13  Average  single  0.7725
14  Average  cross   0.445
--  -------  ------  -------
[rank0]:[W222 17:25:41.734519830 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 17:25:47.265109 1206269 site-packages/torch/distributed/run.py:793] 
W0222 17:25:47.265109 1206269 site-packages/torch/distributed/run.py:793] *****************************************
W0222 17:25:47.265109 1206269 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 17:25:47.265109 1206269 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank2]:[W222 17:26:15.895845693 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 17:26:15.895864775 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 17:26:15.900125301 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 17:26:15.915541317 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.40s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.39s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Max step: 33, bias_value: 0.2
Max step: 33, bias_value: 0.2
Max step: 33, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 33, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:16<13:04, 196.09s/it] 20%|██        | 1/5 [03:27<13:50, 207.69s/it] 20%|██        | 1/5 [03:42<14:51, 222.77s/it] 20%|██        | 1/5 [03:45<15:01, 225.34s/it] 40%|████      | 2/5 [06:23<09:32, 190.88s/it] 40%|████      | 2/5 [06:45<10:05, 201.99s/it] 40%|████      | 2/5 [06:51<10:07, 202.53s/it] 40%|████      | 2/5 [06:58<10:19, 206.50s/it] 60%|██████    | 3/5 [09:25<06:14, 187.10s/it] 60%|██████    | 3/5 [10:18<06:49, 204.86s/it] 60%|██████    | 3/5 [10:19<06:55, 207.59s/it] 60%|██████    | 3/5 [10:55<07:20, 220.13s/it] 80%|████████  | 4/5 [12:38<03:09, 189.11s/it] 80%|████████  | 4/5 [14:23<03:41, 221.80s/it] 80%|████████  | 4/5 [14:31<03:38, 218.83s/it] 80%|████████  | 4/5 [14:45<03:49, 229.20s/it]100%|██████████| 5/5 [16:26<00:00, 203.10s/it]100%|██████████| 5/5 [16:26<00:00, 197.20s/it]
100%|██████████| 5/5 [17:35<00:00, 206.22s/it]100%|██████████| 5/5 [17:35<00:00, 211.14s/it]
100%|██████████| 5/5 [17:55<00:00, 218.39s/it]100%|██████████| 5/5 [17:55<00:00, 215.17s/it]
100%|██████████| 5/5 [18:13<00:00, 221.60s/it]100%|██████████| 5/5 [18:13<00:00, 218.68s/it]
[rank0]:[W222 17:46:00.689449435 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 17:46:07.341207 1242604 site-packages/torch/distributed/run.py:793] 
W0222 17:46:07.341207 1242604 site-packages/torch/distributed/run.py:793] *****************************************
W0222 17:46:07.341207 1242604 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 17:46:07.341207 1242604 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank3]:[W222 17:46:34.717914518 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 17:46:34.717921462 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 17:46:34.717922002 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 17:47:04.271491834 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.12s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.05s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.17s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  2.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.42it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.35it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.27it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_33/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2Max step: 50, bias_value: 0.2

Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_33/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_33/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_33/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:01,  6.41s/it]  5%|▌         | 1/20 [00:06<02:12,  7.00s/it]  5%|▌         | 1/20 [00:07<02:15,  7.15s/it]  5%|▌         | 1/20 [00:07<02:14,  7.05s/it] 10%|█         | 2/20 [00:10<01:28,  4.90s/it] 10%|█         | 2/20 [00:10<01:26,  4.81s/it] 10%|█         | 2/20 [00:10<01:27,  4.84s/it] 10%|█         | 2/20 [00:10<01:28,  4.93s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.71s/it] 15%|█▌        | 3/20 [00:12<01:05,  3.83s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.84s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.82s/it] 20%|██        | 4/20 [00:16<01:00,  3.81s/it] 20%|██        | 4/20 [00:16<01:00,  3.75s/it] 20%|██        | 4/20 [00:16<01:02,  3.88s/it] 20%|██        | 4/20 [00:17<01:02,  3.91s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.52s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.57s/it] 25%|██▌       | 5/20 [00:20<00:56,  3.74s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.71s/it] 30%|███       | 6/20 [00:23<00:48,  3.48s/it] 30%|███       | 6/20 [00:23<00:49,  3.51s/it] 30%|███       | 6/20 [00:23<00:49,  3.52s/it] 30%|███       | 6/20 [00:23<00:49,  3.55s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.19s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.22s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.29s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.24s/it] 40%|████      | 8/20 [00:29<00:39,  3.27s/it] 40%|████      | 8/20 [00:29<00:40,  3.39s/it] 40%|████      | 8/20 [00:29<00:40,  3.37s/it] 40%|████      | 8/20 [00:29<00:41,  3.44s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.18s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.25s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.25s/it] 45%|████▌     | 9/20 [00:33<00:36,  3.32s/it] 50%|█████     | 10/20 [00:35<00:32,  3.30s/it] 50%|█████     | 10/20 [00:36<00:33,  3.38s/it] 50%|█████     | 10/20 [00:37<00:37,  3.70s/it] 50%|█████     | 10/20 [00:37<00:38,  3.82s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.50s/it] 55%|█████▌    | 11/20 [00:40<00:31,  3.55s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.81s/it] 55%|█████▌    | 11/20 [00:42<00:35,  3.93s/it] 60%|██████    | 12/20 [00:44<00:31,  3.96s/it] 60%|██████    | 12/20 [00:45<00:31,  3.92s/it] 60%|██████    | 12/20 [00:45<00:32,  4.05s/it] 60%|██████    | 12/20 [00:47<00:33,  4.22s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.14s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.04s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.16s/it] 65%|██████▌   | 13/20 [00:51<00:30,  4.41s/it] 70%|███████   | 14/20 [00:53<00:24,  4.09s/it] 70%|███████   | 14/20 [00:53<00:24,  4.03s/it] 70%|███████   | 14/20 [00:53<00:24,  4.03s/it] 70%|███████   | 14/20 [00:56<00:26,  4.44s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.87s/it] 75%|███████▌  | 15/20 [00:57<00:20,  4.00s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.83s/it] 80%|████████  | 16/20 [00:59<00:14,  3.58s/it] 80%|████████  | 16/20 [01:00<00:14,  3.69s/it] 75%|███████▌  | 15/20 [01:00<00:21,  4.34s/it] 80%|████████  | 16/20 [01:00<00:14,  3.69s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.48s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.55s/it] 80%|████████  | 16/20 [01:03<00:16,  4.05s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.56s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.41s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.43s/it] 85%|████████▌ | 17/20 [01:06<00:11,  3.77s/it] 90%|█████████ | 18/20 [01:07<00:06,  3.47s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.38s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.47s/it] 90%|█████████ | 18/20 [01:10<00:07,  3.67s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.48s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.60s/it]100%|██████████| 20/20 [01:14<00:00,  3.76s/it]100%|██████████| 20/20 [01:14<00:00,  3.71s/it]
100%|██████████| 20/20 [01:14<00:00,  3.70s/it]100%|██████████| 20/20 [01:14<00:00,  3.73s/it]
100%|██████████| 20/20 [01:14<00:00,  3.74s/it]100%|██████████| 20/20 [01:14<00:00,  3.74s/it]
100%|██████████| 20/20 [01:18<00:00,  3.82s/it]100%|██████████| 20/20 [01:18<00:00,  3.91s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 17:49:53,848 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_33/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 17:49:53,848 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 17:49:53,848 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 17:49:53,848 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 17:51:50,960 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 17:51:50,961 - RUN - INFO - Evaluation Results:
2025-02-22 17:51:50,963 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.60875
13  Average  cross   0.445
14  Average  single  0.7725
--  -------  ------  -------
[rank0]:[W222 17:51:51.787240527 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 17:51:57.391172 1251530 site-packages/torch/distributed/run.py:793] 
W0222 17:51:57.391172 1251530 site-packages/torch/distributed/run.py:793] *****************************************
W0222 17:51:57.391172 1251530 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 17:51:57.391172 1251530 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank2]:[W222 17:52:25.595628242 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 17:52:25.596647370 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 17:52:25.597647092 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 17:52:25.610130134 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.20s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.15s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.16s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.26it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.22it/s]
Max step: 35, bias_value: 0.2
Max step: 35, bias_value: 0.2
Max step: 35, bias_value: 0.2
Max step: 35, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:22<13:30, 202.55s/it] 20%|██        | 1/5 [03:30<14:03, 210.86s/it] 20%|██        | 1/5 [03:49<15:19, 229.98s/it] 20%|██        | 1/5 [03:59<15:58, 239.56s/it] 40%|████      | 2/5 [06:34<09:49, 196.34s/it] 40%|████      | 2/5 [06:56<10:22, 207.65s/it] 40%|████      | 2/5 [07:03<10:25, 208.50s/it] 40%|████      | 2/5 [07:16<10:42, 214.30s/it] 60%|██████    | 3/5 [09:43<06:26, 193.03s/it] 60%|██████    | 3/5 [10:38<07:02, 211.43s/it] 60%|██████    | 3/5 [10:41<07:11, 215.62s/it] 60%|██████    | 3/5 [11:26<07:41, 230.87s/it] 80%|████████  | 4/5 [12:59<03:14, 194.05s/it] 80%|████████  | 4/5 [14:50<03:48, 228.82s/it] 80%|████████  | 4/5 [15:11<03:48, 228.27s/it] 80%|████████  | 4/5 [15:12<03:56, 236.10s/it]100%|██████████| 5/5 [16:53<00:00, 208.47s/it]100%|██████████| 5/5 [16:53<00:00, 202.66s/it]
100%|██████████| 5/5 [18:18<00:00, 213.57s/it]100%|██████████| 5/5 [18:18<00:00, 219.71s/it]
100%|██████████| 5/5 [18:27<00:00, 224.44s/it]100%|██████████| 5/5 [18:27<00:00, 221.42s/it]
100%|██████████| 5/5 [18:44<00:00, 227.57s/it]100%|██████████| 5/5 [18:44<00:00, 224.94s/it]
[rank0]:[W222 18:12:41.989308496 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 18:12:48.642557 1289073 site-packages/torch/distributed/run.py:793] 
W0222 18:12:48.642557 1289073 site-packages/torch/distributed/run.py:793] *****************************************
W0222 18:12:48.642557 1289073 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 18:12:48.642557 1289073 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank2]:[W222 18:13:17.364157185 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 18:13:17.365932414 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 18:13:17.367031823 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 18:13:46.916857730 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.60s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.62s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.60s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_35/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_35/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_35/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_35/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<01:59,  6.31s/it]  5%|▌         | 1/20 [00:07<02:13,  7.04s/it]  5%|▌         | 1/20 [00:07<02:13,  7.05s/it]  5%|▌         | 1/20 [00:07<02:15,  7.16s/it] 10%|█         | 2/20 [00:09<01:25,  4.75s/it] 10%|█         | 2/20 [00:10<01:26,  4.82s/it] 10%|█         | 2/20 [00:10<01:27,  4.85s/it] 10%|█         | 2/20 [00:10<01:28,  4.92s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.73s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.73s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.79s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.79s/it] 20%|██        | 4/20 [00:16<01:00,  3.75s/it] 20%|██        | 4/20 [00:16<00:59,  3.69s/it] 20%|██        | 4/20 [00:16<01:00,  3.81s/it] 20%|██        | 4/20 [00:16<01:01,  3.87s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.49s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.65s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.55s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.68s/it] 30%|███       | 6/20 [00:22<00:47,  3.42s/it] 30%|███       | 6/20 [00:22<00:47,  3.40s/it] 30%|███       | 6/20 [00:22<00:48,  3.43s/it] 30%|███       | 6/20 [00:23<00:49,  3.55s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.12s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.20s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.14s/it] 35%|███▌      | 7/20 [00:26<00:41,  3.21s/it] 40%|████      | 8/20 [00:28<00:38,  3.23s/it] 40%|████      | 8/20 [00:29<00:40,  3.38s/it] 40%|████      | 8/20 [00:29<00:39,  3.32s/it] 40%|████      | 8/20 [00:29<00:40,  3.37s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.14s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.21s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.28s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.26s/it] 50%|█████     | 10/20 [00:35<00:32,  3.30s/it] 50%|█████     | 10/20 [00:38<00:42,  4.20s/it] 50%|█████     | 10/20 [00:38<00:40,  4.02s/it] 50%|█████     | 10/20 [00:38<00:42,  4.29s/it] 55%|█████▌    | 11/20 [00:43<00:42,  4.73s/it] 55%|█████▌    | 11/20 [00:44<00:43,  4.87s/it] 55%|█████▌    | 11/20 [00:45<00:42,  4.77s/it] 55%|█████▌    | 11/20 [00:45<00:44,  4.95s/it] 60%|██████    | 12/20 [00:48<00:37,  4.75s/it] 60%|██████    | 12/20 [00:49<00:38,  4.83s/it] 60%|██████    | 12/20 [00:49<00:38,  4.86s/it] 60%|██████    | 12/20 [00:49<00:38,  4.75s/it] 65%|██████▌   | 13/20 [00:52<00:32,  4.69s/it] 65%|██████▌   | 13/20 [00:54<00:32,  4.70s/it] 65%|██████▌   | 13/20 [00:54<00:32,  4.63s/it] 65%|██████▌   | 13/20 [00:54<00:33,  4.78s/it] 70%|███████   | 14/20 [00:56<00:26,  4.47s/it] 70%|███████   | 14/20 [00:57<00:26,  4.40s/it] 70%|███████   | 14/20 [00:58<00:26,  4.47s/it] 70%|███████   | 14/20 [00:58<00:28,  4.70s/it] 75%|███████▌  | 15/20 [01:00<00:21,  4.25s/it] 75%|███████▌  | 15/20 [01:00<00:20,  4.03s/it] 75%|███████▌  | 15/20 [01:01<00:20,  4.19s/it] 75%|███████▌  | 15/20 [01:02<00:22,  4.52s/it] 80%|████████  | 16/20 [01:03<00:15,  3.76s/it] 80%|████████  | 16/20 [01:04<00:15,  3.82s/it] 80%|████████  | 16/20 [01:04<00:15,  3.91s/it] 80%|████████  | 16/20 [01:06<00:16,  4.10s/it] 85%|████████▌ | 17/20 [01:06<00:10,  3.59s/it] 85%|████████▌ | 17/20 [01:07<00:10,  3.63s/it] 85%|████████▌ | 17/20 [01:08<00:11,  3.69s/it] 85%|████████▌ | 17/20 [01:09<00:11,  3.78s/it] 90%|█████████ | 18/20 [01:09<00:06,  3.48s/it] 90%|█████████ | 18/20 [01:10<00:06,  3.48s/it] 90%|█████████ | 18/20 [01:11<00:07,  3.54s/it] 90%|█████████ | 18/20 [01:12<00:07,  3.62s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.44s/it] 95%|█████████▌| 19/20 [01:14<00:03,  3.50s/it] 95%|█████████▌| 19/20 [01:14<00:03,  3.56s/it] 95%|█████████▌| 19/20 [01:15<00:03,  3.59s/it]100%|██████████| 20/20 [01:17<00:00,  3.79s/it]100%|██████████| 20/20 [01:17<00:00,  3.88s/it]
100%|██████████| 20/20 [01:18<00:00,  3.67s/it]100%|██████████| 20/20 [01:18<00:00,  3.91s/it]
100%|██████████| 20/20 [01:19<00:00,  3.75s/it]100%|██████████| 20/20 [01:19<00:00,  3.96s/it]
100%|██████████| 20/20 [01:20<00:00,  3.80s/it]100%|██████████| 20/20 [01:20<00:00,  4.01s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 18:16:39,073 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_35/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 18:16:39,073 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 18:16:39,073 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 18:16:39,073 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 18:18:49,924 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 18:18:49,924 - RUN - INFO - Evaluation Results:
2025-02-22 18:18:49,926 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.60875
13  Average  cross   0.445
14  Average  single  0.7725
--  -------  ------  -------
[rank0]:[W222 18:18:50.761961696 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 18:18:56.395609 1298476 site-packages/torch/distributed/run.py:793] 
W0222 18:18:56.395609 1298476 site-packages/torch/distributed/run.py:793] *****************************************
W0222 18:18:56.395609 1298476 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 18:18:56.395609 1298476 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank3]:[W222 18:19:24.153274934 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 18:19:24.153274973 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 18:19:24.155067904 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 18:19:24.158595848 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.59s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.60s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.57s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]
Max step: 37, bias_value: 0.2
Max step: 37, bias_value: 0.2
Max step: 37, bias_value: 0.2
Max step: 37, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:26<13:46, 206.70s/it] 20%|██        | 1/5 [03:34<14:19, 214.76s/it] 20%|██        | 1/5 [03:53<15:32, 233.25s/it] 20%|██        | 1/5 [04:00<16:00, 240.14s/it] 40%|████      | 2/5 [06:38<09:53, 197.87s/it] 40%|████      | 2/5 [07:07<10:40, 213.59s/it] 40%|████      | 2/5 [07:12<10:39, 213.29s/it] 40%|████      | 2/5 [07:18<10:46, 215.50s/it] 60%|██████    | 3/5 [09:53<06:33, 196.76s/it] 60%|██████    | 3/5 [10:54<07:14, 217.04s/it] 60%|██████    | 3/5 [10:58<07:23, 221.60s/it] 60%|██████    | 3/5 [11:37<07:50, 235.40s/it] 80%|████████  | 4/5 [13:14<03:18, 198.37s/it] 80%|████████  | 4/5 [15:13<03:54, 234.84s/it] 80%|████████  | 4/5 [15:27<03:53, 233.28s/it] 80%|████████  | 4/5 [15:36<04:02, 242.73s/it]100%|██████████| 5/5 [17:14<00:00, 213.39s/it]100%|██████████| 5/5 [17:14<00:00, 206.94s/it]
100%|██████████| 5/5 [18:36<00:00, 217.41s/it]100%|██████████| 5/5 [18:36<00:00, 223.35s/it]
100%|██████████| 5/5 [18:56<00:00, 230.31s/it]100%|██████████| 5/5 [18:56<00:00, 227.22s/it]
100%|██████████| 5/5 [19:11<00:00, 232.86s/it]100%|██████████| 5/5 [19:11<00:00, 230.31s/it]
[rank0]:[W222 18:40:06.050074420 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 18:40:13.773556 1335832 site-packages/torch/distributed/run.py:793] 
W0222 18:40:13.773556 1335832 site-packages/torch/distributed/run.py:793] *****************************************
W0222 18:40:13.773556 1335832 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 18:40:13.773556 1335832 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank3]:[W222 18:40:38.059442515 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 18:40:38.060911416 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 18:40:38.074601178 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 18:41:08.419345736 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.41s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.41s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.26s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_37/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_37/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_37/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_37/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:05<01:49,  5.75s/it]  5%|▌         | 1/20 [00:06<02:11,  6.91s/it]  5%|▌         | 1/20 [00:07<02:14,  7.08s/it]  5%|▌         | 1/20 [00:07<02:16,  7.18s/it] 10%|█         | 2/20 [00:10<01:25,  4.77s/it] 10%|█         | 2/20 [00:10<01:28,  4.92s/it] 10%|█         | 2/20 [00:09<01:21,  4.53s/it] 10%|█         | 2/20 [00:10<01:29,  5.00s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.70s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.82s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.87s/it] 15%|█▌        | 3/20 [00:11<01:01,  3.63s/it] 20%|██        | 4/20 [00:16<00:58,  3.67s/it] 20%|██        | 4/20 [00:16<01:00,  3.76s/it] 20%|██        | 4/20 [00:15<01:00,  3.77s/it] 20%|██        | 4/20 [00:17<01:03,  3.96s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.48s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.52s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.63s/it] 25%|██▌       | 5/20 [00:20<00:56,  3.77s/it] 30%|███       | 6/20 [00:22<00:47,  3.40s/it] 30%|███       | 6/20 [00:22<00:47,  3.41s/it] 30%|███       | 6/20 [00:22<00:48,  3.43s/it] 30%|███       | 6/20 [00:24<00:50,  3.63s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.12s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.15s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.21s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.30s/it] 40%|████      | 8/20 [00:28<00:38,  3.24s/it] 40%|████      | 8/20 [00:28<00:39,  3.30s/it] 40%|████      | 8/20 [00:28<00:40,  3.36s/it] 40%|████      | 8/20 [00:30<00:41,  3.42s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.16s/it] 45%|████▌     | 9/20 [00:31<00:35,  3.19s/it] 45%|████▌     | 9/20 [00:31<00:35,  3.26s/it] 45%|████▌     | 9/20 [00:33<00:36,  3.33s/it] 50%|█████     | 10/20 [00:35<00:32,  3.26s/it] 50%|█████     | 10/20 [00:36<00:36,  3.63s/it] 50%|█████     | 10/20 [00:37<00:34,  3.48s/it] 50%|█████     | 10/20 [00:36<00:37,  3.75s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.48s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.71s/it] 55%|█████▌    | 11/20 [00:41<00:32,  3.66s/it] 55%|█████▌    | 11/20 [00:40<00:34,  3.87s/it] 60%|██████    | 12/20 [00:44<00:31,  3.89s/it] 60%|██████    | 12/20 [00:45<00:32,  4.03s/it] 60%|██████    | 12/20 [00:46<00:32,  4.03s/it] 60%|██████    | 12/20 [00:45<00:33,  4.19s/it] 65%|██████▌   | 13/20 [00:48<00:28,  4.06s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.12s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.15s/it] 65%|██████▌   | 13/20 [00:50<00:30,  4.36s/it] 70%|███████   | 14/20 [00:52<00:24,  4.03s/it] 70%|███████   | 14/20 [00:53<00:24,  4.01s/it] 70%|███████   | 14/20 [00:54<00:24,  4.14s/it] 70%|███████   | 14/20 [00:55<00:26,  4.43s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.93s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.81s/it] 75%|███████▌  | 15/20 [00:58<00:19,  3.98s/it] 80%|████████  | 16/20 [00:58<00:14,  3.54s/it] 80%|████████  | 16/20 [00:59<00:14,  3.66s/it] 75%|███████▌  | 15/20 [00:59<00:21,  4.36s/it] 80%|████████  | 16/20 [01:01<00:15,  3.78s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.46s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.52s/it] 80%|████████  | 16/20 [01:02<00:16,  4.01s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.60s/it] 90%|█████████ | 18/20 [01:05<00:06,  3.39s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.41s/it] 85%|████████▌ | 17/20 [01:05<00:11,  3.73s/it] 90%|█████████ | 18/20 [01:08<00:06,  3.46s/it] 95%|█████████▌| 19/20 [01:08<00:03,  3.35s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.45s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.60s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.50s/it]100%|██████████| 20/20 [01:13<00:00,  3.72s/it]100%|██████████| 20/20 [01:13<00:00,  3.66s/it]
 95%|█████████▌| 19/20 [01:12<00:03,  3.56s/it]100%|██████████| 20/20 [01:13<00:00,  3.65s/it]100%|██████████| 20/20 [01:13<00:00,  3.70s/it]
100%|██████████| 20/20 [01:16<00:00,  3.78s/it]100%|██████████| 20/20 [01:16<00:00,  3.80s/it]
100%|██████████| 20/20 [01:16<00:00,  3.84s/it]100%|██████████| 20/20 [01:16<00:00,  3.84s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 18:43:56,379 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_37/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 18:43:56,379 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 18:43:56,379 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 18:43:56,379 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 18:46:16,547 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 18:46:16,547 - RUN - INFO - Evaluation Results:
2025-02-22 18:46:16,550 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.60875
13  Average  cross   0.445
14  Average  single  0.7725
--  -------  ------  -------
[rank0]:[W222 18:46:17.518120024 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 18:46:23.190471 1345231 site-packages/torch/distributed/run.py:793] 
W0222 18:46:23.190471 1345231 site-packages/torch/distributed/run.py:793] *****************************************
W0222 18:46:23.190471 1345231 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 18:46:23.190471 1345231 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank0]:[W222 18:46:48.529861895 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 18:46:48.530399636 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 18:46:48.533080897 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 18:46:48.537179352 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.53s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.57s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.58s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]
Max step: 39, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 39, bias_value: 0.2
Max step: 39, bias_value: 0.2
Max step: 39, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:28<13:55, 208.78s/it] 20%|██        | 1/5 [03:39<14:38, 219.65s/it] 20%|██        | 1/5 [04:00<16:02, 240.60s/it] 20%|██        | 1/5 [04:06<16:24, 246.15s/it] 40%|████      | 2/5 [06:47<10:07, 202.62s/it] 40%|████      | 2/5 [07:16<10:53, 217.89s/it] 40%|████      | 2/5 [07:22<10:53, 217.99s/it] 40%|████      | 2/5 [07:23<10:52, 217.66s/it] 60%|██████    | 3/5 [10:06<06:42, 201.06s/it] 60%|██████    | 3/5 [11:10<07:25, 222.55s/it] 60%|██████    | 3/5 [11:12<07:32, 226.10s/it] 60%|██████    | 3/5 [11:47<07:57, 238.70s/it] 80%|████████  | 4/5 [13:27<03:21, 201.13s/it] 80%|████████  | 4/5 [15:30<03:58, 238.73s/it] 80%|████████  | 4/5 [15:49<03:59, 239.98s/it] 80%|████████  | 4/5 [15:59<04:08, 248.66s/it]100%|██████████| 5/5 [17:35<00:00, 218.10s/it]100%|██████████| 5/5 [17:35<00:00, 211.15s/it]
100%|██████████| 5/5 [19:04<00:00, 223.73s/it]100%|██████████| 5/5 [19:04<00:00, 228.89s/it]
100%|██████████| 5/5 [19:16<00:00, 234.33s/it]100%|██████████| 5/5 [19:16<00:00, 231.36s/it]
100%|██████████| 5/5 [19:42<00:00, 239.40s/it]100%|██████████| 5/5 [19:42<00:00, 236.48s/it]
[rank0]:[W222 19:08:01.386351078 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 19:08:08.223189 1384817 site-packages/torch/distributed/run.py:793] 
W0222 19:08:08.223189 1384817 site-packages/torch/distributed/run.py:793] *****************************************
W0222 19:08:08.223189 1384817 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 19:08:08.223189 1384817 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank3]:[W222 19:08:34.871420190 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 19:08:34.871985713 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 19:08:34.875873635 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 19:09:04.527104535 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.16s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.01it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.00it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.27it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.27it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.26it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_39/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2Max step: 50, bias_value: 0.2

Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_39/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_39/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_39/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonRank 0: 

 Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:05,  6.63s/it]  5%|▌         | 1/20 [00:07<02:15,  7.12s/it]  5%|▌         | 1/20 [00:07<02:16,  7.21s/it]  5%|▌         | 1/20 [00:07<02:20,  7.38s/it] 10%|█         | 2/20 [00:10<01:26,  4.80s/it] 10%|█         | 2/20 [00:10<01:27,  4.84s/it] 10%|█         | 2/20 [00:10<01:29,  4.99s/it] 10%|█         | 2/20 [00:10<01:30,  5.01s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.78s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.73s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.85s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.86s/it] 20%|██        | 4/20 [00:16<01:00,  3.80s/it] 20%|██        | 4/20 [00:16<01:01,  3.82s/it] 20%|██        | 4/20 [00:16<00:59,  3.74s/it] 20%|██        | 4/20 [00:17<01:02,  3.93s/it] 25%|██▌       | 5/20 [00:19<00:55,  3.67s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.55s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.53s/it] 25%|██▌       | 5/20 [00:20<00:56,  3.73s/it] 30%|███       | 6/20 [00:24<00:56,  4.01s/it] 30%|███       | 6/20 [00:25<00:58,  4.15s/it] 30%|███       | 6/20 [00:25<00:58,  4.19s/it] 30%|███       | 6/20 [00:26<01:00,  4.31s/it] 35%|███▌      | 7/20 [00:27<00:47,  3.66s/it] 35%|███▌      | 7/20 [00:27<00:47,  3.67s/it] 35%|███▌      | 7/20 [00:28<00:48,  3.75s/it] 35%|███▌      | 7/20 [00:28<00:49,  3.79s/it] 40%|████      | 8/20 [00:31<00:44,  3.69s/it] 40%|████      | 8/20 [00:31<00:44,  3.69s/it] 40%|████      | 8/20 [00:31<00:44,  3.68s/it] 40%|████      | 8/20 [00:32<00:45,  3.79s/it] 45%|████▌     | 9/20 [00:34<00:38,  3.50s/it] 45%|████▌     | 9/20 [00:34<00:38,  3.46s/it] 45%|████▌     | 9/20 [00:34<00:38,  3.47s/it] 45%|████▌     | 9/20 [00:35<00:39,  3.56s/it] 50%|█████     | 10/20 [00:37<00:34,  3.47s/it] 50%|█████     | 10/20 [00:39<00:39,  3.93s/it] 50%|█████     | 10/20 [00:39<00:38,  3.88s/it] 50%|█████     | 10/20 [00:39<00:37,  3.71s/it] 55%|█████▌    | 11/20 [00:42<00:33,  3.72s/it] 55%|█████▌    | 11/20 [00:43<00:35,  3.98s/it] 55%|█████▌    | 11/20 [00:43<00:35,  3.94s/it] 55%|█████▌    | 11/20 [00:43<00:34,  3.82s/it] 60%|██████    | 12/20 [00:47<00:32,  4.07s/it] 60%|██████    | 12/20 [00:48<00:34,  4.32s/it] 60%|██████    | 12/20 [00:48<00:33,  4.21s/it] 60%|██████    | 12/20 [00:48<00:33,  4.15s/it] 65%|██████▌   | 13/20 [00:51<00:29,  4.21s/it] 65%|██████▌   | 13/20 [00:52<00:30,  4.30s/it] 65%|██████▌   | 13/20 [00:53<00:31,  4.50s/it] 65%|██████▌   | 13/20 [00:53<00:30,  4.30s/it] 70%|███████   | 14/20 [00:55<00:24,  4.16s/it] 70%|███████   | 14/20 [00:56<00:24,  4.11s/it] 70%|███████   | 14/20 [00:57<00:25,  4.26s/it] 70%|███████   | 14/20 [00:58<00:27,  4.56s/it] 75%|███████▌  | 15/20 [00:59<00:20,  4.02s/it] 75%|███████▌  | 15/20 [00:59<00:19,  3.85s/it] 75%|███████▌  | 15/20 [01:00<00:20,  4.03s/it] 80%|████████  | 16/20 [01:02<00:14,  3.60s/it] 75%|███████▌  | 15/20 [01:02<00:22,  4.47s/it] 80%|████████  | 16/20 [01:03<00:14,  3.70s/it] 80%|████████  | 16/20 [01:04<00:15,  3.83s/it] 80%|████████  | 16/20 [01:05<00:16,  4.08s/it] 85%|████████▌ | 17/20 [01:05<00:10,  3.52s/it] 85%|████████▌ | 17/20 [01:06<00:10,  3.58s/it] 85%|████████▌ | 17/20 [01:07<00:10,  3.65s/it] 85%|████████▌ | 17/20 [01:08<00:11,  3.78s/it] 90%|█████████ | 18/20 [01:08<00:06,  3.46s/it] 90%|█████████ | 18/20 [01:09<00:06,  3.46s/it] 90%|█████████ | 18/20 [01:10<00:07,  3.55s/it] 90%|█████████ | 18/20 [01:11<00:07,  3.62s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.48s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.49s/it] 95%|█████████▌| 19/20 [01:14<00:03,  3.59s/it] 95%|█████████▌| 19/20 [01:15<00:03,  3.59s/it]100%|██████████| 20/20 [01:16<00:00,  3.87s/it]100%|██████████| 20/20 [01:16<00:00,  3.85s/it]
100%|██████████| 20/20 [01:17<00:00,  3.67s/it]100%|██████████| 20/20 [01:17<00:00,  3.87s/it]
100%|██████████| 20/20 [01:18<00:00,  3.81s/it]100%|██████████| 20/20 [01:18<00:00,  3.94s/it]
100%|██████████| 20/20 [01:19<00:00,  3.83s/it]100%|██████████| 20/20 [01:19<00:00,  3.99s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 19:11:52,689 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_39/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 19:11:52,689 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 19:11:52,689 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 19:11:52,689 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 19:13:48,628 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 19:13:48,628 - RUN - INFO - Evaluation Results:
2025-02-22 19:13:48,630 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.60875
13  Average  cross   0.445
14  Average  single  0.7725
--  -------  ------  -------
[rank0]:[W222 19:13:49.458625736 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 19:13:54.857355 1394198 site-packages/torch/distributed/run.py:793] 
W0222 19:13:54.857355 1394198 site-packages/torch/distributed/run.py:793] *****************************************
W0222 19:13:54.857355 1394198 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 19:13:54.857355 1394198 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank1]:[W222 19:14:21.754011027 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 19:14:21.754012902 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 19:14:21.754015066 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 19:14:21.754015905 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.16s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.19s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.16s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]

Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]
Max step: 41, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 41, bias_value: 0.2Max step: 41, bias_value: 0.2

Max step: 41, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:36<14:26, 216.55s/it] 20%|██        | 1/5 [03:47<15:09, 227.25s/it] 20%|██        | 1/5 [04:09<16:37, 249.38s/it] 20%|██        | 1/5 [04:15<17:00, 255.02s/it] 40%|████      | 2/5 [07:00<10:27, 209.04s/it] 40%|████      | 2/5 [07:28<11:10, 223.46s/it] 40%|████      | 2/5 [07:33<11:05, 221.92s/it] 40%|████      | 2/5 [07:34<11:10, 223.61s/it] 60%|██████    | 3/5 [10:23<06:52, 206.29s/it] 60%|██████    | 3/5 [11:26<07:34, 227.05s/it] 60%|██████    | 3/5 [11:30<07:43, 231.91s/it] 60%|██████    | 3/5 [12:04<08:08, 244.35s/it] 80%|████████  | 4/5 [13:52<03:27, 207.36s/it] 80%|████████  | 4/5 [15:56<04:05, 245.51s/it] 80%|████████  | 4/5 [16:07<04:03, 243.76s/it] 80%|████████  | 4/5 [16:17<04:12, 252.27s/it]100%|██████████| 5/5 [18:08<00:00, 225.02s/it]100%|██████████| 5/5 [18:08<00:00, 217.74s/it]
100%|██████████| 5/5 [19:22<00:00, 226.13s/it]100%|██████████| 5/5 [19:22<00:00, 232.51s/it]
100%|██████████| 5/5 [19:44<00:00, 239.26s/it]100%|██████████| 5/5 [19:44<00:00, 236.91s/it]
100%|██████████| 5/5 [20:02<00:00, 242.77s/it]100%|██████████| 5/5 [20:02<00:00, 240.59s/it]
[rank0]:[W222 19:35:51.781434579 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 19:35:58.847227 1439010 site-packages/torch/distributed/run.py:793] 
W0222 19:35:58.847227 1439010 site-packages/torch/distributed/run.py:793] *****************************************
W0222 19:35:58.847227 1439010 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 19:35:58.847227 1439010 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank2]:[W222 19:36:25.675747740 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 19:36:25.680766082 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 19:36:25.683264462 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 19:36:54.078425720 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.18s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.20s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.12s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.29it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.23it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_41/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_41/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_41/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_41/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonRank 0: 

 Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:05,  6.62s/it]  5%|▌         | 1/20 [00:07<02:14,  7.08s/it]  5%|▌         | 1/20 [00:07<02:16,  7.17s/it]  5%|▌         | 1/20 [00:07<02:17,  7.23s/it] 10%|█         | 2/20 [00:10<01:27,  4.87s/it] 10%|█         | 2/20 [00:10<01:27,  4.84s/it] 10%|█         | 2/20 [00:10<01:29,  4.98s/it] 10%|█         | 2/20 [00:10<01:30,  5.02s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.81s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.81s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.78s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.86s/it] 20%|██        | 4/20 [00:16<00:59,  3.70s/it] 20%|██        | 4/20 [00:16<01:01,  3.86s/it] 20%|██        | 4/20 [00:17<01:02,  3.91s/it] 20%|██        | 4/20 [00:17<01:03,  3.98s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.51s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.72s/it] 25%|██▌       | 5/20 [00:20<00:54,  3.65s/it] 25%|██▌       | 5/20 [00:20<00:56,  3.77s/it] 30%|███       | 6/20 [00:22<00:48,  3.44s/it] 30%|███       | 6/20 [00:23<00:49,  3.50s/it] 30%|███       | 6/20 [00:23<00:49,  3.52s/it] 30%|███       | 6/20 [00:24<00:51,  3.69s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.20s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.29s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.24s/it] 35%|███▌      | 7/20 [00:26<00:43,  3.36s/it] 40%|████      | 8/20 [00:29<00:39,  3.29s/it] 40%|████      | 8/20 [00:30<00:41,  3.49s/it] 40%|████      | 8/20 [00:30<00:41,  3.44s/it] 40%|████      | 8/20 [00:30<00:41,  3.49s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.19s/it] 45%|████▌     | 9/20 [00:33<00:37,  3.37s/it] 45%|████▌     | 9/20 [00:33<00:36,  3.32s/it] 45%|████▌     | 9/20 [00:33<00:37,  3.37s/it] 50%|█████     | 10/20 [00:36<00:34,  3.43s/it] 50%|█████     | 10/20 [00:37<00:37,  3.73s/it] 50%|█████     | 10/20 [00:37<00:35,  3.53s/it] 50%|█████     | 10/20 [00:38<00:38,  3.82s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.65s/it] 55%|█████▌    | 11/20 [00:41<00:35,  3.91s/it] 55%|█████▌    | 11/20 [00:41<00:33,  3.69s/it] 55%|█████▌    | 11/20 [00:42<00:35,  3.92s/it] 60%|██████    | 12/20 [00:45<00:32,  4.03s/it] 60%|██████    | 12/20 [00:46<00:33,  4.14s/it] 60%|██████    | 12/20 [00:46<00:33,  4.14s/it] 60%|██████    | 12/20 [00:47<00:34,  4.30s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.17s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.24s/it] 65%|██████▌   | 13/20 [00:51<00:29,  4.27s/it] 65%|██████▌   | 13/20 [00:52<00:31,  4.50s/it] 70%|███████   | 14/20 [00:54<00:24,  4.09s/it] 70%|███████   | 14/20 [00:54<00:24,  4.09s/it] 70%|███████   | 14/20 [00:55<00:25,  4.27s/it] 70%|███████   | 14/20 [00:56<00:26,  4.47s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.84s/it] 75%|███████▌  | 15/20 [00:58<00:19,  4.00s/it] 75%|███████▌  | 15/20 [00:59<00:20,  4.13s/it] 80%|████████  | 16/20 [01:00<00:14,  3.63s/it] 75%|███████▌  | 15/20 [01:01<00:22,  4.43s/it] 80%|████████  | 16/20 [01:00<00:14,  3.69s/it] 80%|████████  | 16/20 [01:02<00:15,  3.88s/it] 80%|████████  | 16/20 [01:04<00:16,  4.05s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.51s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.57s/it] 85%|████████▌ | 17/20 [01:06<00:11,  3.70s/it] 85%|████████▌ | 17/20 [01:07<00:11,  3.88s/it] 90%|█████████ | 18/20 [01:07<00:07,  3.51s/it] 90%|█████████ | 18/20 [01:07<00:07,  3.56s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.56s/it] 90%|█████████ | 18/20 [01:10<00:07,  3.69s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.51s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.54s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.54s/it] 95%|█████████▌| 19/20 [01:14<00:03,  3.64s/it]100%|██████████| 20/20 [01:15<00:00,  3.76s/it]100%|██████████| 20/20 [01:15<00:00,  3.77s/it]
100%|██████████| 20/20 [01:15<00:00,  3.85s/it]100%|██████████| 20/20 [01:15<00:00,  3.79s/it]
100%|██████████| 20/20 [01:17<00:00,  3.75s/it]100%|██████████| 20/20 [01:17<00:00,  3.86s/it]
100%|██████████| 20/20 [01:18<00:00,  3.81s/it]100%|██████████| 20/20 [01:18<00:00,  3.93s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 19:39:41,569 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_41/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 19:39:41,570 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 19:39:41,570 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 19:39:41,570 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 19:41:50,375 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 19:41:50,375 - RUN - INFO - Evaluation Results:
2025-02-22 19:41:50,377 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        single  0.76
 2  0        cross   0.55
 3  1        all     0.605
 4  1        single  0.79
 5  1        cross   0.42
 6  2        all     0.58
 7  2        single  0.76
 8  2        cross   0.4
 9  3        all     0.595
10  3        single  0.78
11  3        cross   0.41
12  Average  all     0.60875
13  Average  single  0.7725
14  Average  cross   0.445
--  -------  ------  -------
[rank0]:[W222 19:41:51.221136048 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 19:41:56.711258 1451266 site-packages/torch/distributed/run.py:793] 
W0222 19:41:56.711258 1451266 site-packages/torch/distributed/run.py:793] *****************************************
W0222 19:41:56.711258 1451266 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 19:41:56.711258 1451266 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank3]:[W222 19:42:20.854905470 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 19:42:20.854905431 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 19:42:20.880679914 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 19:42:20.885427542 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.04it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.12s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.10s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  2.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.94it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.95it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.27it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]
Max step: 43, bias_value: 0.2
Max step: 43, bias_value: 0.2Max step: 43, bias_value: 0.2

Max step: 43, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:34<14:19, 214.99s/it] 20%|██        | 1/5 [03:50<15:22, 230.68s/it] 20%|██        | 1/5 [04:11<16:45, 251.35s/it] 20%|██        | 1/5 [04:19<17:17, 259.42s/it] 40%|████      | 2/5 [06:59<10:25, 208.60s/it] 40%|████      | 2/5 [07:37<11:25, 228.58s/it] 40%|████      | 2/5 [07:40<11:14, 224.98s/it] 40%|████      | 2/5 [07:40<11:20, 226.72s/it] 60%|██████    | 3/5 [10:27<06:57, 208.58s/it] 60%|██████    | 3/5 [11:38<07:43, 231.77s/it] 60%|██████    | 3/5 [11:47<07:56, 238.15s/it] 60%|██████    | 3/5 [12:16<08:17, 248.57s/it] 80%|████████  | 4/5 [14:02<03:30, 210.94s/it] 80%|████████  | 4/5 [16:18<04:11, 251.19s/it] 80%|████████  | 4/5 [16:27<04:09, 249.22s/it] 80%|████████  | 4/5 [16:35<04:17, 257.56s/it]100%|██████████| 5/5 [18:21<00:00, 228.25s/it]100%|██████████| 5/5 [18:21<00:00, 220.24s/it]
100%|██████████| 5/5 [19:44<00:00, 230.52s/it]100%|██████████| 5/5 [19:44<00:00, 236.91s/it]
100%|██████████| 5/5 [20:18<00:00, 246.97s/it]100%|██████████| 5/5 [20:18<00:00, 243.60s/it]
100%|██████████| 5/5 [20:24<00:00, 247.03s/it]100%|██████████| 5/5 [20:24<00:00, 244.81s/it]
[rank0]:[W222 20:04:12.203021958 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 20:04:18.444974 1503691 site-packages/torch/distributed/run.py:793] 
W0222 20:04:18.444974 1503691 site-packages/torch/distributed/run.py:793] *****************************************
W0222 20:04:18.444974 1503691 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 20:04:18.444974 1503691 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank3]:[W222 20:04:43.746985877 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 20:04:43.746985625 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 20:04:43.756913047 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 20:05:13.284916464 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_43/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.03s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.01s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.26it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.30it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.23it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.27it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_43/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_43/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_43/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
  5%|▌         | 1/20 [00:05<01:45,  5.57s/it]You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 10%|█         | 2/20 [00:09<01:20,  4.47s/it] 15%|█▌        | 3/20 [00:11<01:00,  3.57s/it] 20%|██        | 4/20 [00:15<00:58,  3.64s/it]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s] 25%|██▌       | 5/20 [00:19<00:54,  3.63s/it]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
 30%|███       | 6/20 [00:22<00:47,  3.42s/it]  5%|▌         | 1/20 [00:06<02:12,  6.97s/it]  5%|▌         | 1/20 [00:06<02:07,  6.70s/it]  5%|▌         | 1/20 [00:07<02:21,  7.47s/it] 35%|███▌      | 7/20 [00:24<00:41,  3.21s/it] 10%|█         | 2/20 [00:10<01:25,  4.76s/it] 10%|█         | 2/20 [00:10<01:25,  4.73s/it] 10%|█         | 2/20 [00:10<01:30,  5.02s/it] 40%|████      | 8/20 [00:28<00:40,  3.40s/it] 15%|█▌        | 3/20 [00:12<01:05,  3.83s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.74s/it] 15%|█▌        | 3/20 [00:13<01:06,  3.88s/it] 45%|████▌     | 9/20 [00:31<00:36,  3.31s/it] 20%|██        | 4/20 [00:16<00:59,  3.72s/it] 20%|██        | 4/20 [00:16<01:01,  3.81s/it] 20%|██        | 4/20 [00:17<01:02,  3.93s/it] 50%|█████     | 10/20 [00:36<00:37,  3.75s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.51s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.67s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.69s/it] 30%|███       | 6/20 [00:22<00:48,  3.45s/it] 55%|█████▌    | 11/20 [00:40<00:34,  3.87s/it] 30%|███       | 6/20 [00:23<00:49,  3.55s/it] 30%|███       | 6/20 [00:23<00:49,  3.56s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.17s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.25s/it] 35%|███▌      | 7/20 [00:25<00:42,  3.28s/it] 60%|██████    | 12/20 [00:45<00:33,  4.16s/it] 40%|████      | 8/20 [00:29<00:39,  3.29s/it] 40%|████      | 8/20 [00:30<00:41,  3.44s/it] 40%|████      | 8/20 [00:29<00:41,  3.44s/it] 45%|████▌     | 9/20 [00:32<00:34,  3.18s/it] 65%|██████▌   | 13/20 [00:50<00:30,  4.32s/it] 45%|████▌     | 9/20 [00:33<00:36,  3.32s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.30s/it] 50%|█████     | 10/20 [00:36<00:34,  3.41s/it] 50%|█████     | 10/20 [00:36<00:36,  3.68s/it] 50%|█████     | 10/20 [00:36<00:35,  3.51s/it] 70%|███████   | 14/20 [00:54<00:26,  4.39s/it] 55%|█████▌    | 11/20 [00:41<00:32,  3.64s/it] 55%|█████▌    | 11/20 [00:40<00:34,  3.82s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.72s/it] 75%|███████▌  | 15/20 [00:58<00:21,  4.33s/it] 80%|████████  | 16/20 [01:02<00:16,  4.02s/it] 60%|██████    | 12/20 [00:45<00:32,  4.10s/it] 60%|██████    | 12/20 [00:46<00:32,  4.03s/it] 60%|██████    | 12/20 [00:45<00:32,  4.10s/it] 85%|████████▌ | 17/20 [01:05<00:11,  3.75s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.22s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.27s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.22s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.63s/it] 70%|███████   | 14/20 [00:53<00:24,  4.07s/it] 70%|███████   | 14/20 [00:54<00:24,  4.15s/it] 70%|███████   | 14/20 [00:54<00:25,  4.21s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.62s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.82s/it] 75%|███████▌  | 15/20 [00:58<00:20,  4.04s/it] 75%|███████▌  | 15/20 [00:58<00:20,  4.06s/it]100%|██████████| 20/20 [01:16<00:00,  3.83s/it]100%|██████████| 20/20 [01:16<00:00,  3.83s/it]
 80%|████████  | 16/20 [01:00<00:14,  3.68s/it] 80%|████████  | 16/20 [01:01<00:14,  3.62s/it] 80%|████████  | 16/20 [01:01<00:15,  3.86s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.57s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.55s/it] 85%|████████▌ | 17/20 [01:05<00:11,  3.71s/it] 90%|█████████ | 18/20 [01:07<00:06,  3.46s/it] 90%|█████████ | 18/20 [01:07<00:06,  3.46s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.54s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.51s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.42s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.59s/it]100%|██████████| 20/20 [01:14<00:00,  3.76s/it]100%|██████████| 20/20 [01:14<00:00,  3.75s/it]
100%|██████████| 20/20 [01:15<00:00,  3.80s/it]100%|██████████| 20/20 [01:15<00:00,  3.79s/it]
100%|██████████| 20/20 [01:16<00:00,  3.79s/it]100%|██████████| 20/20 [01:16<00:00,  3.81s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 20:07:52,678 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_43/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 20:07:52,679 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 20:07:52,679 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 20:07:52,679 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 20:09:49,562 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 20:09:49,562 - RUN - INFO - Evaluation Results:
2025-02-22 20:09:49,566 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.60875
13  Average  cross   0.445
14  Average  single  0.7725
--  -------  ------  -------
[rank0]:[W222 20:09:50.415339742 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 20:09:56.142227 1514429 site-packages/torch/distributed/run.py:793] 
W0222 20:09:56.142227 1514429 site-packages/torch/distributed/run.py:793] *****************************************
W0222 20:09:56.142227 1514429 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 20:09:56.142227 1514429 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank0]:[W222 20:10:24.295727661 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 20:10:24.296679263 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 20:10:24.298640270 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 20:10:24.301040784 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.28s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Max step: 45, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
  0%|          | 0/5 [00:00<?, ?it/s]Max step: 45, bias_value: 0.2
Max step: 45, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ovMax step: 45, bias_value: 0.2

You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:35<14:23, 215.76s/it] 20%|██        | 1/5 [03:51<15:26, 231.57s/it] 20%|██        | 1/5 [04:14<16:57, 254.44s/it] 20%|██        | 1/5 [04:27<17:49, 267.39s/it] 40%|████      | 2/5 [07:01<10:30, 210.11s/it] 40%|████      | 2/5 [07:41<11:32, 230.79s/it] 40%|████      | 2/5 [07:47<11:23, 227.75s/it] 40%|████      | 2/5 [07:49<11:33, 231.15s/it] 60%|██████    | 3/5 [10:29<06:57, 208.93s/it] 60%|██████    | 3/5 [11:50<07:51, 235.53s/it] 60%|██████    | 3/5 [11:56<08:02, 241.48s/it] 60%|██████    | 3/5 [12:28<08:24, 252.21s/it] 80%|████████  | 4/5 [14:05<03:31, 211.71s/it] 80%|████████  | 4/5 [16:31<04:14, 254.80s/it] 80%|████████  | 4/5 [16:42<04:12, 252.84s/it] 80%|████████  | 4/5 [16:47<04:19, 259.94s/it]100%|██████████| 5/5 [18:26<00:00, 229.61s/it]100%|██████████| 5/5 [18:26<00:00, 221.35s/it]
100%|██████████| 5/5 [20:02<00:00, 233.92s/it]100%|██████████| 5/5 [20:02<00:00, 240.58s/it]
100%|██████████| 5/5 [20:29<00:00, 248.78s/it]100%|██████████| 5/5 [20:29<00:00, 245.87s/it]
100%|██████████| 5/5 [20:39<00:00, 249.88s/it]100%|██████████| 5/5 [20:39<00:00, 247.89s/it]
[rank0]:[W222 20:32:29.161114055 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 20:32:36.332945 1564061 site-packages/torch/distributed/run.py:793] 
W0222 20:32:36.332945 1564061 site-packages/torch/distributed/run.py:793] *****************************************
W0222 20:32:36.332945 1564061 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 20:32:36.332945 1564061 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank2]:[W222 20:33:02.715779627 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 20:33:02.717682819 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 20:33:02.724602574 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 20:33:31.868640732 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.41s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.39s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.41s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_45/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_45/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_45/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_45/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<01:59,  6.31s/it]  5%|▌         | 1/20 [00:07<02:13,  7.02s/it]  5%|▌         | 1/20 [00:07<02:13,  7.00s/it]  5%|▌         | 1/20 [00:06<02:07,  6.72s/it] 10%|█         | 2/20 [00:09<01:25,  4.76s/it] 10%|█         | 2/20 [00:10<01:26,  4.81s/it] 10%|█         | 2/20 [00:10<01:25,  4.77s/it] 10%|█         | 2/20 [00:10<01:26,  4.81s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.79s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.73s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.76s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.79s/it] 20%|██        | 4/20 [00:16<00:58,  3.67s/it] 20%|██        | 4/20 [00:16<01:00,  3.81s/it] 20%|██        | 4/20 [00:16<01:00,  3.77s/it] 20%|██        | 4/20 [00:16<01:02,  3.88s/it] 25%|██▌       | 5/20 [00:19<00:51,  3.45s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.53s/it] 25%|██▌       | 5/20 [00:19<00:55,  3.68s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.69s/it] 30%|███       | 6/20 [00:22<00:47,  3.42s/it] 30%|███       | 6/20 [00:22<00:48,  3.43s/it] 30%|███       | 6/20 [00:22<00:48,  3.47s/it] 30%|███       | 6/20 [00:23<00:49,  3.54s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.12s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.12s/it] 35%|███▌      | 7/20 [00:25<00:42,  3.24s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.23s/it] 40%|████      | 8/20 [00:28<00:38,  3.24s/it] 40%|████      | 8/20 [00:29<00:39,  3.32s/it] 40%|████      | 8/20 [00:29<00:40,  3.40s/it] 40%|████      | 8/20 [00:29<00:40,  3.37s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.16s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.19s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.29s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.26s/it] 50%|█████     | 10/20 [00:35<00:32,  3.25s/it] 50%|█████     | 10/20 [00:36<00:36,  3.64s/it] 50%|█████     | 10/20 [00:36<00:33,  3.40s/it] 50%|█████     | 10/20 [00:37<00:38,  3.82s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.47s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.76s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.59s/it] 55%|█████▌    | 11/20 [00:41<00:35,  3.92s/it] 60%|██████    | 12/20 [00:44<00:30,  3.84s/it] 60%|██████    | 12/20 [00:45<00:32,  4.08s/it] 60%|██████    | 12/20 [00:45<00:31,  3.96s/it] 60%|██████    | 12/20 [00:46<00:33,  4.20s/it] 65%|██████▌   | 13/20 [00:48<00:28,  4.04s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.15s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.08s/it] 65%|██████▌   | 13/20 [00:51<00:30,  4.38s/it] 70%|███████   | 14/20 [00:52<00:23,  3.97s/it] 70%|███████   | 14/20 [00:54<00:26,  4.44s/it] 70%|███████   | 14/20 [00:56<00:30,  5.00s/it] 70%|███████   | 14/20 [00:59<00:32,  5.41s/it] 75%|███████▌  | 15/20 [00:59<00:24,  4.97s/it] 75%|███████▌  | 15/20 [00:59<00:23,  4.67s/it] 75%|███████▌  | 15/20 [01:00<00:22,  4.58s/it] 80%|████████  | 16/20 [01:02<00:17,  4.25s/it] 80%|████████  | 16/20 [01:03<00:16,  4.24s/it] 75%|███████▌  | 15/20 [01:03<00:25,  5.00s/it] 80%|████████  | 16/20 [01:03<00:16,  4.20s/it] 85%|████████▌ | 17/20 [01:05<00:11,  3.94s/it] 80%|████████  | 16/20 [01:06<00:17,  4.44s/it] 85%|████████▌ | 17/20 [01:06<00:11,  3.92s/it] 85%|████████▌ | 17/20 [01:06<00:11,  3.89s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.73s/it] 85%|████████▌ | 17/20 [01:09<00:12,  4.03s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.71s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.68s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.61s/it] 90%|█████████ | 18/20 [01:12<00:07,  3.80s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.70s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.65s/it] 95%|█████████▌| 19/20 [01:16<00:03,  3.69s/it]100%|██████████| 20/20 [01:16<00:00,  3.90s/it]100%|██████████| 20/20 [01:16<00:00,  3.83s/it]
100%|██████████| 20/20 [01:17<00:00,  3.84s/it]100%|██████████| 20/20 [01:17<00:00,  3.87s/it]
100%|██████████| 20/20 [01:17<00:00,  3.88s/it]100%|██████████| 20/20 [01:17<00:00,  3.90s/it]
100%|██████████| 20/20 [01:20<00:00,  3.90s/it]100%|██████████| 20/20 [01:20<00:00,  4.02s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 20:36:23,078 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_45/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 20:36:23,078 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 20:36:23,078 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 20:36:23,079 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 20:38:31,586 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 20:38:31,586 - RUN - INFO - Evaluation Results:
2025-02-22 20:38:31,589 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        single  0.76
 2  0        cross   0.55
 3  1        all     0.605
 4  1        single  0.79
 5  1        cross   0.42
 6  2        all     0.58
 7  2        single  0.76
 8  2        cross   0.4
 9  3        all     0.595
10  3        single  0.78
11  3        cross   0.41
12  Average  all     0.60875
13  Average  single  0.7725
14  Average  cross   0.445
--  -------  ------  -------
[rank0]:[W222 20:38:32.441195208 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 20:38:38.249535 1575335 site-packages/torch/distributed/run.py:793] 
W0222 20:38:38.249535 1575335 site-packages/torch/distributed/run.py:793] *****************************************
W0222 20:38:38.249535 1575335 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 20:38:38.249535 1575335 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank0]:[W222 20:39:04.541484364 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 20:39:04.544143635 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 20:39:04.544329535 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 20:39:04.544504039 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.20it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.12s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.72it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.37it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.64it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.65it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]

Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]
Max step: 47, bias_value: 0.2
Max step: 47, bias_value: 0.2
Max step: 47, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 47, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:38<14:34, 218.75s/it] 20%|██        | 1/5 [03:57<15:48, 237.18s/it] 20%|██        | 1/5 [04:23<17:33, 263.39s/it] 20%|██        | 1/5 [04:36<18:26, 276.65s/it] 40%|████      | 2/5 [07:08<10:41, 213.68s/it] 40%|████      | 2/5 [07:52<11:48, 236.05s/it] 40%|████      | 2/5 [07:56<11:33, 231.31s/it] 40%|████      | 2/5 [08:02<11:51, 237.26s/it] 60%|██████    | 3/5 [10:38<07:03, 211.89s/it] 60%|██████    | 3/5 [12:06<08:00, 240.42s/it] 60%|██████    | 3/5 [12:12<08:13, 246.98s/it] 60%|██████    | 3/5 [12:44<08:34, 257.42s/it] 80%|████████  | 4/5 [14:17<03:34, 214.49s/it] 80%|████████  | 4/5 [16:53<04:20, 260.48s/it] 80%|████████  | 4/5 [17:05<04:18, 258.92s/it] 80%|████████  | 4/5 [17:06<04:24, 264.09s/it]100%|██████████| 5/5 [18:42<00:00, 232.98s/it]100%|██████████| 5/5 [18:42<00:00, 224.58s/it]
100%|██████████| 5/5 [20:27<00:00, 238.16s/it]100%|██████████| 5/5 [20:27<00:00, 245.46s/it]
100%|██████████| 5/5 [20:53<00:00, 252.96s/it]100%|██████████| 5/5 [20:53<00:00, 250.65s/it]
100%|██████████| 5/5 [21:00<00:00, 253.22s/it]100%|██████████| 5/5 [21:00<00:00, 252.17s/it]
[rank0]:[W222 21:01:34.055437367 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 21:01:41.775812 1626964 site-packages/torch/distributed/run.py:793] 
W0222 21:01:41.775812 1626964 site-packages/torch/distributed/run.py:793] *****************************************
W0222 21:01:41.775812 1626964 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 21:01:41.775812 1626964 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W222 21:02:09.164830499 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 21:02:09.165991928 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 21:02:09.169411193 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 21:02:39.413046999 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.40s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.40s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_47/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_47/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_47/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_47/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Rank 0:  Model Class: LlavaQwenForCausalLM
  5%|▌         | 1/20 [00:06<01:59,  6.28s/it] 10%|█         | 2/20 [00:09<01:21,  4.52s/it]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s] 15%|█▌        | 3/20 [00:12<01:04,  3.82s/it]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
 20%|██        | 4/20 [00:16<00:59,  3.72s/it]  5%|▌         | 1/20 [00:05<01:48,  5.69s/it]  5%|▌         | 1/20 [00:06<02:02,  6.43s/it]  5%|▌         | 1/20 [00:06<02:05,  6.62s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.53s/it] 10%|█         | 2/20 [00:09<01:20,  4.45s/it] 10%|█         | 2/20 [00:09<01:23,  4.61s/it] 10%|█         | 2/20 [00:09<01:24,  4.71s/it] 15%|█▌        | 3/20 [00:11<01:00,  3.54s/it] 30%|███       | 6/20 [00:22<00:48,  3.43s/it] 15%|█▌        | 3/20 [00:12<01:00,  3.58s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.69s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.13s/it] 20%|██        | 4/20 [00:15<00:58,  3.64s/it] 20%|██        | 4/20 [00:16<00:59,  3.70s/it] 20%|██        | 4/20 [00:16<01:00,  3.80s/it] 40%|████      | 8/20 [00:28<00:38,  3.23s/it] 25%|██▌       | 5/20 [00:18<00:53,  3.57s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.49s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.65s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.15s/it] 30%|███       | 6/20 [00:21<00:47,  3.38s/it] 30%|███       | 6/20 [00:22<00:47,  3.40s/it] 30%|███       | 6/20 [00:23<00:49,  3.51s/it] 35%|███▌      | 7/20 [00:24<00:41,  3.18s/it] 35%|███▌      | 7/20 [00:24<00:40,  3.11s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.18s/it] 50%|█████     | 10/20 [00:36<00:36,  3.63s/it] 40%|████      | 8/20 [00:28<00:40,  3.36s/it] 40%|████      | 8/20 [00:28<00:39,  3.31s/it] 40%|████      | 8/20 [00:29<00:40,  3.34s/it] 55%|█████▌    | 11/20 [00:40<00:34,  3.78s/it] 45%|████▌     | 9/20 [00:31<00:35,  3.27s/it] 45%|████▌     | 9/20 [00:31<00:35,  3.23s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.23s/it] 60%|██████    | 12/20 [00:44<00:32,  4.04s/it] 50%|█████     | 10/20 [00:35<00:33,  3.31s/it] 50%|█████     | 10/20 [00:35<00:33,  3.35s/it] 50%|█████     | 10/20 [00:36<00:37,  3.72s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.15s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.51s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.53s/it] 55%|█████▌    | 11/20 [00:40<00:34,  3.80s/it] 70%|███████   | 14/20 [00:52<00:23,  3.97s/it] 60%|██████    | 12/20 [00:44<00:31,  3.97s/it] 60%|██████    | 12/20 [00:44<00:31,  3.89s/it] 60%|██████    | 12/20 [00:44<00:32,  4.07s/it] 75%|███████▌  | 15/20 [00:56<00:18,  3.75s/it] 65%|██████▌   | 13/20 [00:48<00:28,  4.13s/it] 80%|████████  | 16/20 [00:59<00:14,  3.63s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.09s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.28s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.49s/it] 70%|███████   | 14/20 [00:52<00:24,  4.10s/it] 70%|███████   | 14/20 [00:53<00:24,  4.08s/it] 70%|███████   | 14/20 [00:54<00:25,  4.33s/it] 90%|█████████ | 18/20 [01:05<00:06,  3.39s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.97s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.92s/it] 75%|███████▌  | 15/20 [00:58<00:21,  4.25s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.43s/it] 80%|████████  | 16/20 [00:58<00:14,  3.57s/it] 80%|████████  | 16/20 [00:59<00:14,  3.73s/it] 80%|████████  | 16/20 [01:01<00:15,  3.90s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.48s/it]100%|██████████| 20/20 [01:13<00:00,  3.63s/it]100%|██████████| 20/20 [01:13<00:00,  3.67s/it]
 85%|████████▌ | 17/20 [01:03<00:10,  3.57s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.66s/it] 90%|█████████ | 18/20 [01:05<00:06,  3.39s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.44s/it] 90%|█████████ | 18/20 [01:07<00:07,  3.51s/it] 95%|█████████▌| 19/20 [01:08<00:03,  3.35s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.47s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.48s/it]100%|██████████| 20/20 [01:13<00:00,  3.72s/it]100%|██████████| 20/20 [01:13<00:00,  3.66s/it]
100%|██████████| 20/20 [01:14<00:00,  3.69s/it]100%|██████████| 20/20 [01:14<00:00,  3.70s/it]
100%|██████████| 20/20 [01:15<00:00,  3.72s/it]100%|██████████| 20/20 [01:15<00:00,  3.76s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 21:05:17,543 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_47/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 21:05:17,543 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 21:05:17,543 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 21:05:17,543 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 21:07:41,924 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 21:07:41,924 - RUN - INFO - Evaluation Results:
2025-02-22 21:07:41,927 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.60875
13  Average  cross   0.445
14  Average  single  0.7725
--  -------  ------  -------
[rank0]:[W222 21:07:42.932063837 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 21:07:48.506207 1639684 site-packages/torch/distributed/run.py:793] 
W0222 21:07:48.506207 1639684 site-packages/torch/distributed/run.py:793] *****************************************
W0222 21:07:48.506207 1639684 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 21:07:48.506207 1639684 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank3]:[W222 21:08:15.577242568 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 21:08:15.591570970 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 21:08:15.602418808 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 21:08:15.609941312 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.28s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]
Max step: 49, bias_value: 0.2
Max step: 49, bias_value: 0.2
Max step: 49, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 49, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:37<14:31, 217.77s/it] 20%|██        | 1/5 [04:02<16:11, 242.84s/it] 20%|██        | 1/5 [04:26<17:44, 266.17s/it] 20%|██        | 1/5 [04:41<18:44, 281.19s/it] 40%|████      | 2/5 [07:10<10:43, 214.61s/it] 40%|████      | 2/5 [07:59<11:36, 232.24s/it] 40%|████      | 2/5 [08:02<12:03, 241.22s/it] 40%|████      | 2/5 [08:07<12:00, 240.08s/it] 60%|██████    | 3/5 [10:44<07:09, 214.54s/it] 60%|██████    | 3/5 [12:14<08:06, 243.00s/it] 60%|██████    | 3/5 [12:29<08:25, 252.66s/it] 60%|██████    | 3/5 [12:57<08:44, 262.22s/it] 80%|████████  | 4/5 [14:28<03:38, 218.15s/it] 80%|████████  | 4/5 [17:15<04:26, 266.00s/it] 80%|████████  | 4/5 [17:21<04:28, 268.35s/it] 80%|████████  | 4/5 [17:25<04:24, 264.81s/it]100%|██████████| 5/5 [18:58<00:00, 237.05s/it]100%|██████████| 5/5 [18:58<00:00, 227.78s/it]
100%|██████████| 5/5 [20:46<00:00, 241.82s/it]100%|██████████| 5/5 [20:46<00:00, 249.38s/it]
100%|██████████| 5/5 [21:19<00:00, 257.18s/it]100%|██████████| 5/5 [21:19<00:00, 255.81s/it]
100%|██████████| 5/5 [21:19<00:00, 258.09s/it]100%|██████████| 5/5 [21:19<00:00, 255.94s/it]
[rank0]:[W222 21:31:04.220488027 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 21:31:11.497633 1694505 site-packages/torch/distributed/run.py:793] 
W0222 21:31:11.497633 1694505 site-packages/torch/distributed/run.py:793] *****************************************
W0222 21:31:11.497633 1694505 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 21:31:11.497633 1694505 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank3]:[W222 21:31:38.788349940 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 21:31:38.788933310 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 21:31:38.798170275 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 21:32:07.110260253 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.42s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.52s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_49/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_49/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_49/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_49/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<01:59,  6.29s/it]  5%|▌         | 1/20 [00:07<02:14,  7.07s/it]  5%|▌         | 1/20 [00:06<02:12,  6.99s/it]  5%|▌         | 1/20 [00:07<02:14,  7.10s/it] 10%|█         | 2/20 [00:09<01:25,  4.73s/it] 10%|█         | 2/20 [00:10<01:26,  4.82s/it] 10%|█         | 2/20 [00:10<01:28,  4.93s/it] 10%|█         | 2/20 [00:10<01:28,  4.93s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.73s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.77s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.77s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.83s/it] 20%|██        | 4/20 [00:16<01:00,  3.76s/it] 20%|██        | 4/20 [00:16<00:59,  3.69s/it] 20%|██        | 4/20 [00:16<01:00,  3.80s/it] 20%|██        | 4/20 [00:17<01:02,  3.90s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.50s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.65s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.56s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.70s/it] 30%|███       | 6/20 [00:22<00:48,  3.44s/it] 30%|███       | 6/20 [00:22<00:48,  3.44s/it] 30%|███       | 6/20 [00:23<00:48,  3.47s/it] 30%|███       | 6/20 [00:23<00:49,  3.55s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.23s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.21s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.17s/it] 35%|███▌      | 7/20 [00:26<00:41,  3.23s/it] 40%|████      | 8/20 [00:28<00:39,  3.26s/it] 40%|████      | 8/20 [00:29<00:40,  3.38s/it] 40%|████      | 8/20 [00:29<00:40,  3.37s/it] 40%|████      | 8/20 [00:29<00:40,  3.36s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.17s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.27s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.23s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.25s/it] 50%|█████     | 10/20 [00:36<00:33,  3.35s/it] 50%|█████     | 10/20 [00:36<00:33,  3.37s/it] 50%|█████     | 10/20 [00:36<00:36,  3.68s/it] 50%|█████     | 10/20 [00:36<00:37,  3.75s/it] 55%|█████▌    | 11/20 [00:40<00:31,  3.54s/it] 55%|█████▌    | 11/20 [00:40<00:31,  3.54s/it] 55%|█████▌    | 11/20 [00:40<00:34,  3.78s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.87s/it] 60%|██████    | 12/20 [00:44<00:31,  3.97s/it] 60%|██████    | 12/20 [00:45<00:31,  3.89s/it] 60%|██████    | 12/20 [00:45<00:32,  4.06s/it] 60%|██████    | 12/20 [00:45<00:33,  4.15s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.15s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.04s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.22s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.25s/it] 70%|███████   | 14/20 [00:53<00:24,  4.04s/it] 70%|███████   | 14/20 [00:53<00:24,  4.07s/it] 70%|███████   | 14/20 [00:53<00:24,  4.03s/it] 70%|███████   | 14/20 [00:54<00:25,  4.29s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.92s/it] 75%|███████▌  | 15/20 [00:57<00:18,  3.77s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.90s/it] 75%|███████▌  | 15/20 [00:58<00:21,  4.23s/it] 80%|████████  | 16/20 [00:59<00:14,  3.54s/it] 80%|████████  | 16/20 [01:00<00:14,  3.60s/it] 80%|████████  | 16/20 [01:00<00:15,  3.80s/it] 80%|████████  | 16/20 [01:01<00:15,  3.89s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.44s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.48s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.63s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.64s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.37s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.41s/it] 90%|█████████ | 18/20 [01:07<00:06,  3.48s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.52s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.34s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.43s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.56s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.50s/it]100%|██████████| 20/20 [01:14<00:00,  3.76s/it]100%|██████████| 20/20 [01:14<00:00,  3.70s/it]
100%|██████████| 20/20 [01:14<00:00,  3.60s/it]100%|██████████| 20/20 [01:14<00:00,  3.71s/it]
100%|██████████| 20/20 [01:14<00:00,  3.72s/it]100%|██████████| 20/20 [01:14<00:00,  3.75s/it]
100%|██████████| 20/20 [01:15<00:00,  3.72s/it]100%|██████████| 20/20 [01:15<00:00,  3.79s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 21:34:54,740 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_49/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 21:34:54,740 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 21:34:54,740 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 21:34:54,740 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 21:37:04,202 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 21:37:04,202 - RUN - INFO - Evaluation Results:
2025-02-22 21:37:04,204 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        single  0.76
 2  0        cross   0.55
 3  1        all     0.605
 4  1        single  0.79
 5  1        cross   0.42
 6  2        all     0.58
 7  2        single  0.76
 8  2        cross   0.4
 9  3        all     0.595
10  3        single  0.78
11  3        cross   0.41
12  Average  all     0.60875
13  Average  single  0.7725
14  Average  cross   0.445
--  -------  ------  -------
[rank0]:[W222 21:37:04.003402736 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 21:37:10.549787 1705215 site-packages/torch/distributed/run.py:793] 
W0222 21:37:10.549787 1705215 site-packages/torch/distributed/run.py:793] *****************************************
W0222 21:37:10.549787 1705215 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 21:37:10.549787 1705215 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank2]:[W222 21:37:35.445265178 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 21:37:35.445268022 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 21:37:35.458313604 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 21:37:35.462513431 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Max step: 51, bias_value: 0.2
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
  0%|          | 0/5 [00:00<?, ?it/s]Max step: 51, bias_value: 0.2
Max step: 51, bias_value: 0.2
Max step: 51, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:40<14:43, 220.83s/it] 20%|██        | 1/5 [04:10<16:41, 250.43s/it] 20%|██        | 1/5 [04:31<18:07, 271.94s/it] 20%|██        | 1/5 [04:43<18:55, 283.97s/it] 40%|████      | 2/5 [07:16<10:54, 218.05s/it] 40%|████      | 2/5 [08:01<11:40, 233.34s/it] 40%|████      | 2/5 [08:14<12:19, 246.58s/it] 40%|████      | 2/5 [08:18<12:14, 244.98s/it] 60%|██████    | 3/5 [10:54<07:16, 218.03s/it] 60%|██████    | 3/5 [12:30<08:16, 248.40s/it] 60%|██████    | 3/5 [12:42<08:33, 256.63s/it] 60%|██████    | 3/5 [13:02<08:48, 264.05s/it] 80%|████████  | 4/5 [14:38<03:40, 220.27s/it] 80%|████████  | 4/5 [17:32<04:29, 269.70s/it] 80%|████████  | 4/5 [17:34<04:27, 267.07s/it] 80%|████████  | 4/5 [17:38<04:31, 271.97s/it]100%|██████████| 5/5 [19:14<00:00, 240.37s/it]100%|██████████| 5/5 [19:14<00:00, 230.93s/it]
100%|██████████| 5/5 [21:00<00:00, 245.26s/it]100%|██████████| 5/5 [21:00<00:00, 252.15s/it]
100%|██████████| 5/5 [21:40<00:00, 261.98s/it]100%|██████████| 5/5 [21:40<00:00, 260.18s/it]
100%|██████████| 5/5 [21:41<00:00, 261.56s/it]100%|██████████| 5/5 [21:41<00:00, 260.35s/it]
[rank0]:[W222 22:00:44.497564597 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 22:00:50.861920 1763394 site-packages/torch/distributed/run.py:793] 
W0222 22:00:50.861920 1763394 site-packages/torch/distributed/run.py:793] *****************************************
W0222 22:00:50.861920 1763394 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 22:00:50.861920 1763394 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank3]:[W222 22:01:18.267115307 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 22:01:18.278203142 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 22:01:18.284677467 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 22:01:47.426012530 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.17s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.05s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.24it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]

Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_51/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2Max step: 50, bias_value: 0.2

Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_51/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_51/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_51/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json


You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<01:56,  6.14s/it]  5%|▌         | 1/20 [00:06<02:11,  6.93s/it]  5%|▌         | 1/20 [00:07<02:13,  7.01s/it]  5%|▌         | 1/20 [00:07<02:21,  7.44s/it] 10%|█         | 2/20 [00:09<01:23,  4.65s/it] 10%|█         | 2/20 [00:10<01:26,  4.83s/it] 10%|█         | 2/20 [00:10<01:25,  4.77s/it] 10%|█         | 2/20 [00:10<01:32,  5.12s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.69s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.77s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.79s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.88s/it] 20%|██        | 4/20 [00:16<00:59,  3.74s/it] 20%|██        | 4/20 [00:16<00:59,  3.72s/it] 20%|██        | 4/20 [00:16<01:01,  3.86s/it] 20%|██        | 4/20 [00:17<01:02,  3.89s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.64s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.53s/it] 25%|██▌       | 5/20 [00:20<00:54,  3.63s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.68s/it] 30%|███       | 6/20 [00:22<00:48,  3.44s/it] 30%|███       | 6/20 [00:22<00:48,  3.45s/it] 30%|███       | 6/20 [00:23<00:49,  3.54s/it] 30%|███       | 6/20 [00:23<00:50,  3.60s/it] 35%|███▌      | 7/20 [00:25<00:42,  3.25s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.22s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.25s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.27s/it] 40%|████      | 8/20 [00:29<00:41,  3.42s/it] 40%|████      | 8/20 [00:29<00:40,  3.35s/it] 40%|████      | 8/20 [00:30<00:40,  3.41s/it] 40%|████      | 8/20 [00:29<00:40,  3.39s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.25s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.31s/it] 45%|████▌     | 9/20 [00:33<00:36,  3.28s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.29s/it] 50%|█████     | 10/20 [00:36<00:33,  3.33s/it] 50%|█████     | 10/20 [00:36<00:34,  3.42s/it] 50%|█████     | 10/20 [00:37<00:37,  3.73s/it] 50%|█████     | 10/20 [00:37<00:38,  3.81s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.56s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.58s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.84s/it] 55%|█████▌    | 11/20 [00:41<00:35,  3.92s/it] 60%|██████    | 12/20 [00:45<00:31,  4.00s/it] 60%|██████    | 12/20 [00:45<00:31,  3.98s/it] 60%|██████    | 12/20 [00:46<00:33,  4.20s/it] 60%|██████    | 12/20 [00:46<00:33,  4.21s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.18s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.17s/it] 65%|██████▌   | 13/20 [00:50<00:30,  4.37s/it] 65%|██████▌   | 13/20 [00:50<00:30,  4.30s/it] 70%|███████   | 14/20 [00:54<00:24,  4.11s/it] 70%|███████   | 14/20 [00:54<00:24,  4.11s/it] 70%|███████   | 14/20 [00:54<00:24,  4.14s/it] 70%|███████   | 14/20 [00:55<00:26,  4.40s/it] 75%|███████▌  | 15/20 [00:58<00:20,  4.01s/it] 75%|███████▌  | 15/20 [00:57<00:20,  4.05s/it] 75%|███████▌  | 15/20 [00:58<00:19,  3.92s/it] 75%|███████▌  | 15/20 [00:59<00:21,  4.34s/it] 80%|████████  | 16/20 [01:00<00:14,  3.58s/it] 80%|████████  | 16/20 [01:01<00:15,  3.82s/it] 80%|████████  | 16/20 [01:01<00:15,  3.79s/it] 80%|████████  | 16/20 [01:02<00:16,  4.02s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.48s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.65s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.65s/it] 85%|████████▌ | 17/20 [01:05<00:11,  3.73s/it] 90%|█████████ | 18/20 [01:07<00:06,  3.43s/it] 90%|█████████ | 18/20 [01:07<00:07,  3.51s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.51s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.62s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.42s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.55s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.52s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.55s/it]100%|██████████| 20/20 [01:15<00:00,  3.77s/it]100%|██████████| 20/20 [01:15<00:00,  3.76s/it]
100%|██████████| 20/20 [01:15<00:00,  3.82s/it]100%|██████████| 20/20 [01:15<00:00,  3.79s/it]
100%|██████████| 20/20 [01:15<00:00,  3.76s/it]100%|██████████| 20/20 [01:15<00:00,  3.80s/it]
100%|██████████| 20/20 [01:17<00:00,  3.83s/it]100%|██████████| 20/20 [01:17<00:00,  3.86s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 22:04:32,569 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_51/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 22:04:32,569 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 22:04:32,569 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 22:04:32,569 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 22:06:20,397 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 22:06:20,398 - RUN - INFO - Evaluation Results:
2025-02-22 22:06:20,400 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.60875
13  Average  cross   0.445
14  Average  single  0.7725
--  -------  ------  -------
[rank0]:[W222 22:06:21.282673334 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 22:06:26.840426 1775393 site-packages/torch/distributed/run.py:793] 
W0222 22:06:26.840426 1775393 site-packages/torch/distributed/run.py:793] *****************************************
W0222 22:06:26.840426 1775393 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 22:06:26.840426 1775393 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank1]:[W222 22:06:51.172335153 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 22:06:51.247721550 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 22:06:51.247960002 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 22:06:51.256425358 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.10s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]
Max step: 53, bias_value: 0.2
Max step: 53, bias_value: 0.2
Max step: 53, bias_value: 0.2
Max step: 53, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:43<14:52, 223.22s/it] 20%|██        | 1/5 [04:16<17:05, 256.41s/it] 20%|██        | 1/5 [04:36<18:26, 276.53s/it] 20%|██        | 1/5 [04:52<19:30, 292.57s/it] 40%|████      | 2/5 [07:21<11:01, 220.54s/it] 40%|████      | 2/5 [08:11<11:53, 237.77s/it] 40%|████      | 2/5 [08:25<12:36, 252.05s/it] 40%|████      | 2/5 [08:26<12:27, 249.21s/it] 60%|██████    | 3/5 [11:05<07:24, 222.05s/it] 60%|██████    | 3/5 [12:43<08:25, 252.62s/it] 60%|██████    | 3/5 [13:00<08:44, 262.36s/it] 60%|██████    | 3/5 [13:19<08:58, 269.49s/it] 80%|████████  | 4/5 [14:52<03:44, 224.10s/it] 80%|████████  | 4/5 [17:53<04:35, 275.44s/it] 80%|████████  | 4/5 [17:54<04:35, 275.04s/it] 80%|████████  | 4/5 [17:57<04:32, 272.97s/it]100%|██████████| 5/5 [19:32<00:00, 244.15s/it]100%|██████████| 5/5 [19:32<00:00, 234.54s/it]
100%|██████████| 5/5 [21:31<00:00, 251.56s/it]100%|██████████| 5/5 [21:31<00:00, 258.23s/it]
100%|██████████| 5/5 [21:58<00:00, 264.36s/it]100%|██████████| 5/5 [21:58<00:00, 263.69s/it]
100%|██████████| 5/5 [22:10<00:00, 268.17s/it]100%|██████████| 5/5 [22:10<00:00, 266.10s/it]
[rank0]:[W222 22:30:29.828902880 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 22:30:36.156067 1831319 site-packages/torch/distributed/run.py:793] 
W0222 22:30:36.156067 1831319 site-packages/torch/distributed/run.py:793] *****************************************
W0222 22:30:36.156067 1831319 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 22:30:36.156067 1831319 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank2]:[W222 22:31:03.507845779 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 22:31:03.507845106 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 22:31:03.508774146 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 22:31:32.092825223 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.07s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.03s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.34it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.28it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.35it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_53/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Max step: 50, bias_value: 0.2Max step: 50, bias_value: 0.2Max step: 50, bias_value: 0.2


load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_53/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_53/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_53/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonRank 0: 

 Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
  0%|          | 0/20 [00:00<?, ?it/s]Rank 0:  Model Class: LlavaQwenForCausalLM
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<01:55,  6.08s/it]  0%|          | 0/20 [00:00<?, ?it/s] 10%|█         | 2/20 [00:09<01:19,  4.44s/it]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s] 15%|█▌        | 3/20 [00:12<01:03,  3.72s/it]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:05<01:47,  5.64s/it] 20%|██        | 4/20 [00:16<01:00,  3.79s/it]  5%|▌         | 1/20 [00:06<02:06,  6.68s/it]  5%|▌         | 1/20 [00:06<02:12,  6.98s/it] 10%|█         | 2/20 [00:09<01:21,  4.50s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.57s/it] 10%|█         | 2/20 [00:09<01:24,  4.70s/it] 10%|█         | 2/20 [00:10<01:28,  4.91s/it] 15%|█▌        | 3/20 [00:11<01:01,  3.61s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.70s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.82s/it] 30%|███       | 6/20 [00:22<00:48,  3.47s/it] 20%|██        | 4/20 [00:15<00:59,  3.73s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.15s/it] 20%|██        | 4/20 [00:16<00:58,  3.63s/it] 20%|██        | 4/20 [00:16<01:02,  3.88s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.62s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.47s/it] 40%|████      | 8/20 [00:28<00:40,  3.37s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.67s/it] 30%|███       | 6/20 [00:22<00:48,  3.44s/it] 45%|████▌     | 9/20 [00:31<00:35,  3.27s/it] 30%|███       | 6/20 [00:22<00:47,  3.42s/it] 30%|███       | 6/20 [00:23<00:50,  3.59s/it] 35%|███▌      | 7/20 [00:25<00:42,  3.23s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.14s/it] 50%|█████     | 10/20 [00:35<00:33,  3.34s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.27s/it] 40%|████      | 8/20 [00:28<00:38,  3.25s/it] 40%|████      | 8/20 [00:28<00:40,  3.38s/it] 40%|████      | 8/20 [00:30<00:41,  3.45s/it] 55%|█████▌    | 11/20 [00:39<00:32,  3.61s/it] 45%|████▌     | 9/20 [00:31<00:35,  3.27s/it] 45%|████▌     | 9/20 [00:31<00:35,  3.19s/it] 45%|████▌     | 9/20 [00:33<00:36,  3.32s/it] 60%|██████    | 12/20 [00:44<00:32,  4.04s/it] 50%|█████     | 10/20 [00:36<00:36,  3.70s/it] 50%|█████     | 10/20 [00:36<00:37,  3.79s/it] 50%|█████     | 10/20 [00:37<00:35,  3.54s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.24s/it] 55%|█████▌    | 11/20 [00:40<00:34,  3.80s/it] 55%|█████▌    | 11/20 [00:41<00:35,  3.91s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.78s/it] 70%|███████   | 14/20 [00:53<00:24,  4.13s/it] 60%|██████    | 12/20 [00:45<00:32,  4.10s/it] 60%|██████    | 12/20 [00:45<00:33,  4.22s/it] 60%|██████    | 12/20 [00:46<00:33,  4.16s/it] 75%|███████▌  | 15/20 [00:57<00:20,  4.02s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.17s/it] 80%|████████  | 16/20 [00:59<00:14,  3.60s/it] 65%|██████▌   | 13/20 [00:50<00:30,  4.39s/it] 65%|██████▌   | 13/20 [00:51<00:30,  4.29s/it] 70%|███████   | 14/20 [00:53<00:24,  4.01s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.52s/it] 70%|███████   | 14/20 [00:55<00:26,  4.41s/it] 70%|███████   | 14/20 [00:55<00:25,  4.24s/it] 75%|███████▌  | 15/20 [00:56<00:18,  3.77s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.46s/it] 75%|███████▌  | 15/20 [00:58<00:20,  4.06s/it] 75%|███████▌  | 15/20 [00:59<00:21,  4.39s/it] 80%|████████  | 16/20 [00:59<00:14,  3.70s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.44s/it] 80%|████████  | 16/20 [01:02<00:15,  3.87s/it] 80%|████████  | 16/20 [01:02<00:16,  4.02s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.57s/it]100%|██████████| 20/20 [01:14<00:00,  3.82s/it]100%|██████████| 20/20 [01:14<00:00,  3.72s/it]
 85%|████████▌ | 17/20 [01:05<00:11,  3.69s/it] 85%|████████▌ | 17/20 [01:05<00:11,  3.74s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.43s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.55s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.63s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.49s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.58s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.57s/it]100%|██████████| 20/20 [01:14<00:00,  3.70s/it]100%|██████████| 20/20 [01:14<00:00,  3.71s/it]
100%|██████████| 20/20 [01:16<00:00,  3.83s/it]100%|██████████| 20/20 [01:16<00:00,  3.85s/it]
100%|██████████| 20/20 [01:16<00:00,  3.85s/it]100%|██████████| 20/20 [01:16<00:00,  3.85s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 22:34:11,587 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_53/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 22:34:11,587 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 22:34:11,588 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 22:34:11,588 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 22:36:45,504 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 22:36:45,504 - RUN - INFO - Evaluation Results:
2025-02-22 22:36:45,506 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        single  0.76
 2  0        cross   0.55
 3  1        all     0.605
 4  1        single  0.79
 5  1        cross   0.42
 6  2        all     0.58
 7  2        single  0.76
 8  2        cross   0.4
 9  3        all     0.595
10  3        single  0.78
11  3        cross   0.41
12  Average  all     0.60875
13  Average  single  0.7725
14  Average  cross   0.445
--  -------  ------  -------
[rank0]:[W222 22:36:46.331533318 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 22:36:52.055753 1844277 site-packages/torch/distributed/run.py:793] 
W0222 22:36:52.055753 1844277 site-packages/torch/distributed/run.py:793] *****************************************
W0222 22:36:52.055753 1844277 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 22:36:52.055753 1844277 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank2]:[W222 22:37:19.697733972 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 22:37:19.697736975 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 22:37:19.697760277 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 22:37:19.703627760 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.28s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.40s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]
Max step: 55, bias_value: 0.2
Max step: 55, bias_value: 0.2
Max step: 55, bias_value: 0.2
Max step: 55, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:44<14:57, 224.33s/it] 20%|██        | 1/5 [04:17<17:11, 257.86s/it] 20%|██        | 1/5 [04:40<18:43, 280.87s/it] 20%|██        | 1/5 [05:00<20:01, 300.38s/it] 40%|████      | 2/5 [07:24<11:05, 221.94s/it] 40%|████      | 2/5 [08:17<11:59, 239.89s/it] 40%|████      | 2/5 [08:28<12:40, 253.37s/it] 40%|████      | 2/5 [08:35<12:40, 253.64s/it] 60%|██████    | 3/5 [11:11<07:28, 224.17s/it] 60%|██████    | 3/5 [12:57<08:35, 257.60s/it] 60%|██████    | 3/5 [13:06<08:49, 264.64s/it] 60%|██████    | 3/5 [13:30<09:05, 272.86s/it] 80%|████████  | 4/5 [15:00<03:46, 226.00s/it] 80%|████████  | 4/5 [18:07<04:39, 279.10s/it] 80%|████████  | 4/5 [18:11<04:39, 279.76s/it] 80%|████████  | 4/5 [18:13<04:36, 276.99s/it]100%|██████████| 5/5 [19:42<00:00, 246.32s/it]100%|██████████| 5/5 [19:42<00:00, 236.52s/it]
100%|██████████| 5/5 [21:48<00:00, 254.75s/it]100%|██████████| 5/5 [21:48<00:00, 261.73s/it]
100%|██████████| 5/5 [22:19<00:00, 268.22s/it]100%|██████████| 5/5 [22:19<00:00, 267.85s/it]
100%|██████████| 5/5 [22:24<00:00, 271.01s/it]100%|██████████| 5/5 [22:24<00:00, 268.81s/it]
[rank0]:[W222 23:01:13.534737999 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 23:01:20.103578 1896506 site-packages/torch/distributed/run.py:793] 
W0222 23:01:20.103578 1896506 site-packages/torch/distributed/run.py:793] *****************************************
W0222 23:01:20.103578 1896506 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 23:01:20.103578 1896506 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W222 23:01:47.393525894 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 23:01:47.393757375 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 23:01:47.394719317 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 23:02:16.024722091 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.59s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.49s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.60s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_55/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2Max step: 50, bias_value: 0.2Max step: 50, bias_value: 0.2


load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_55/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_55/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_55/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:03,  6.48s/it]  5%|▌         | 1/20 [00:07<02:16,  7.16s/it]  5%|▌         | 1/20 [00:07<02:17,  7.23s/it]  5%|▌         | 1/20 [00:07<02:19,  7.36s/it] 10%|█         | 2/20 [00:10<01:25,  4.75s/it] 10%|█         | 2/20 [00:10<01:27,  4.88s/it] 10%|█         | 2/20 [00:10<01:29,  4.98s/it] 10%|█         | 2/20 [00:10<01:29,  4.96s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.73s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.73s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.83s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.85s/it] 20%|██        | 4/20 [00:16<00:59,  3.75s/it] 20%|██        | 4/20 [00:16<01:00,  3.77s/it] 20%|██        | 4/20 [00:16<00:59,  3.71s/it] 20%|██        | 4/20 [00:17<01:01,  3.87s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.62s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.53s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.49s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.69s/it] 30%|███       | 6/20 [00:22<00:47,  3.42s/it] 30%|███       | 6/20 [00:22<00:47,  3.42s/it] 30%|███       | 6/20 [00:23<00:47,  3.42s/it] 30%|███       | 6/20 [00:23<00:49,  3.54s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.20s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.14s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.14s/it] 35%|███▌      | 7/20 [00:26<00:41,  3.20s/it] 40%|████      | 8/20 [00:29<00:38,  3.24s/it] 40%|████      | 8/20 [00:29<00:39,  3.31s/it] 40%|████      | 8/20 [00:29<00:40,  3.40s/it] 40%|████      | 8/20 [00:29<00:40,  3.37s/it] 45%|████▌     | 9/20 [00:32<00:34,  3.16s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.21s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.29s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.24s/it] 50%|█████     | 10/20 [00:35<00:32,  3.26s/it] 50%|█████     | 10/20 [00:36<00:33,  3.37s/it] 50%|█████     | 10/20 [00:36<00:36,  3.64s/it] 50%|█████     | 10/20 [00:37<00:37,  3.73s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.49s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.60s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.73s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.83s/it] 60%|██████    | 12/20 [00:44<00:30,  3.87s/it] 60%|██████    | 12/20 [00:45<00:31,  3.99s/it] 60%|██████    | 12/20 [00:45<00:31,  3.95s/it] 60%|██████    | 12/20 [00:45<00:32,  4.09s/it] 65%|██████▌   | 13/20 [00:48<00:28,  4.04s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.09s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.07s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.28s/it] 70%|███████   | 14/20 [00:52<00:23,  3.99s/it] 70%|███████   | 14/20 [00:53<00:23,  3.95s/it] 70%|███████   | 14/20 [00:53<00:24,  4.06s/it] 70%|███████   | 14/20 [00:54<00:25,  4.29s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.94s/it] 75%|███████▌  | 15/20 [00:56<00:18,  3.72s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.88s/it] 75%|███████▌  | 15/20 [00:58<00:21,  4.23s/it] 80%|████████  | 16/20 [00:59<00:14,  3.53s/it] 80%|████████  | 16/20 [00:59<00:14,  3.60s/it] 80%|████████  | 16/20 [01:00<00:14,  3.67s/it] 80%|████████  | 16/20 [01:02<00:15,  3.90s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.45s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.48s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.53s/it] 85%|████████▌ | 17/20 [01:05<00:10,  3.64s/it] 90%|█████████ | 18/20 [01:05<00:06,  3.39s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.37s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.45s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.51s/it] 95%|█████████▌| 19/20 [01:08<00:03,  3.35s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.43s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.48s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.49s/it]100%|██████████| 20/20 [01:13<00:00,  3.71s/it]100%|██████████| 20/20 [01:13<00:00,  3.67s/it]
100%|██████████| 20/20 [01:13<00:00,  3.64s/it]100%|██████████| 20/20 [01:13<00:00,  3.69s/it]
100%|██████████| 20/20 [01:14<00:00,  3.71s/it]100%|██████████| 20/20 [01:14<00:00,  3.73s/it]
100%|██████████| 20/20 [01:16<00:00,  3.73s/it]100%|██████████| 20/20 [01:16<00:00,  3.80s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 23:05:04,407 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_55/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 23:05:04,407 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 23:05:04,407 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 23:05:04,407 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 23:07:41,961 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 23:07:41,961 - RUN - INFO - Evaluation Results:
2025-02-22 23:07:41,963 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.60875
13  Average  cross   0.445
14  Average  single  0.7725
--  -------  ------  -------
[rank0]:[W222 23:07:42.761779477 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 23:07:48.269579 1907166 site-packages/torch/distributed/run.py:793] 
W0222 23:07:48.269579 1907166 site-packages/torch/distributed/run.py:793] *****************************************
W0222 23:07:48.269579 1907166 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 23:07:48.269579 1907166 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank0]:[W222 23:08:14.592183615 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 23:08:14.595575477 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 23:08:14.619944436 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 23:08:14.622347768 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.27s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]
Max step: 57, bias_value: 0.2
Max step: 57, bias_value: 0.2
Max step: 57, bias_value: 0.2
Max step: 57, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:46<15:04, 226.25s/it] 20%|██        | 1/5 [04:27<17:49, 267.40s/it] 20%|██        | 1/5 [04:46<19:04, 286.19s/it] 20%|██        | 1/5 [05:03<20:15, 303.76s/it] 40%|████      | 2/5 [07:31<11:16, 225.40s/it] 40%|████      | 2/5 [08:22<12:05, 241.93s/it] 40%|████      | 2/5 [08:44<12:54, 258.03s/it] 40%|████      | 2/5 [08:45<13:05, 261.74s/it] 60%|██████    | 3/5 [11:20<07:34, 227.41s/it] 60%|██████    | 3/5 [13:10<08:42, 261.50s/it] 60%|██████    | 3/5 [13:27<09:02, 271.13s/it] 60%|██████    | 3/5 [13:42<09:15, 277.59s/it] 80%|████████  | 4/5 [15:12<03:49, 229.22s/it] 80%|████████  | 4/5 [18:25<04:42, 282.64s/it] 80%|████████  | 4/5 [18:33<04:42, 282.80s/it] 80%|████████  | 4/5 [18:34<04:45, 285.43s/it]100%|██████████| 5/5 [19:59<00:00, 250.01s/it]100%|██████████| 5/5 [19:59<00:00, 239.95s/it]
100%|██████████| 5/5 [22:12<00:00, 259.87s/it]100%|██████████| 5/5 [22:12<00:00, 266.49s/it]
100%|██████████| 5/5 [22:36<00:00, 271.39s/it]100%|██████████| 5/5 [22:36<00:00, 271.32s/it]
100%|██████████| 5/5 [22:54<00:00, 276.13s/it]100%|██████████| 5/5 [22:54<00:00, 274.89s/it]
[rank0]:[W222 23:32:38.443995134 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 23:32:45.113211 1958797 site-packages/torch/distributed/run.py:793] 
W0222 23:32:45.113211 1958797 site-packages/torch/distributed/run.py:793] *****************************************
W0222 23:32:45.113211 1958797 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 23:32:45.113211 1958797 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank2]:[W222 23:33:12.029417566 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W222 23:33:12.046704277 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 23:33:12.048336912 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W222 23:33:42.045809370 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.47s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.37s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.54s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_57/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_57/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_57/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_57/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<01:59,  6.27s/it]  5%|▌         | 1/20 [00:07<02:13,  7.01s/it]  5%|▌         | 1/20 [00:07<02:16,  7.18s/it]  5%|▌         | 1/20 [00:07<02:14,  7.10s/it] 10%|█         | 2/20 [00:09<01:23,  4.64s/it] 10%|█         | 2/20 [00:10<01:26,  4.79s/it] 10%|█         | 2/20 [00:10<01:28,  4.92s/it] 10%|█         | 2/20 [00:10<01:29,  4.95s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.68s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.76s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.77s/it] 15%|█▌        | 3/20 [00:13<01:04,  3.82s/it] 20%|██        | 4/20 [00:16<00:59,  3.75s/it] 20%|██        | 4/20 [00:16<00:58,  3.68s/it] 20%|██        | 4/20 [00:16<01:01,  3.83s/it] 20%|██        | 4/20 [00:16<01:01,  3.87s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.47s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.64s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.57s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.70s/it] 30%|███       | 6/20 [00:22<00:47,  3.42s/it] 30%|███       | 6/20 [00:22<00:47,  3.39s/it] 30%|███       | 6/20 [00:23<00:48,  3.48s/it] 30%|███       | 6/20 [00:23<00:49,  3.55s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.13s/it] 35%|███▌      | 7/20 [00:25<00:42,  3.24s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.18s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.25s/it] 40%|████      | 8/20 [00:28<00:38,  3.21s/it] 40%|████      | 8/20 [00:29<00:40,  3.39s/it] 40%|████      | 8/20 [00:29<00:40,  3.35s/it] 40%|████      | 8/20 [00:29<00:40,  3.40s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.13s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.28s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.23s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.27s/it] 50%|█████     | 10/20 [00:36<00:33,  3.33s/it] 50%|█████     | 10/20 [00:36<00:36,  3.63s/it] 50%|█████     | 10/20 [00:36<00:34,  3.43s/it] 50%|█████     | 10/20 [00:36<00:37,  3.73s/it] 55%|█████▌    | 11/20 [00:42<00:38,  4.24s/it] 55%|█████▌    | 11/20 [00:42<00:38,  4.29s/it] 55%|█████▌    | 11/20 [00:42<00:39,  4.37s/it] 55%|█████▌    | 11/20 [00:42<00:37,  4.11s/it] 60%|██████    | 12/20 [00:49<00:40,  5.02s/it] 60%|██████    | 12/20 [00:49<00:40,  5.10s/it] 60%|██████    | 12/20 [00:49<00:40,  5.01s/it] 60%|██████    | 12/20 [00:49<00:41,  5.17s/it] 65%|██████▌   | 13/20 [00:53<00:34,  4.88s/it] 65%|██████▌   | 13/20 [00:53<00:33,  4.83s/it] 65%|██████▌   | 13/20 [00:53<00:34,  4.93s/it] 65%|██████▌   | 13/20 [00:54<00:34,  4.98s/it] 70%|███████   | 14/20 [00:57<00:26,  4.49s/it] 70%|███████   | 14/20 [00:58<00:27,  4.63s/it] 70%|███████   | 14/20 [00:57<00:27,  4.60s/it] 70%|███████   | 14/20 [00:58<00:28,  4.75s/it] 75%|███████▌  | 15/20 [01:00<00:20,  4.09s/it] 75%|███████▌  | 15/20 [01:01<00:21,  4.27s/it] 75%|███████▌  | 15/20 [01:01<00:21,  4.35s/it] 75%|███████▌  | 15/20 [01:02<00:22,  4.55s/it] 80%|████████  | 16/20 [01:03<00:15,  3.85s/it] 80%|████████  | 16/20 [01:04<00:15,  3.82s/it] 80%|████████  | 16/20 [01:04<00:15,  3.98s/it] 80%|████████  | 16/20 [01:05<00:16,  4.13s/it] 85%|████████▌ | 17/20 [01:06<00:10,  3.65s/it] 85%|████████▌ | 17/20 [01:07<00:10,  3.64s/it] 85%|████████▌ | 17/20 [01:08<00:11,  3.78s/it] 85%|████████▌ | 17/20 [01:08<00:11,  3.81s/it] 90%|█████████ | 18/20 [01:09<00:06,  3.50s/it] 90%|█████████ | 18/20 [01:10<00:07,  3.54s/it] 90%|█████████ | 18/20 [01:11<00:07,  3.59s/it] 90%|█████████ | 18/20 [01:11<00:07,  3.65s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.53s/it] 95%|█████████▌| 19/20 [01:14<00:03,  3.46s/it] 95%|█████████▌| 19/20 [01:14<00:03,  3.57s/it] 95%|█████████▌| 19/20 [01:15<00:03,  3.58s/it]100%|██████████| 20/20 [01:17<00:00,  3.68s/it]100%|██████████| 20/20 [01:17<00:00,  3.88s/it]
100%|██████████| 20/20 [01:18<00:00,  3.76s/it]100%|██████████| 20/20 [01:18<00:00,  3.93s/it]
100%|██████████| 20/20 [01:18<00:00,  3.77s/it]100%|██████████| 20/20 [01:18<00:00,  3.95s/it]
100%|██████████| 20/20 [01:19<00:00,  3.79s/it]100%|██████████| 20/20 [01:19<00:00,  3.98s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-22 23:36:34,522 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_57/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-22 23:36:34,523 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-22 23:36:34,523 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-22 23:36:34,523 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-22 23:38:55,067 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-22 23:38:55,067 - RUN - INFO - Evaluation Results:
2025-02-22 23:38:55,069 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        single  0.76
 2  0        cross   0.55
 3  1        all     0.605
 4  1        single  0.79
 5  1        cross   0.42
 6  2        all     0.58
 7  2        single  0.76
 8  2        cross   0.4
 9  3        all     0.595
10  3        single  0.78
11  3        cross   0.41
12  Average  all     0.60875
13  Average  single  0.7725
14  Average  cross   0.445
--  -------  ------  -------
[rank0]:[W222 23:38:55.056521809 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0222 23:39:01.697385 1972207 site-packages/torch/distributed/run.py:793] 
W0222 23:39:01.697385 1972207 site-packages/torch/distributed/run.py:793] *****************************************
W0222 23:39:01.697385 1972207 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0222 23:39:01.697385 1972207 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank1]:[W222 23:39:29.334490737 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W222 23:39:29.335029275 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W222 23:39:29.336716417 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W222 23:39:29.341274812 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.39s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.39s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.39s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]
Max step: 59, bias_value: 0.2
Max step: 59, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 59, bias_value: 0.2
Max step: 59, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:48<15:12, 228.18s/it] 20%|██        | 1/5 [04:28<17:55, 268.82s/it] 20%|██        | 1/5 [04:50<19:23, 290.91s/it] 20%|██        | 1/5 [05:04<20:19, 304.84s/it] 40%|████      | 2/5 [07:36<11:24, 228.10s/it] 40%|████      | 2/5 [08:23<12:06, 242.10s/it] 40%|████      | 2/5 [08:51<13:15, 265.15s/it] 40%|████      | 2/5 [08:53<13:08, 262.71s/it] 60%|██████    | 3/5 [11:29<07:41, 230.68s/it] 60%|██████    | 3/5 [13:22<08:50, 265.35s/it] 60%|██████    | 3/5 [13:37<09:09, 274.93s/it] 60%|██████    | 3/5 [13:44<09:17, 278.51s/it] 80%|████████  | 4/5 [15:25<03:52, 232.40s/it] 80%|████████  | 4/5 [18:39<04:44, 284.96s/it] 80%|████████  | 4/5 [18:42<04:46, 286.94s/it] 80%|████████  | 4/5 [18:48<04:49, 289.18s/it]100%|██████████| 5/5 [20:19<00:00, 254.86s/it]100%|██████████| 5/5 [20:19<00:00, 243.94s/it]
100%|██████████| 5/5 [22:21<00:00, 262.12s/it]100%|██████████| 5/5 [22:21<00:00, 268.26s/it]
100%|██████████| 5/5 [22:57<00:00, 275.29s/it]100%|██████████| 5/5 [22:57<00:00, 275.41s/it]
100%|██████████| 5/5 [23:10<00:00, 279.24s/it]100%|██████████| 5/5 [23:10<00:00, 278.12s/it]
[rank0]:[W223 00:04:10.155050456 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 00:04:16.307188 2030708 site-packages/torch/distributed/run.py:793] 
W0223 00:04:16.307188 2030708 site-packages/torch/distributed/run.py:793] *****************************************
W0223 00:04:16.307188 2030708 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 00:04:16.307188 2030708 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W223 00:04:43.108614670 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 00:04:43.109464420 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 00:04:43.111661181 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 00:05:13.404159482 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.48s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.47s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.45s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_59/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_59/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_59/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_59/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<01:59,  6.28s/it]  5%|▌         | 1/20 [00:07<02:15,  7.15s/it]  5%|▌         | 1/20 [00:06<02:12,  6.97s/it]  5%|▌         | 1/20 [00:06<02:11,  6.93s/it] 10%|█         | 2/20 [00:09<01:25,  4.72s/it] 10%|█         | 2/20 [00:10<01:27,  4.86s/it] 10%|█         | 2/20 [00:10<01:26,  4.79s/it] 10%|█         | 2/20 [00:10<01:27,  4.87s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.73s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.69s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.79s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.77s/it] 20%|██        | 4/20 [00:16<00:59,  3.70s/it] 20%|██        | 4/20 [00:16<01:00,  3.79s/it] 20%|██        | 4/20 [00:16<01:00,  3.79s/it] 20%|██        | 4/20 [00:16<01:01,  3.85s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.48s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.54s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.65s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.69s/it] 30%|███       | 6/20 [00:22<00:47,  3.39s/it] 30%|███       | 6/20 [00:22<00:48,  3.43s/it] 30%|███       | 6/20 [00:22<00:47,  3.42s/it] 30%|███       | 6/20 [00:23<00:49,  3.55s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.11s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.16s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.22s/it] 35%|███▌      | 7/20 [00:26<00:41,  3.22s/it] 40%|████      | 8/20 [00:28<00:38,  3.22s/it] 40%|████      | 8/20 [00:29<00:40,  3.34s/it] 40%|████      | 8/20 [00:29<00:40,  3.37s/it] 40%|████      | 8/20 [00:29<00:40,  3.36s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.13s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.22s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.27s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.25s/it] 50%|█████     | 10/20 [00:35<00:32,  3.27s/it] 50%|█████     | 10/20 [00:36<00:36,  3.63s/it] 50%|█████     | 10/20 [00:36<00:33,  3.37s/it] 50%|█████     | 10/20 [00:36<00:37,  3.71s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.50s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.74s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.59s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.84s/it] 60%|██████    | 12/20 [00:44<00:30,  3.86s/it] 60%|██████    | 12/20 [00:45<00:31,  4.00s/it] 60%|██████    | 12/20 [00:45<00:31,  3.95s/it] 60%|██████    | 12/20 [00:45<00:33,  4.13s/it] 65%|██████▌   | 13/20 [00:48<00:28,  4.06s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.11s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.08s/it] 65%|██████▌   | 13/20 [00:50<00:30,  4.36s/it] 70%|███████   | 14/20 [00:52<00:23,  3.97s/it] 70%|███████   | 14/20 [00:52<00:23,  3.91s/it] 70%|███████   | 14/20 [00:53<00:24,  4.03s/it] 70%|███████   | 14/20 [00:55<00:26,  4.37s/it] 75%|███████▌  | 15/20 [00:56<00:18,  3.71s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.98s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.87s/it] 80%|████████  | 16/20 [00:59<00:14,  3.54s/it] 80%|████████  | 16/20 [00:59<00:14,  3.58s/it] 75%|███████▌  | 15/20 [00:59<00:21,  4.30s/it] 80%|████████  | 16/20 [01:00<00:14,  3.74s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.46s/it] 80%|████████  | 16/20 [01:02<00:15,  3.93s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.48s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.58s/it] 85%|████████▌ | 17/20 [01:05<00:11,  3.68s/it] 90%|█████████ | 18/20 [01:05<00:06,  3.38s/it] 90%|█████████ | 18/20 [01:05<00:06,  3.40s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.44s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.57s/it] 95%|█████████▌| 19/20 [01:08<00:03,  3.35s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.44s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.47s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.51s/it]100%|██████████| 20/20 [01:13<00:00,  3.70s/it]100%|██████████| 20/20 [01:13<00:00,  3.67s/it]
100%|██████████| 20/20 [01:13<00:00,  3.64s/it]100%|██████████| 20/20 [01:13<00:00,  3.67s/it]
100%|██████████| 20/20 [01:14<00:00,  3.70s/it]100%|██████████| 20/20 [01:14<00:00,  3.73s/it]
100%|██████████| 20/20 [01:16<00:00,  3.75s/it]100%|██████████| 20/20 [01:16<00:00,  3.82s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 00:08:01,402 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_59/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 00:08:01,402 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 00:08:01,402 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 00:08:01,402 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 00:10:01,554 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 00:10:01,554 - RUN - INFO - Evaluation Results:
2025-02-23 00:10:01,556 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        single  0.76
 2  0        cross   0.55
 3  1        all     0.605
 4  1        single  0.79
 5  1        cross   0.42
 6  2        all     0.58
 7  2        single  0.76
 8  2        cross   0.4
 9  3        all     0.595
10  3        single  0.78
11  3        cross   0.41
12  Average  all     0.60875
13  Average  single  0.7725
14  Average  cross   0.445
--  -------  ------  -------
[rank0]:[W223 00:10:02.373877937 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 00:10:07.998035 2040781 site-packages/torch/distributed/run.py:793] 
W0223 00:10:07.998035 2040781 site-packages/torch/distributed/run.py:793] *****************************************
W0223 00:10:07.998035 2040781 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 00:10:07.998035 2040781 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank0]:[W223 00:10:34.828199331 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 00:10:34.838655831 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 00:10:34.839072548 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 00:10:34.839087798 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.10s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.03s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.64it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.27it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.63it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.63it/s]

Max step: 61, bias_value: 0.2
Max step: 61, bias_value: 0.2
Max step: 61, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 61, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:49<15:19, 229.83s/it] 20%|██        | 1/5 [04:34<18:16, 274.01s/it] 20%|██        | 1/5 [04:55<19:41, 295.35s/it] 20%|██        | 1/5 [05:09<20:36, 309.23s/it] 40%|████      | 2/5 [07:42<11:34, 231.53s/it] 40%|████      | 2/5 [08:27<12:11, 243.94s/it] 40%|████      | 2/5 [08:58<13:24, 268.28s/it] 40%|████      | 2/5 [09:03<13:22, 267.35s/it] 60%|██████    | 3/5 [11:38<07:47, 233.52s/it] 60%|██████    | 3/5 [13:36<08:59, 269.95s/it] 60%|██████    | 3/5 [13:48<09:16, 278.36s/it] 60%|██████    | 3/5 [13:58<09:27, 283.88s/it] 80%|████████  | 4/5 [15:34<03:54, 234.71s/it] 80%|████████  | 4/5 [18:59<04:50, 290.44s/it] 80%|████████  | 4/5 [19:00<04:51, 291.28s/it] 80%|████████  | 4/5 [19:09<04:55, 295.14s/it]100%|██████████| 5/5 [20:31<00:00, 257.15s/it]100%|██████████| 5/5 [20:31<00:00, 246.38s/it]
100%|██████████| 5/5 [22:38<00:00, 264.90s/it]100%|██████████| 5/5 [22:38<00:00, 271.80s/it]
100%|██████████| 5/5 [23:16<00:00, 278.61s/it]100%|██████████| 5/5 [23:16<00:00, 279.25s/it]
100%|██████████| 5/5 [23:32<00:00, 283.56s/it]100%|██████████| 5/5 [23:32<00:00, 282.50s/it]
[rank0]:[W223 00:35:37.183005603 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 00:35:44.763931 2098105 site-packages/torch/distributed/run.py:793] 
W0223 00:35:44.763931 2098105 site-packages/torch/distributed/run.py:793] *****************************************
W0223 00:35:44.763931 2098105 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 00:35:44.763931 2098105 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank3]:[W223 00:36:15.199363757 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 00:36:15.204022799 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 00:36:15.207109811 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 00:36:46.391971378 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_61/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_61/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_61/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_61/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json


Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:01,  6.41s/it]  5%|▌         | 1/20 [00:07<02:14,  7.06s/it]  5%|▌         | 1/20 [00:07<02:16,  7.16s/it]  5%|▌         | 1/20 [00:07<02:19,  7.35s/it] 10%|█         | 2/20 [00:10<01:26,  4.83s/it] 10%|█         | 2/20 [00:10<01:27,  4.88s/it] 10%|█         | 2/20 [00:10<01:29,  4.96s/it] 10%|█         | 2/20 [00:10<01:30,  5.02s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.77s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.75s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.84s/it] 15%|█▌        | 3/20 [00:13<01:06,  3.90s/it] 20%|██        | 4/20 [00:16<01:01,  3.83s/it] 20%|██        | 4/20 [00:16<01:00,  3.76s/it] 20%|██        | 4/20 [00:16<01:02,  3.89s/it] 20%|██        | 4/20 [00:17<01:01,  3.87s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.69s/it] 25%|██▌       | 5/20 [00:20<00:53,  3.55s/it] 25%|██▌       | 5/20 [00:20<00:54,  3.63s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.71s/it] 30%|███       | 6/20 [00:23<00:48,  3.50s/it] 30%|███       | 6/20 [00:23<00:48,  3.50s/it] 30%|███       | 6/20 [00:23<00:48,  3.48s/it] 30%|███       | 6/20 [00:23<00:49,  3.55s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.17s/it] 35%|███▌      | 7/20 [00:25<00:42,  3.28s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.20s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.28s/it] 40%|████      | 8/20 [00:29<00:39,  3.28s/it] 40%|████      | 8/20 [00:29<00:41,  3.43s/it] 40%|████      | 8/20 [00:29<00:40,  3.39s/it] 40%|████      | 8/20 [00:30<00:40,  3.40s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.19s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.26s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.33s/it] 45%|████▌     | 9/20 [00:33<00:36,  3.29s/it] 50%|█████     | 10/20 [00:36<00:33,  3.32s/it] 50%|█████     | 10/20 [00:37<00:34,  3.49s/it] 50%|█████     | 10/20 [00:37<00:36,  3.64s/it] 50%|█████     | 10/20 [00:37<00:37,  3.78s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.60s/it] 55%|█████▌    | 11/20 [00:41<00:33,  3.76s/it] 55%|█████▌    | 11/20 [00:41<00:33,  3.70s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.88s/it] 60%|██████    | 12/20 [00:45<00:32,  4.04s/it] 60%|██████    | 12/20 [00:45<00:32,  4.04s/it] 60%|██████    | 12/20 [00:45<00:32,  4.02s/it] 60%|██████    | 12/20 [00:46<00:33,  4.22s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.19s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.16s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.16s/it] 65%|██████▌   | 13/20 [00:51<00:30,  4.38s/it] 70%|███████   | 14/20 [00:53<00:24,  4.08s/it] 70%|███████   | 14/20 [00:53<00:23,  3.99s/it] 70%|███████   | 14/20 [00:54<00:24,  4.13s/it] 70%|███████   | 14/20 [00:55<00:26,  4.38s/it] 75%|███████▌  | 15/20 [00:56<00:18,  3.74s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.96s/it] 75%|███████▌  | 15/20 [00:58<00:20,  4.06s/it] 80%|████████  | 16/20 [01:00<00:14,  3.54s/it] 75%|███████▌  | 15/20 [01:00<00:21,  4.36s/it] 80%|████████  | 16/20 [01:00<00:14,  3.67s/it] 80%|████████  | 16/20 [01:01<00:15,  3.85s/it] 80%|████████  | 16/20 [01:03<00:15,  3.97s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.47s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.54s/it] 85%|████████▌ | 17/20 [01:05<00:11,  3.67s/it] 85%|████████▌ | 17/20 [01:06<00:11,  3.71s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.40s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.41s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.55s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.59s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.36s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.48s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.57s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.57s/it]100%|██████████| 20/20 [01:14<00:00,  3.68s/it]100%|██████████| 20/20 [01:14<00:00,  3.73s/it]
100%|██████████| 20/20 [01:14<00:00,  3.82s/it]100%|██████████| 20/20 [01:14<00:00,  3.74s/it]
100%|██████████| 20/20 [01:16<00:00,  3.74s/it]100%|██████████| 20/20 [01:16<00:00,  3.80s/it]
100%|██████████| 20/20 [01:17<00:00,  3.80s/it]100%|██████████| 20/20 [01:17<00:00,  3.87s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 00:39:37,860 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_61/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 00:39:37,860 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 00:39:37,860 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 00:39:37,860 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 00:41:58,268 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 00:41:58,268 - RUN - INFO - Evaluation Results:
2025-02-23 00:41:58,270 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.60875
13  Average  cross   0.445
14  Average  single  0.7725
--  -------  ------  -------
[rank0]:[W223 00:41:58.106592637 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 00:42:05.540557 2109948 site-packages/torch/distributed/run.py:793] 
W0223 00:42:05.540557 2109948 site-packages/torch/distributed/run.py:793] *****************************************
W0223 00:42:05.540557 2109948 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 00:42:05.540557 2109948 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank1]:[W223 00:42:29.904787398 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 00:42:29.904794035 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W223 00:42:29.904790532 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 00:42:29.922795654 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.01it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.14s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.19s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.23it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]
Max step: 63, bias_value: 0.2
Max step: 63, bias_value: 0.2
Max step: 63, bias_value: 0.2
Max step: 63, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:54<15:36, 234.07s/it] 20%|██        | 1/5 [04:41<18:44, 281.20s/it] 20%|██        | 1/5 [05:00<20:02, 300.63s/it] 20%|██        | 1/5 [05:13<20:55, 313.96s/it] 40%|████      | 2/5 [07:47<11:41, 233.72s/it] 40%|████      | 2/5 [08:34<12:21, 247.04s/it] 40%|████      | 2/5 [09:09<13:31, 270.39s/it] 40%|████      | 2/5 [09:11<13:43, 274.53s/it] 60%|██████    | 3/5 [11:47<07:52, 236.41s/it] 60%|██████    | 3/5 [13:47<09:06, 273.48s/it] 60%|██████    | 3/5 [14:05<09:26, 283.44s/it] 60%|██████    | 3/5 [14:08<09:33, 286.92s/it] 80%|████████  | 4/5 [15:52<03:59, 239.76s/it] 80%|████████  | 4/5 [19:13<04:54, 294.29s/it] 80%|████████  | 4/5 [19:17<04:55, 295.64s/it] 80%|████████  | 4/5 [19:27<04:58, 298.81s/it]100%|██████████| 5/5 [20:57<00:00, 263.35s/it]100%|██████████| 5/5 [20:57<00:00, 251.45s/it]
100%|██████████| 5/5 [22:57<00:00, 268.40s/it]100%|██████████| 5/5 [22:57<00:00, 275.53s/it]
100%|██████████| 5/5 [23:35<00:00, 282.89s/it]100%|██████████| 5/5 [23:35<00:00, 283.17s/it]
100%|██████████| 5/5 [23:52<00:00, 286.68s/it]100%|██████████| 5/5 [23:52<00:00, 286.53s/it]
[rank0]:[W223 01:07:51.973575791 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 01:07:58.458294 2166229 site-packages/torch/distributed/run.py:793] 
W0223 01:07:58.458294 2166229 site-packages/torch/distributed/run.py:793] *****************************************
W0223 01:07:58.458294 2166229 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 01:07:58.458294 2166229 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W223 01:08:26.127169590 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 01:08:26.133044670 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 01:08:26.133245527 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 01:08:56.988963370 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_63/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.28s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.31s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.45s/it]
  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:05<01:41,  5.37s/it] 10%|█         | 2/20 [00:09<01:18,  4.38s/it] 15%|█▌        | 3/20 [00:11<00:59,  3.51s/it]Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_63/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_63/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_63/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 20%|██        | 4/20 [00:15<00:58,  3.63s/it] 25%|██▌       | 5/20 [00:18<00:53,  3.56s/it] 30%|███       | 6/20 [00:21<00:47,  3.39s/it] 35%|███▌      | 7/20 [00:24<00:41,  3.22s/it]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s] 40%|████      | 8/20 [00:28<00:41,  3.46s/it]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
 45%|████▌     | 9/20 [00:31<00:36,  3.33s/it]  5%|▌         | 1/20 [00:07<02:15,  7.11s/it]  5%|▌         | 1/20 [00:06<02:09,  6.81s/it]  5%|▌         | 1/20 [00:07<02:22,  7.50s/it] 10%|█         | 2/20 [00:10<01:27,  4.88s/it] 10%|█         | 2/20 [00:10<01:24,  4.71s/it] 10%|█         | 2/20 [00:10<01:31,  5.07s/it] 50%|█████     | 10/20 [00:36<00:38,  3.82s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.82s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.72s/it] 15%|█▌        | 3/20 [00:13<01:06,  3.89s/it] 55%|█████▌    | 11/20 [00:40<00:35,  3.96s/it] 20%|██        | 4/20 [00:16<00:58,  3.67s/it] 20%|██        | 4/20 [00:16<01:01,  3.86s/it] 20%|██        | 4/20 [00:17<01:02,  3.93s/it] 25%|██▌       | 5/20 [00:19<00:51,  3.46s/it] 25%|██▌       | 5/20 [00:20<00:54,  3.62s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.73s/it] 60%|██████    | 12/20 [00:45<00:34,  4.30s/it] 30%|███       | 6/20 [00:22<00:47,  3.42s/it] 30%|███       | 6/20 [00:23<00:48,  3.46s/it] 30%|███       | 6/20 [00:23<00:50,  3.58s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.18s/it] 65%|██████▌   | 13/20 [00:50<00:31,  4.47s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.20s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.26s/it] 40%|████      | 8/20 [00:28<00:39,  3.31s/it] 40%|████      | 8/20 [00:29<00:41,  3.42s/it] 40%|████      | 8/20 [00:30<00:40,  3.41s/it] 70%|███████   | 14/20 [00:55<00:26,  4.48s/it] 45%|████▌     | 9/20 [00:32<00:39,  3.56s/it] 45%|████▌     | 9/20 [00:34<00:43,  3.97s/it] 45%|████▌     | 9/20 [00:35<00:44,  4.04s/it] 75%|███████▌  | 15/20 [01:01<00:24,  4.89s/it] 50%|█████     | 10/20 [00:37<00:39,  4.00s/it] 50%|█████     | 10/20 [00:38<00:38,  3.85s/it] 80%|████████  | 16/20 [01:04<00:17,  4.36s/it] 50%|█████     | 10/20 [00:39<00:39,  3.95s/it] 55%|█████▌    | 11/20 [00:41<00:36,  4.00s/it] 85%|████████▌ | 17/20 [01:07<00:11,  3.98s/it] 55%|█████▌    | 11/20 [00:42<00:35,  3.93s/it] 55%|█████▌    | 11/20 [00:43<00:36,  4.01s/it] 90%|█████████ | 18/20 [01:10<00:07,  3.77s/it] 60%|██████    | 12/20 [00:46<00:33,  4.23s/it] 60%|██████    | 12/20 [00:47<00:33,  4.23s/it] 60%|██████    | 12/20 [00:48<00:34,  4.27s/it] 95%|█████████▌| 19/20 [01:14<00:03,  3.68s/it] 65%|██████▌   | 13/20 [00:51<00:29,  4.29s/it] 65%|██████▌   | 13/20 [00:52<00:30,  4.33s/it] 65%|██████▌   | 13/20 [00:52<00:30,  4.32s/it]100%|██████████| 20/20 [01:18<00:00,  3.97s/it]100%|██████████| 20/20 [01:18<00:00,  3.94s/it]
 70%|███████   | 14/20 [00:54<00:24,  4.08s/it] 70%|███████   | 14/20 [00:55<00:25,  4.17s/it] 70%|███████   | 14/20 [00:56<00:25,  4.22s/it] 75%|███████▌  | 15/20 [00:58<00:19,  3.83s/it] 75%|███████▌  | 15/20 [00:59<00:20,  4.08s/it] 75%|███████▌  | 15/20 [01:00<00:19,  3.99s/it] 80%|████████  | 16/20 [01:01<00:14,  3.66s/it] 80%|████████  | 16/20 [01:02<00:14,  3.63s/it] 80%|████████  | 16/20 [01:03<00:15,  3.82s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.54s/it] 85%|████████▌ | 17/20 [01:05<00:10,  3.51s/it] 85%|████████▌ | 17/20 [01:07<00:10,  3.64s/it] 90%|█████████ | 18/20 [01:07<00:06,  3.40s/it] 90%|█████████ | 18/20 [01:08<00:06,  3.42s/it] 90%|█████████ | 18/20 [01:10<00:07,  3.51s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.45s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.38s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.52s/it]100%|██████████| 20/20 [01:15<00:00,  3.65s/it]100%|██████████| 20/20 [01:15<00:00,  3.77s/it]
100%|██████████| 20/20 [01:16<00:00,  3.80s/it]100%|██████████| 20/20 [01:16<00:00,  3.84s/it]
100%|██████████| 20/20 [01:18<00:00,  3.74s/it]100%|██████████| 20/20 [01:18<00:00,  3.90s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 01:11:42,167 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_63/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 01:11:42,167 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 01:11:42,167 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 01:11:42,167 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 01:14:17,393 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 01:14:17,393 - RUN - INFO - Evaluation Results:
2025-02-23 01:14:17,397 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.60875
13  Average  cross   0.445
14  Average  single  0.7725
--  -------  ------  -------
[rank0]:[W223 01:14:18.490207334 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 01:14:24.403776 2175352 site-packages/torch/distributed/run.py:793] 
W0223 01:14:24.403776 2175352 site-packages/torch/distributed/run.py:793] *****************************************
W0223 01:14:24.403776 2175352 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 01:14:24.403776 2175352 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank3]:[W223 01:14:49.121582670 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 01:14:49.137705678 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 01:14:49.137820516 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W223 01:14:50.162026969 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.24it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]
Max step: 65, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.29s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.12s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]
  0%|          | 0/5 [00:00<?, ?it/s]Max step: 65, bias_value: 0.2
Max step: 65, bias_value: 0.2
Max step: 65, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:52<15:29, 232.27s/it] 20%|██        | 1/5 [04:48<19:15, 288.76s/it] 20%|██        | 1/5 [05:02<20:11, 302.83s/it] 20%|██        | 1/5 [05:19<21:17, 319.47s/it] 40%|████      | 2/5 [07:44<11:36, 232.10s/it] 40%|████      | 2/5 [08:36<12:22, 247.38s/it] 40%|████      | 2/5 [09:20<13:55, 278.58s/it] 40%|████      | 2/5 [09:13<13:36, 272.08s/it] 60%|██████    | 3/5 [11:45<07:52, 236.08s/it] 60%|██████    | 3/5 [14:17<09:34, 287.03s/it] 60%|██████    | 3/5 [13:55<09:13, 276.74s/it] 60%|██████    | 3/5 [14:16<09:39, 289.77s/it] 80%|████████  | 4/5 [15:46<03:58, 238.35s/it] 80%|████████  | 4/5 [19:46<05:03, 303.68s/it] 80%|████████  | 4/5 [19:26<04:58, 298.17s/it] 80%|████████  | 4/5 [19:27<04:58, 298.07s/it]100%|██████████| 5/5 [20:52<00:00, 262.49s/it]100%|██████████| 5/5 [20:52<00:00, 250.44s/it]
100%|██████████| 5/5 [23:12<00:00, 271.92s/it]100%|██████████| 5/5 [23:12<00:00, 278.59s/it]
100%|██████████| 5/5 [24:16<00:00, 291.70s/it]100%|██████████| 5/5 [24:16<00:00, 291.40s/it]
100%|██████████| 5/5 [23:56<00:00, 287.90s/it]100%|██████████| 5/5 [23:56<00:00, 287.27s/it]
[rank0]:[W223 01:40:18.797828955 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 01:40:25.711970 2231068 site-packages/torch/distributed/run.py:793] 
W0223 01:40:25.711970 2231068 site-packages/torch/distributed/run.py:793] *****************************************
W0223 01:40:25.711970 2231068 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 01:40:25.711970 2231068 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank3]:[W223 01:40:51.382782468 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 01:40:51.382788258 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 01:40:51.388088170 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 01:41:21.310627198 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.51s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.45s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.50s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_65/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_65/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_65/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_65/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:01,  6.40s/it]  5%|▌         | 1/20 [00:06<02:12,  6.99s/it]  5%|▌         | 1/20 [00:07<02:14,  7.09s/it]  5%|▌         | 1/20 [00:07<02:16,  7.18s/it] 10%|█         | 2/20 [00:09<01:25,  4.73s/it] 10%|█         | 2/20 [00:10<01:26,  4.82s/it] 10%|█         | 2/20 [00:10<01:26,  4.82s/it] 10%|█         | 2/20 [00:10<01:29,  4.97s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.75s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.71s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.77s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.88s/it] 20%|██        | 4/20 [00:16<01:00,  3.78s/it] 20%|██        | 4/20 [00:16<00:59,  3.70s/it] 20%|██        | 4/20 [00:16<01:00,  3.79s/it] 20%|██        | 4/20 [00:17<01:02,  3.91s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.62s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.51s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.55s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.69s/it] 30%|███       | 6/20 [00:22<00:48,  3.45s/it] 30%|███       | 6/20 [00:22<00:48,  3.43s/it] 30%|███       | 6/20 [00:22<00:48,  3.44s/it] 30%|███       | 6/20 [00:23<00:49,  3.54s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.22s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.15s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.18s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.25s/it] 40%|████      | 8/20 [00:28<00:39,  3.26s/it] 40%|████      | 8/20 [00:29<00:39,  3.31s/it] 40%|████      | 8/20 [00:29<00:40,  3.38s/it] 40%|████      | 8/20 [00:30<00:40,  3.38s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.17s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.20s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.28s/it] 45%|████▌     | 9/20 [00:33<00:35,  3.26s/it] 50%|█████     | 10/20 [00:35<00:32,  3.28s/it] 50%|█████     | 10/20 [00:36<00:33,  3.39s/it] 50%|█████     | 10/20 [00:36<00:37,  3.71s/it] 50%|█████     | 10/20 [00:37<00:37,  3.73s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.52s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.61s/it] 55%|█████▌    | 11/20 [00:40<00:34,  3.81s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.85s/it] 60%|██████    | 12/20 [00:44<00:31,  3.93s/it] 60%|██████    | 12/20 [00:45<00:32,  4.08s/it] 60%|██████    | 12/20 [00:45<00:31,  3.99s/it] 60%|██████    | 12/20 [00:46<00:33,  4.16s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.11s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.17s/it] 65%|██████▌   | 13/20 [00:50<00:28,  4.12s/it] 65%|██████▌   | 13/20 [00:50<00:30,  4.36s/it] 70%|███████   | 14/20 [00:52<00:24,  4.03s/it] 70%|███████   | 14/20 [00:53<00:23,  3.98s/it] 70%|███████   | 14/20 [00:54<00:24,  4.13s/it] 70%|███████   | 14/20 [00:55<00:26,  4.40s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.94s/it] 75%|███████▌  | 15/20 [00:56<00:18,  3.74s/it] 75%|███████▌  | 15/20 [00:57<00:19,  4.00s/it] 80%|████████  | 16/20 [00:59<00:14,  3.54s/it] 75%|███████▌  | 15/20 [00:59<00:21,  4.32s/it] 80%|████████  | 16/20 [01:00<00:14,  3.61s/it] 80%|████████  | 16/20 [01:01<00:15,  3.78s/it] 80%|████████  | 16/20 [01:02<00:15,  3.97s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.48s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.49s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.60s/it] 85%|████████▌ | 17/20 [01:05<00:11,  3.68s/it] 90%|█████████ | 18/20 [01:05<00:06,  3.41s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.38s/it] 90%|█████████ | 18/20 [01:07<00:06,  3.48s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.57s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.39s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.43s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.50s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.54s/it]100%|██████████| 20/20 [01:13<00:00,  3.72s/it]100%|██████████| 20/20 [01:13<00:00,  3.68s/it]
100%|██████████| 20/20 [01:13<00:00,  3.61s/it]100%|██████████| 20/20 [01:13<00:00,  3.70s/it]
100%|██████████| 20/20 [01:15<00:00,  3.79s/it]100%|██████████| 20/20 [01:15<00:00,  3.78s/it]
100%|██████████| 20/20 [01:16<00:00,  3.76s/it]100%|██████████| 20/20 [01:16<00:00,  3.84s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 01:44:12,105 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_65/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 01:44:12,105 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 01:44:12,105 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 01:44:12,105 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 01:46:19,153 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 01:46:19,153 - RUN - INFO - Evaluation Results:
2025-02-23 01:46:19,155 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.60875
13  Average  cross   0.445
14  Average  single  0.7725
--  -------  ------  -------
[rank0]:[W223 01:46:19.008669157 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 01:46:25.833280 2242166 site-packages/torch/distributed/run.py:793] 
W0223 01:46:25.833280 2242166 site-packages/torch/distributed/run.py:793] *****************************************
W0223 01:46:25.833280 2242166 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 01:46:25.833280 2242166 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank0]:[W223 01:46:52.727543447 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 01:46:52.728540502 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 01:46:52.729863503 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 01:46:52.736654981 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.27s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.26s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.25s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]
Max step: 67, bias_value: 0.2
Max step: 67, bias_value: 0.2
Max step: 67, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 67, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:57<15:51, 237.75s/it] 20%|██        | 1/5 [04:50<19:22, 290.62s/it] 20%|██        | 1/5 [05:09<20:38, 309.66s/it] 20%|██        | 1/5 [05:26<21:45, 326.30s/it] 40%|████      | 2/5 [07:53<11:50, 236.76s/it] 40%|████      | 2/5 [08:45<12:34, 251.55s/it] 40%|████      | 2/5 [09:27<13:56, 278.92s/it] 40%|████      | 2/5 [09:30<14:12, 284.13s/it] 60%|██████    | 3/5 [11:56<07:58, 239.44s/it] 60%|██████    | 3/5 [14:12<09:24, 282.12s/it] 60%|██████    | 3/5 [14:29<09:42, 291.24s/it] 60%|██████    | 3/5 [14:32<09:50, 295.12s/it] 80%|████████  | 4/5 [16:02<04:02, 242.07s/it] 80%|████████  | 4/5 [19:45<05:01, 302.00s/it] 80%|████████  | 4/5 [19:46<05:02, 302.31s/it] 80%|████████  | 4/5 [20:02<05:07, 307.38s/it]100%|██████████| 5/5 [21:13<00:00, 266.94s/it]100%|██████████| 5/5 [21:13<00:00, 254.72s/it]
100%|██████████| 5/5 [23:26<00:00, 273.12s/it]100%|██████████| 5/5 [23:26<00:00, 281.39s/it]
100%|██████████| 5/5 [24:16<00:00, 290.86s/it]100%|██████████| 5/5 [24:16<00:00, 291.36s/it]
100%|██████████| 5/5 [24:33<00:00, 294.57s/it]100%|██████████| 5/5 [24:33<00:00, 294.78s/it]
[rank0]:[W223 02:12:57.580091709 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 02:13:04.201575 2300559 site-packages/torch/distributed/run.py:793] 
W0223 02:13:04.201575 2300559 site-packages/torch/distributed/run.py:793] *****************************************
W0223 02:13:04.201575 2300559 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 02:13:04.201575 2300559 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank2]:[W223 02:13:32.841610575 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 02:13:32.841618891 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 02:13:32.854159438 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 02:14:02.300628720 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.41s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.37s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_67/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_67/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_67/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_67/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<01:59,  6.29s/it]  5%|▌         | 1/20 [00:06<02:10,  6.87s/it]  5%|▌         | 1/20 [00:06<02:07,  6.71s/it]  5%|▌         | 1/20 [00:07<02:17,  7.23s/it] 10%|█         | 2/20 [00:09<01:25,  4.73s/it] 10%|█         | 2/20 [00:09<01:23,  4.67s/it] 10%|█         | 2/20 [00:10<01:26,  4.83s/it] 10%|█         | 2/20 [00:10<01:29,  4.99s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.74s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.76s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.72s/it] 15%|█▌        | 3/20 [00:13<01:04,  3.82s/it] 20%|██        | 4/20 [00:16<01:00,  3.78s/it] 20%|██        | 4/20 [00:16<00:58,  3.65s/it] 20%|██        | 4/20 [00:16<01:01,  3.85s/it] 20%|██        | 4/20 [00:17<01:01,  3.87s/it] 25%|██▌       | 5/20 [00:21<01:02,  4.20s/it] 25%|██▌       | 5/20 [00:23<01:15,  5.06s/it] 25%|██▌       | 5/20 [00:23<01:17,  5.17s/it] 25%|██▌       | 5/20 [00:24<01:15,  5.02s/it] 30%|███       | 6/20 [00:26<01:04,  4.60s/it] 30%|███       | 6/20 [00:26<01:02,  4.43s/it] 30%|███       | 6/20 [00:27<01:02,  4.43s/it] 30%|███       | 6/20 [00:27<01:02,  4.45s/it] 35%|███▌      | 7/20 [00:29<00:51,  3.93s/it] 35%|███▌      | 7/20 [00:29<00:50,  3.89s/it] 35%|███▌      | 7/20 [00:29<00:49,  3.83s/it] 35%|███▌      | 7/20 [00:30<00:50,  3.85s/it] 40%|████      | 8/20 [00:32<00:45,  3.78s/it] 40%|████      | 8/20 [00:33<00:45,  3.82s/it] 40%|████      | 8/20 [00:33<00:45,  3.80s/it] 40%|████      | 8/20 [00:33<00:45,  3.78s/it] 45%|████▌     | 9/20 [00:35<00:38,  3.52s/it] 45%|████▌     | 9/20 [00:36<00:38,  3.53s/it] 45%|████▌     | 9/20 [00:36<00:39,  3.57s/it] 45%|████▌     | 9/20 [00:36<00:39,  3.57s/it] 50%|█████     | 10/20 [00:39<00:35,  3.52s/it] 50%|█████     | 10/20 [00:40<00:36,  3.65s/it] 50%|█████     | 10/20 [00:40<00:38,  3.89s/it] 50%|█████     | 10/20 [00:41<00:39,  3.94s/it] 55%|█████▌    | 11/20 [00:44<00:33,  3.73s/it] 55%|█████▌    | 11/20 [00:44<00:35,  3.93s/it] 55%|█████▌    | 11/20 [00:44<00:34,  3.80s/it] 55%|█████▌    | 11/20 [00:45<00:36,  4.02s/it] 60%|██████    | 12/20 [00:48<00:32,  4.04s/it] 60%|██████    | 12/20 [00:49<00:33,  4.23s/it] 60%|██████    | 12/20 [00:49<00:32,  4.11s/it] 60%|██████    | 12/20 [00:50<00:33,  4.23s/it] 65%|██████▌   | 13/20 [00:53<00:29,  4.17s/it] 65%|██████▌   | 13/20 [00:53<00:29,  4.28s/it] 65%|██████▌   | 13/20 [00:54<00:29,  4.23s/it] 65%|██████▌   | 13/20 [00:54<00:30,  4.37s/it] 70%|███████   | 14/20 [00:57<00:24,  4.06s/it] 70%|███████   | 14/20 [00:57<00:24,  4.04s/it] 70%|███████   | 14/20 [00:58<00:25,  4.17s/it] 70%|███████   | 14/20 [00:59<00:26,  4.39s/it] 75%|███████▌  | 15/20 [01:00<00:18,  3.76s/it] 75%|███████▌  | 15/20 [01:00<00:19,  3.95s/it] 75%|███████▌  | 15/20 [01:01<00:19,  3.97s/it] 75%|███████▌  | 15/20 [01:03<00:21,  4.32s/it] 80%|████████  | 16/20 [01:03<00:14,  3.54s/it] 80%|████████  | 16/20 [01:03<00:14,  3.63s/it] 80%|████████  | 16/20 [01:04<00:15,  3.79s/it] 80%|████████  | 16/20 [01:06<00:15,  3.97s/it] 85%|████████▌ | 17/20 [01:06<00:10,  3.45s/it] 85%|████████▌ | 17/20 [01:06<00:10,  3.51s/it] 85%|████████▌ | 17/20 [01:08<00:10,  3.63s/it] 85%|████████▌ | 17/20 [01:09<00:11,  3.70s/it] 90%|█████████ | 18/20 [01:09<00:06,  3.38s/it] 90%|█████████ | 18/20 [01:09<00:06,  3.39s/it] 90%|█████████ | 18/20 [01:11<00:06,  3.48s/it] 90%|█████████ | 18/20 [01:12<00:07,  3.55s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.35s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.44s/it] 95%|█████████▌| 19/20 [01:14<00:03,  3.52s/it] 95%|█████████▌| 19/20 [01:16<00:03,  3.52s/it]100%|██████████| 20/20 [01:17<00:00,  3.70s/it]100%|██████████| 20/20 [01:17<00:00,  3.88s/it]
100%|██████████| 20/20 [01:17<00:00,  3.62s/it]100%|██████████| 20/20 [01:17<00:00,  3.88s/it]
100%|██████████| 20/20 [01:19<00:00,  3.73s/it]100%|██████████| 20/20 [01:19<00:00,  3.96s/it]
100%|██████████| 20/20 [01:20<00:00,  3.75s/it]100%|██████████| 20/20 [01:20<00:00,  4.03s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 02:16:56,406 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_67/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 02:16:56,406 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 02:16:56,406 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 02:16:56,406 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 02:19:16,011 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 02:19:16,011 - RUN - INFO - Evaluation Results:
2025-02-23 02:19:16,014 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.60875
13  Average  cross   0.445
14  Average  single  0.7725
--  -------  ------  -------
[rank0]:[W223 02:19:16.909601122 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 02:19:22.421483 2312146 site-packages/torch/distributed/run.py:793] 
W0223 02:19:22.421483 2312146 site-packages/torch/distributed/run.py:793] *****************************************
W0223 02:19:22.421483 2312146 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 02:19:22.421483 2312146 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank2]:[W223 02:19:47.596521641 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W223 02:19:47.597397834 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 02:19:47.597676740 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 02:19:47.602902983 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.56s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.58s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.60s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]
Max step: 69, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 69, bias_value: 0.2
Max step: 69, bias_value: 0.2
Max step: 69, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [03:57<15:49, 237.25s/it] 20%|██        | 1/5 [04:52<19:31, 292.95s/it] 20%|██        | 1/5 [05:12<20:50, 312.54s/it] 20%|██        | 1/5 [05:31<22:07, 331.85s/it] 40%|████      | 2/5 [07:57<11:56, 238.86s/it] 40%|████      | 2/5 [08:50<12:39, 253.25s/it] 40%|████      | 2/5 [09:28<13:57, 279.14s/it] 40%|████      | 2/5 [09:37<14:23, 287.78s/it] 60%|██████    | 3/5 [12:00<08:01, 240.83s/it] 60%|██████    | 3/5 [14:22<09:32, 286.06s/it] 60%|██████    | 3/5 [14:44<09:53, 296.81s/it] 60%|██████    | 3/5 [14:45<09:59, 299.79s/it] 80%|████████  | 4/5 [16:06<04:02, 242.79s/it] 80%|████████  | 4/5 [19:57<05:04, 304.85s/it] 80%|████████  | 4/5 [20:04<05:07, 308.00s/it] 80%|████████  | 4/5 [20:23<05:13, 313.41s/it]100%|██████████| 5/5 [21:23<00:00, 269.59s/it]100%|██████████| 5/5 [21:23<00:00, 256.67s/it]
100%|██████████| 5/5 [23:44<00:00, 276.63s/it]100%|██████████| 5/5 [23:44<00:00, 284.89s/it]
100%|██████████| 5/5 [24:35<00:00, 294.82s/it]100%|██████████| 5/5 [24:35<00:00, 295.13s/it]
100%|██████████| 5/5 [24:58<00:00, 299.62s/it]100%|██████████| 5/5 [24:58<00:00, 299.74s/it]
[rank0]:[W223 02:46:21.832232347 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 02:46:28.175212 2369982 site-packages/torch/distributed/run.py:793] 
W0223 02:46:28.175212 2369982 site-packages/torch/distributed/run.py:793] *****************************************
W0223 02:46:28.175212 2369982 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 02:46:28.175212 2369982 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank3]:[W223 02:46:54.928605566 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 02:46:54.936437276 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 02:46:54.936627580 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 02:47:24.196595220 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.24s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.23s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.20s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]

Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_69/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_69/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_69/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_69/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:02,  6.44s/it]  5%|▌         | 1/20 [00:07<02:14,  7.06s/it]  5%|▌         | 1/20 [00:06<02:09,  6.82s/it]  5%|▌         | 1/20 [00:07<02:21,  7.44s/it] 10%|█         | 2/20 [00:10<01:25,  4.76s/it] 10%|█         | 2/20 [00:10<01:26,  4.82s/it] 10%|█         | 2/20 [00:10<01:25,  4.75s/it] 10%|█         | 2/20 [00:10<01:31,  5.10s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.77s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.74s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.77s/it] 15%|█▌        | 3/20 [00:13<01:06,  3.91s/it] 20%|██        | 4/20 [00:16<01:00,  3.80s/it] 20%|██        | 4/20 [00:16<00:58,  3.68s/it] 20%|██        | 4/20 [00:16<01:01,  3.86s/it] 20%|██        | 4/20 [00:17<01:03,  3.97s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.48s/it] 25%|██▌       | 5/20 [00:20<00:54,  3.63s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.71s/it] 25%|██▌       | 5/20 [00:20<00:56,  3.77s/it] 30%|███       | 6/20 [00:23<00:49,  3.51s/it] 30%|███       | 6/20 [00:22<00:48,  3.46s/it] 30%|███       | 6/20 [00:23<00:48,  3.48s/it] 30%|███       | 6/20 [00:24<00:51,  3.65s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.17s/it] 35%|███▌      | 7/20 [00:25<00:42,  3.24s/it] 35%|███▌      | 7/20 [00:26<00:43,  3.31s/it] 35%|███▌      | 7/20 [00:26<00:43,  3.31s/it] 40%|████      | 8/20 [00:28<00:39,  3.29s/it] 40%|████      | 8/20 [00:29<00:40,  3.38s/it] 40%|████      | 8/20 [00:29<00:42,  3.51s/it] 40%|████      | 8/20 [00:30<00:41,  3.46s/it] 45%|████▌     | 9/20 [00:31<00:35,  3.19s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.25s/it] 45%|████▌     | 9/20 [00:33<00:37,  3.37s/it] 45%|████▌     | 9/20 [00:33<00:36,  3.33s/it] 50%|█████     | 10/20 [00:36<00:32,  3.29s/it] 50%|█████     | 10/20 [00:36<00:36,  3.66s/it] 50%|█████     | 10/20 [00:37<00:35,  3.51s/it] 50%|█████     | 10/20 [00:38<00:39,  3.93s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.65s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.78s/it] 55%|█████▌    | 11/20 [00:41<00:33,  3.72s/it] 55%|█████▌    | 11/20 [00:42<00:36,  4.04s/it] 60%|██████    | 12/20 [00:45<00:32,  4.03s/it] 60%|██████    | 12/20 [00:45<00:32,  4.05s/it] 60%|██████    | 12/20 [00:46<00:32,  4.12s/it] 60%|██████    | 12/20 [00:47<00:34,  4.28s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.19s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.20s/it] 65%|██████▌   | 13/20 [00:51<00:29,  4.25s/it] 65%|██████▌   | 13/20 [00:52<00:31,  4.46s/it] 70%|███████   | 14/20 [00:53<00:24,  4.01s/it] 70%|███████   | 14/20 [00:53<00:24,  4.09s/it] 70%|███████   | 14/20 [00:55<00:25,  4.19s/it] 70%|███████   | 14/20 [00:56<00:27,  4.51s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.85s/it] 75%|███████▌  | 15/20 [00:57<00:20,  4.04s/it] 75%|███████▌  | 15/20 [00:59<00:19,  4.00s/it] 80%|████████  | 16/20 [01:00<00:14,  3.65s/it] 80%|████████  | 16/20 [01:00<00:15,  3.79s/it] 75%|███████▌  | 15/20 [01:00<00:22,  4.40s/it] 80%|████████  | 16/20 [01:02<00:15,  3.81s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.57s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.63s/it] 80%|████████  | 16/20 [01:04<00:16,  4.10s/it] 85%|████████▌ | 17/20 [01:05<00:10,  3.64s/it] 90%|█████████ | 18/20 [01:07<00:06,  3.47s/it] 90%|█████████ | 18/20 [01:07<00:07,  3.52s/it] 85%|████████▌ | 17/20 [01:07<00:11,  3.80s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.51s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.46s/it] 90%|█████████ | 18/20 [01:10<00:07,  3.65s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.53s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.53s/it] 95%|█████████▌| 19/20 [01:14<00:03,  3.62s/it]100%|██████████| 20/20 [01:14<00:00,  3.77s/it]100%|██████████| 20/20 [01:14<00:00,  3.75s/it]
100%|██████████| 20/20 [01:15<00:00,  3.87s/it]100%|██████████| 20/20 [01:15<00:00,  3.77s/it]
100%|██████████| 20/20 [01:16<00:00,  3.77s/it]100%|██████████| 20/20 [01:16<00:00,  3.84s/it]
100%|██████████| 20/20 [01:18<00:00,  3.86s/it]100%|██████████| 20/20 [01:18<00:00,  3.94s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 02:50:12,577 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_69/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 02:50:12,577 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 02:50:12,577 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 02:50:12,577 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 02:52:28,597 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 02:52:28,597 - RUN - INFO - Evaluation Results:
2025-02-23 02:52:28,599 - RUN - INFO - 
--  -------  ------  -------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.595
10  3        cross   0.41
11  3        single  0.78
12  Average  all     0.60875
13  Average  cross   0.445
14  Average  single  0.7725
--  -------  ------  -------
[rank0]:[W223 02:52:29.462464944 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 02:52:35.496759 2381835 site-packages/torch/distributed/run.py:793] 
W0223 02:52:35.496759 2381835 site-packages/torch/distributed/run.py:793] *****************************************
W0223 02:52:35.496759 2381835 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 02:52:35.496759 2381835 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank0]:[W223 02:53:00.154443046 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 02:53:00.161504402 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 02:53:00.161791919 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 02:53:00.161937294 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.23s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Max step: 71, bias_value: 0.2
Max step: 71, bias_value: 0.2Max step: 71, bias_value: 0.2Max step: 71, bias_value: 0.2


Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [04:00<16:02, 240.62s/it] 20%|██        | 1/5 [04:56<19:45, 296.41s/it] 20%|██        | 1/5 [05:16<21:06, 316.54s/it] 20%|██        | 1/5 [05:42<22:50, 342.61s/it] 40%|████      | 2/5 [08:01<12:01, 240.49s/it] 40%|████      | 2/5 [09:01<12:54, 258.26s/it] 40%|████      | 2/5 [09:40<14:16, 285.55s/it] 40%|████      | 2/5 [09:42<14:31, 290.52s/it] 60%|██████    | 3/5 [12:04<08:03, 241.64s/it] 60%|██████    | 3/5 [14:37<09:41, 290.92s/it] 60%|██████    | 3/5 [14:55<10:01, 300.81s/it] 60%|██████    | 3/5 [15:01<10:09, 304.63s/it] 80%|████████  | 4/5 [16:12<04:04, 244.37s/it] 80%|████████  | 4/5 [20:17<05:08, 308.95s/it] 80%|████████  | 4/5 [20:20<05:11, 311.46s/it] 80%|████████  | 4/5 [20:41<05:18, 318.64s/it]100%|██████████| 5/5 [21:35<00:00, 272.70s/it]100%|██████████| 5/5 [21:35<00:00, 259.10s/it]
100%|██████████| 5/5 [24:08<00:00, 281.10s/it]100%|██████████| 5/5 [24:08<00:00, 289.79s/it]
100%|██████████| 5/5 [24:55<00:00, 298.22s/it]100%|██████████| 5/5 [24:55<00:00, 299.08s/it]
100%|██████████| 5/5 [25:21<00:00, 304.58s/it]100%|██████████| 5/5 [25:21<00:00, 304.30s/it]
[rank0]:[W223 03:19:51.124204452 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 03:19:58.663891 2439788 site-packages/torch/distributed/run.py:793] 
W0223 03:19:58.663891 2439788 site-packages/torch/distributed/run.py:793] *****************************************
W0223 03:19:58.663891 2439788 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 03:19:58.663891 2439788 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank2]:[W223 03:20:25.808704042 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 03:20:25.808704088 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 03:20:25.812383971 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 03:20:57.524029352 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.05s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.29s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.06s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_71/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2Max step: 50, bias_value: 0.2

Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_71/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_71/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_71/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:01,  6.39s/it]  5%|▌         | 1/20 [00:06<02:09,  6.80s/it]  5%|▌         | 1/20 [00:07<02:15,  7.13s/it]  5%|▌         | 1/20 [00:07<02:14,  7.09s/it] 10%|█         | 2/20 [00:09<01:25,  4.75s/it] 10%|█         | 2/20 [00:10<01:25,  4.77s/it] 10%|█         | 2/20 [00:10<01:28,  4.91s/it] 10%|█         | 2/20 [00:10<01:27,  4.87s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.73s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.72s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.78s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.84s/it] 20%|██        | 4/20 [00:16<01:00,  3.76s/it] 20%|██        | 4/20 [00:16<00:59,  3.72s/it] 20%|██        | 4/20 [00:16<01:01,  3.83s/it] 20%|██        | 4/20 [00:16<01:01,  3.84s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.50s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.62s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.60s/it] 25%|██▌       | 5/20 [00:20<00:54,  3.67s/it] 30%|███       | 6/20 [00:22<00:47,  3.43s/it] 30%|███       | 6/20 [00:22<00:47,  3.42s/it] 30%|███       | 6/20 [00:23<00:49,  3.50s/it] 30%|███       | 6/20 [00:23<00:49,  3.53s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.20s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.14s/it] 35%|███▌      | 7/20 [00:25<00:42,  3.23s/it] 35%|███▌      | 7/20 [00:26<00:41,  3.23s/it] 40%|████      | 8/20 [00:28<00:38,  3.24s/it] 40%|████      | 8/20 [00:29<00:40,  3.38s/it] 40%|████      | 8/20 [00:29<00:40,  3.38s/it] 40%|████      | 8/20 [00:29<00:40,  3.37s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.16s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.26s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.25s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.25s/it] 50%|█████     | 10/20 [00:35<00:33,  3.33s/it] 50%|█████     | 10/20 [00:36<00:34,  3.40s/it] 50%|█████     | 10/20 [00:36<00:36,  3.63s/it] 50%|█████     | 10/20 [00:37<00:37,  3.74s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.60s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.59s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.72s/it] 55%|█████▌    | 11/20 [00:41<00:35,  3.90s/it] 60%|██████    | 12/20 [00:45<00:31,  3.93s/it] 60%|██████    | 12/20 [00:45<00:31,  3.99s/it] 60%|██████    | 12/20 [00:45<00:31,  4.00s/it] 60%|██████    | 12/20 [00:46<00:34,  4.27s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.05s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.10s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.17s/it] 65%|██████▌   | 13/20 [00:51<00:30,  4.39s/it] 70%|███████   | 14/20 [00:53<00:23,  3.95s/it] 70%|███████   | 14/20 [00:53<00:24,  4.02s/it] 70%|███████   | 14/20 [00:53<00:24,  4.10s/it] 70%|███████   | 14/20 [00:55<00:26,  4.41s/it] 75%|███████▌  | 15/20 [00:56<00:18,  3.72s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.84s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.98s/it] 75%|███████▌  | 15/20 [00:59<00:21,  4.31s/it] 80%|████████  | 16/20 [00:59<00:14,  3.65s/it] 80%|████████  | 16/20 [00:59<00:14,  3.58s/it] 80%|████████  | 16/20 [01:00<00:14,  3.67s/it] 80%|████████  | 16/20 [01:02<00:15,  3.98s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.53s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.53s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.50s/it] 85%|████████▌ | 17/20 [01:05<00:11,  3.72s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.40s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.39s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.45s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.59s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.50s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.44s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.41s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.55s/it]100%|██████████| 20/20 [01:13<00:00,  3.67s/it]100%|██████████| 20/20 [01:13<00:00,  3.70s/it]
100%|██████████| 20/20 [01:14<00:00,  3.67s/it]100%|██████████| 20/20 [01:14<00:00,  3.71s/it]
100%|██████████| 20/20 [01:14<00:00,  3.76s/it]100%|██████████| 20/20 [01:14<00:00,  3.72s/it]
100%|██████████| 20/20 [01:17<00:00,  3.88s/it]100%|██████████| 20/20 [01:17<00:00,  3.87s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 03:23:47,555 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_71/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 03:23:47,555 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 03:23:47,555 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 03:23:47,555 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 03:26:09,990 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 03:26:09,990 - RUN - INFO - Evaluation Results:
2025-02-23 03:26:09,992 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.655
 1  0        single  0.76
 2  0        cross   0.55
 3  1        all     0.605
 4  1        single  0.79
 5  1        cross   0.42
 6  2        all     0.58
 7  2        single  0.76
 8  2        cross   0.4
 9  3        all     0.6
10  3        single  0.78
11  3        cross   0.42
12  Average  all     0.61
13  Average  single  0.7725
14  Average  cross   0.4475
--  -------  ------  ------
[rank0]:[W223 03:26:10.816155015 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 03:26:16.403842 2450379 site-packages/torch/distributed/run.py:793] 
W0223 03:26:16.403842 2450379 site-packages/torch/distributed/run.py:793] *****************************************
W0223 03:26:16.403842 2450379 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 03:26:16.403842 2450379 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank1]:[W223 03:26:43.440259093 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 03:26:43.441287966 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W223 03:26:43.458195976 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 03:26:43.460582118 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.26s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.25s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.25s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]
Max step: 73, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 73, bias_value: 0.2Max step: 73, bias_value: 0.2Max step: 73, bias_value: 0.2


You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [04:03<16:15, 243.96s/it] 20%|██        | 1/5 [04:59<19:59, 299.86s/it] 20%|██        | 1/5 [05:18<21:15, 318.88s/it] 20%|██        | 1/5 [05:39<22:38, 339.75s/it] 40%|████      | 2/5 [08:04<12:06, 242.08s/it] 40%|████      | 2/5 [08:59<12:52, 257.44s/it] 40%|████      | 2/5 [09:43<14:20, 286.98s/it] 40%|████      | 2/5 [09:50<14:42, 294.30s/it] 60%|██████    | 3/5 [12:10<08:07, 243.83s/it] 60%|██████    | 3/5 [14:47<09:49, 294.52s/it] 60%|██████    | 3/5 [15:02<10:11, 305.70s/it] 60%|██████    | 3/5 [15:08<10:10, 305.18s/it] 80%|████████  | 4/5 [16:23<04:07, 247.25s/it] 80%|████████  | 4/5 [20:17<05:09, 309.49s/it] 80%|████████  | 4/5 [20:32<05:14, 314.78s/it] 80%|████████  | 4/5 [21:00<05:23, 323.57s/it]100%|██████████| 5/5 [21:52<00:00, 276.82s/it]100%|██████████| 5/5 [21:52<00:00, 262.48s/it]
100%|██████████| 5/5 [24:12<00:00, 282.62s/it]100%|██████████| 5/5 [24:12<00:00, 290.60s/it]
100%|██████████| 5/5 [25:09<00:00, 301.05s/it]100%|██████████| 5/5 [25:09<00:00, 301.92s/it]
100%|██████████| 5/5 [25:42<00:00, 308.61s/it]100%|██████████| 5/5 [25:42<00:00, 308.45s/it]
[rank0]:[W223 03:53:59.063276000 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 03:54:07.126627 2507162 site-packages/torch/distributed/run.py:793] 
W0223 03:54:07.126627 2507162 site-packages/torch/distributed/run.py:793] *****************************************
W0223 03:54:07.126627 2507162 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 03:54:07.126627 2507162 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank2]:[W223 03:54:33.546695964 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 03:54:33.550336942 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 03:54:33.551122270 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 03:55:04.544644360 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.43s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.42s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.46s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_73/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_73/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_73/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_73/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<01:59,  6.30s/it]  5%|▌         | 1/20 [00:06<02:02,  6.44s/it]  5%|▌         | 1/20 [00:07<02:15,  7.12s/it]  5%|▌         | 1/20 [00:07<02:15,  7.12s/it] 10%|█         | 2/20 [00:09<01:25,  4.75s/it] 10%|█         | 2/20 [00:09<01:22,  4.58s/it] 10%|█         | 2/20 [00:10<01:27,  4.87s/it] 10%|█         | 2/20 [00:10<01:29,  4.95s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.72s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.77s/it] 15%|█▌        | 3/20 [00:12<01:01,  3.64s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.83s/it] 20%|██        | 4/20 [00:16<01:00,  3.76s/it] 20%|██        | 4/20 [00:15<00:57,  3.61s/it] 20%|██        | 4/20 [00:16<01:00,  3.79s/it] 20%|██        | 4/20 [00:16<01:01,  3.86s/it] 25%|██▌       | 5/20 [00:18<00:51,  3.44s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.53s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.65s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.68s/it] 30%|███       | 6/20 [00:22<00:47,  3.39s/it] 30%|███       | 6/20 [00:22<00:48,  3.44s/it] 30%|███       | 6/20 [00:23<00:48,  3.43s/it] 30%|███       | 6/20 [00:23<00:49,  3.56s/it] 35%|███▌      | 7/20 [00:24<00:40,  3.14s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.13s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.20s/it] 35%|███▌      | 7/20 [00:26<00:41,  3.23s/it] 40%|████      | 8/20 [00:28<00:38,  3.23s/it] 40%|████      | 8/20 [00:29<00:39,  3.32s/it] 40%|████      | 8/20 [00:29<00:40,  3.35s/it] 40%|████      | 8/20 [00:29<00:40,  3.37s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.16s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.21s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.25s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.25s/it] 50%|█████     | 10/20 [00:35<00:32,  3.26s/it] 50%|█████     | 10/20 [00:35<00:36,  3.63s/it] 50%|█████     | 10/20 [00:36<00:33,  3.37s/it] 50%|█████     | 10/20 [00:36<00:37,  3.71s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.52s/it] 55%|█████▌    | 11/20 [00:39<00:33,  3.72s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.62s/it] 55%|█████▌    | 11/20 [00:40<00:34,  3.83s/it] 60%|██████    | 12/20 [00:44<00:31,  3.91s/it] 60%|██████    | 12/20 [00:44<00:31,  4.00s/it] 60%|██████    | 12/20 [00:45<00:32,  4.01s/it] 60%|██████    | 12/20 [00:45<00:32,  4.09s/it] 65%|██████▌   | 13/20 [00:48<00:28,  4.08s/it] 65%|██████▌   | 13/20 [00:48<00:28,  4.12s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.13s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.28s/it] 70%|███████   | 14/20 [00:52<00:24,  4.01s/it] 70%|███████   | 14/20 [00:52<00:23,  3.95s/it] 70%|███████   | 14/20 [00:53<00:24,  4.10s/it] 70%|███████   | 14/20 [00:54<00:25,  4.30s/it] 75%|███████▌  | 15/20 [00:55<00:18,  3.72s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.91s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.97s/it] 75%|███████▌  | 15/20 [00:58<00:21,  4.26s/it] 80%|████████  | 16/20 [00:59<00:14,  3.51s/it] 80%|████████  | 16/20 [00:58<00:14,  3.59s/it] 80%|████████  | 16/20 [01:00<00:15,  3.75s/it] 80%|████████  | 16/20 [01:01<00:15,  3.89s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.43s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.46s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.57s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.63s/it] 90%|█████████ | 18/20 [01:05<00:06,  3.35s/it] 90%|█████████ | 18/20 [01:05<00:06,  3.36s/it] 90%|█████████ | 18/20 [01:07<00:06,  3.45s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.52s/it] 95%|█████████▌| 19/20 [01:08<00:03,  3.38s/it] 95%|█████████▌| 19/20 [01:08<00:03,  3.43s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.49s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.48s/it]100%|██████████| 20/20 [01:12<00:00,  3.60s/it]100%|██████████| 20/20 [01:12<00:00,  3.64s/it]
100%|██████████| 20/20 [01:13<00:00,  3.76s/it]100%|██████████| 20/20 [01:13<00:00,  3.68s/it]
100%|██████████| 20/20 [01:15<00:00,  3.76s/it]100%|██████████| 20/20 [01:15<00:00,  3.76s/it]
100%|██████████| 20/20 [01:15<00:00,  3.71s/it]100%|██████████| 20/20 [01:15<00:00,  3.79s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 03:57:57,814 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_73/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 03:57:57,814 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 03:57:57,814 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 03:57:57,814 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 04:00:17,630 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 04:00:17,630 - RUN - INFO - Evaluation Results:
2025-02-23 04:00:17,632 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.6
10  3        cross   0.42
11  3        single  0.78
12  Average  all     0.61
13  Average  cross   0.4475
14  Average  single  0.7725
--  -------  ------  ------
[rank0]:[W223 04:00:18.457750502 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 04:00:24.084096 2519800 site-packages/torch/distributed/run.py:793] 
W0223 04:00:24.084096 2519800 site-packages/torch/distributed/run.py:793] *****************************************
W0223 04:00:24.084096 2519800 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 04:00:24.084096 2519800 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank0]:[W223 04:00:49.986044547 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 04:00:49.988363311 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 04:00:49.989080886 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 04:00:49.995205290 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.39s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]
Max step: 75, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 75, bias_value: 0.2
Max step: 75, bias_value: 0.2
Max step: 75, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [04:03<16:14, 243.56s/it] 20%|██        | 1/5 [05:01<20:05, 301.26s/it] 20%|██        | 1/5 [05:21<21:25, 321.34s/it] 20%|██        | 1/5 [05:41<22:46, 341.60s/it] 40%|████      | 2/5 [08:05<12:08, 242.67s/it] 40%|████      | 2/5 [08:59<12:51, 257.00s/it] 40%|████      | 2/5 [09:48<14:28, 289.62s/it] 40%|████      | 2/5 [09:59<14:58, 299.42s/it] 60%|██████    | 3/5 [12:10<08:07, 243.68s/it] 60%|██████    | 3/5 [14:54<09:53, 296.79s/it] 60%|██████    | 3/5 [15:10<10:17, 308.89s/it] 60%|██████    | 3/5 [15:22<10:20, 310.26s/it] 80%|████████  | 4/5 [16:25<04:07, 247.97s/it] 80%|████████  | 4/5 [20:25<05:11, 311.63s/it] 80%|████████  | 4/5 [20:42<05:17, 317.02s/it] 80%|████████  | 4/5 [21:22<05:29, 329.82s/it]100%|██████████| 5/5 [21:55<00:00, 277.54s/it]100%|██████████| 5/5 [21:55<00:00, 263.00s/it]
100%|██████████| 5/5 [24:20<00:00, 284.04s/it]100%|██████████| 5/5 [24:20<00:00, 292.20s/it]
100%|██████████| 5/5 [25:20<00:00, 303.10s/it]100%|██████████| 5/5 [25:20<00:00, 304.11s/it]
100%|██████████| 5/5 [26:05<00:00, 313.09s/it]100%|██████████| 5/5 [26:05<00:00, 313.15s/it]
[rank0]:[W223 04:28:28.356898431 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 04:28:35.430239 2571272 site-packages/torch/distributed/run.py:793] 
W0223 04:28:35.430239 2571272 site-packages/torch/distributed/run.py:793] *****************************************
W0223 04:28:35.430239 2571272 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 04:28:35.430239 2571272 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W223 04:29:01.944101489 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 04:29:01.944101581 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 04:29:01.944101568 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 04:29:32.108303532 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.29s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.30s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.27s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_75/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_75/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_75/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_75/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:04,  6.55s/it]  5%|▌         | 1/20 [00:07<02:14,  7.08s/it]  5%|▌         | 1/20 [00:07<02:17,  7.25s/it]  5%|▌         | 1/20 [00:07<02:22,  7.49s/it] 10%|█         | 2/20 [00:10<01:26,  4.82s/it] 10%|█         | 2/20 [00:10<01:28,  4.89s/it] 10%|█         | 2/20 [00:10<01:28,  4.93s/it] 10%|█         | 2/20 [00:10<01:31,  5.08s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.79s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.76s/it] 15%|█▌        | 3/20 [00:13<01:06,  3.89s/it] 15%|█▌        | 3/20 [00:13<01:06,  3.90s/it] 20%|██        | 4/20 [00:16<01:00,  3.79s/it] 20%|██        | 4/20 [00:16<01:01,  3.84s/it] 20%|██        | 4/20 [00:16<01:00,  3.77s/it] 20%|██        | 4/20 [00:17<01:03,  3.94s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.56s/it] 25%|██▌       | 5/20 [00:20<00:53,  3.57s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.69s/it] 25%|██▌       | 5/20 [00:20<00:56,  3.78s/it] 30%|███       | 6/20 [00:23<00:48,  3.46s/it] 30%|███       | 6/20 [00:23<00:49,  3.52s/it] 30%|███       | 6/20 [00:23<00:49,  3.50s/it] 30%|███       | 6/20 [00:24<00:50,  3.62s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.16s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.20s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.29s/it] 35%|███▌      | 7/20 [00:26<00:43,  3.33s/it] 40%|████      | 8/20 [00:29<00:40,  3.35s/it] 40%|████      | 8/20 [00:29<00:39,  3.33s/it] 40%|████      | 8/20 [00:30<00:42,  3.51s/it] 40%|████      | 8/20 [00:30<00:41,  3.48s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.23s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.23s/it] 45%|████▌     | 9/20 [00:33<00:37,  3.40s/it] 45%|████▌     | 9/20 [00:33<00:36,  3.35s/it] 50%|█████     | 10/20 [00:35<00:33,  3.33s/it] 50%|█████     | 10/20 [00:37<00:37,  3.78s/it] 50%|█████     | 10/20 [00:37<00:35,  3.53s/it] 50%|█████     | 10/20 [00:38<00:39,  3.90s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.60s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.86s/it] 55%|█████▌    | 11/20 [00:41<00:33,  3.74s/it] 55%|█████▌    | 11/20 [00:42<00:36,  4.02s/it] 60%|██████    | 12/20 [00:45<00:32,  4.03s/it] 60%|██████    | 12/20 [00:46<00:32,  4.11s/it] 60%|██████    | 12/20 [00:46<00:32,  4.11s/it] 60%|██████    | 12/20 [00:47<00:34,  4.33s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.18s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.19s/it] 65%|██████▌   | 13/20 [00:51<00:29,  4.21s/it] 65%|██████▌   | 13/20 [00:52<00:31,  4.49s/it] 70%|███████   | 14/20 [00:53<00:24,  4.07s/it] 70%|███████   | 14/20 [00:54<00:24,  4.05s/it] 70%|███████   | 14/20 [00:55<00:25,  4.17s/it] 70%|███████   | 14/20 [00:56<00:26,  4.48s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.98s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.82s/it] 75%|███████▌  | 15/20 [00:59<00:20,  4.08s/it] 80%|████████  | 16/20 [00:59<00:14,  3.56s/it] 80%|████████  | 16/20 [01:01<00:14,  3.69s/it] 75%|███████▌  | 15/20 [01:01<00:22,  4.42s/it] 80%|████████  | 16/20 [01:02<00:15,  3.90s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.48s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.56s/it] 80%|████████  | 16/20 [01:04<00:16,  4.04s/it] 85%|████████▌ | 17/20 [01:06<00:11,  3.71s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.41s/it] 85%|████████▌ | 17/20 [01:07<00:11,  3.76s/it] 90%|█████████ | 18/20 [01:07<00:06,  3.46s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.53s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.42s/it] 90%|█████████ | 18/20 [01:10<00:07,  3.60s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.53s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.54s/it] 95%|█████████▌| 19/20 [01:14<00:03,  3.57s/it]100%|██████████| 20/20 [01:14<00:00,  3.77s/it]100%|██████████| 20/20 [01:14<00:00,  3.72s/it]
100%|██████████| 20/20 [01:15<00:00,  3.70s/it]100%|██████████| 20/20 [01:15<00:00,  3.77s/it]
100%|██████████| 20/20 [01:17<00:00,  3.78s/it]100%|██████████| 20/20 [01:17<00:00,  3.85s/it]
100%|██████████| 20/20 [01:18<00:00,  3.89s/it]100%|██████████| 20/20 [01:18<00:00,  3.94s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 04:32:29,246 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_75/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 04:32:29,246 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 04:32:29,247 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 04:32:29,247 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 04:34:53,329 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 04:34:53,329 - RUN - INFO - Evaluation Results:
2025-02-23 04:34:53,332 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.6
10  3        cross   0.42
11  3        single  0.78
12  Average  all     0.61
13  Average  cross   0.4475
14  Average  single  0.7725
--  -------  ------  ------
[rank0]:[W223 04:34:54.189831403 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 04:34:59.763432 2579685 site-packages/torch/distributed/run.py:793] 
W0223 04:34:59.763432 2579685 site-packages/torch/distributed/run.py:793] *****************************************
W0223 04:34:59.763432 2579685 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 04:34:59.763432 2579685 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank0]:[W223 04:35:25.984180882 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 04:35:25.986544048 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 04:35:25.986709642 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 04:35:25.986943706 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.22s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.20s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.16s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Max step: 77, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 77, bias_value: 0.2
Max step: 77, bias_value: 0.2
Max step: 77, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [04:07<16:30, 247.63s/it] 20%|██        | 1/5 [05:05<20:23, 305.81s/it] 20%|██        | 1/5 [05:28<21:53, 328.30s/it] 20%|██        | 1/5 [05:46<23:06, 346.57s/it] 40%|████      | 2/5 [08:12<12:17, 245.98s/it] 40%|████      | 2/5 [09:04<12:57, 259.21s/it] 40%|████      | 2/5 [09:59<14:43, 294.59s/it] 40%|████      | 2/5 [10:06<15:09, 303.06s/it] 60%|██████    | 3/5 [12:17<08:10, 245.43s/it] 60%|██████    | 3/5 [15:08<10:02, 301.48s/it] 60%|██████    | 3/5 [15:20<10:25, 312.57s/it] 60%|██████    | 3/5 [15:31<10:25, 312.76s/it] 80%|████████  | 4/5 [16:34<04:09, 249.95s/it] 80%|████████  | 4/5 [20:38<05:14, 314.50s/it] 80%|████████  | 4/5 [20:59<05:20, 320.83s/it] 80%|████████  | 4/5 [21:36<05:33, 333.62s/it]100%|██████████| 5/5 [22:09<00:00, 280.83s/it]100%|██████████| 5/5 [22:09<00:00, 265.94s/it]
100%|██████████| 5/5 [24:35<00:00, 286.76s/it]100%|██████████| 5/5 [24:35<00:00, 295.15s/it]
100%|██████████| 5/5 [25:40<00:00, 306.66s/it]100%|██████████| 5/5 [25:40<00:00, 308.20s/it]
100%|██████████| 5/5 [26:23<00:00, 316.70s/it]100%|██████████| 5/5 [26:23<00:00, 316.71s/it]
[rank0]:[W223 05:03:21.886291380 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 05:03:28.460815 2620532 site-packages/torch/distributed/run.py:793] 
W0223 05:03:28.460815 2620532 site-packages/torch/distributed/run.py:793] *****************************************
W0223 05:03:28.460815 2620532 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 05:03:28.460815 2620532 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W223 05:03:54.198737595 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 05:03:54.199072283 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 05:03:54.200477234 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 05:04:24.081983057 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.40s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.40s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_77/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2Max step: 50, bias_value: 0.2

Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_77/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_77/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_77/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:06,  6.66s/it]  5%|▌         | 1/20 [00:06<02:06,  6.67s/it]  5%|▌         | 1/20 [00:07<02:18,  7.29s/it]  5%|▌         | 1/20 [00:07<02:19,  7.35s/it] 10%|█         | 2/20 [00:10<01:27,  4.88s/it] 10%|█         | 2/20 [00:10<01:28,  4.90s/it] 10%|█         | 2/20 [00:10<01:25,  4.75s/it] 10%|█         | 2/20 [00:10<01:30,  5.03s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.81s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.68s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.83s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.85s/it] 20%|██        | 4/20 [00:16<00:59,  3.71s/it] 20%|██        | 4/20 [00:16<01:01,  3.86s/it] 20%|██        | 4/20 [00:16<01:00,  3.76s/it] 20%|██        | 4/20 [00:17<01:02,  3.90s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.51s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.53s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.69s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.70s/it] 30%|███       | 6/20 [00:23<00:48,  3.43s/it] 30%|███       | 6/20 [00:23<00:48,  3.45s/it] 30%|███       | 6/20 [00:22<00:48,  3.44s/it] 30%|███       | 6/20 [00:23<00:49,  3.55s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.18s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.14s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.23s/it] 35%|███▌      | 7/20 [00:26<00:41,  3.22s/it] 40%|████      | 8/20 [00:29<00:39,  3.28s/it] 40%|████      | 8/20 [00:29<00:40,  3.38s/it] 40%|████      | 8/20 [00:29<00:41,  3.42s/it] 40%|████      | 8/20 [00:30<00:40,  3.38s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.19s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.25s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.32s/it] 45%|████▌     | 9/20 [00:33<00:35,  3.26s/it] 50%|█████     | 10/20 [00:35<00:33,  3.31s/it] 50%|█████     | 10/20 [00:36<00:34,  3.41s/it] 50%|█████     | 10/20 [00:36<00:36,  3.66s/it] 50%|█████     | 10/20 [00:37<00:38,  3.84s/it] 55%|█████▌    | 11/20 [00:39<00:32,  3.57s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.77s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.64s/it] 55%|█████▌    | 11/20 [00:42<00:35,  3.94s/it] 60%|██████    | 12/20 [00:44<00:31,  3.94s/it] 60%|██████    | 12/20 [00:45<00:32,  4.05s/it] 60%|██████    | 12/20 [00:45<00:31,  3.98s/it] 60%|██████    | 12/20 [00:46<00:34,  4.26s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.11s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.14s/it] 65%|██████▌   | 13/20 [00:50<00:28,  4.09s/it] 65%|██████▌   | 13/20 [00:51<00:30,  4.41s/it] 70%|███████   | 14/20 [00:52<00:24,  4.02s/it] 70%|███████   | 14/20 [00:53<00:23,  4.00s/it] 70%|███████   | 14/20 [00:53<00:24,  4.04s/it] 70%|███████   | 14/20 [00:56<00:26,  4.44s/it] 75%|███████▌  | 15/20 [00:56<00:18,  3.74s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.92s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.88s/it] 80%|████████  | 16/20 [00:59<00:14,  3.51s/it] 80%|████████  | 16/20 [01:00<00:14,  3.61s/it] 75%|███████▌  | 15/20 [01:00<00:22,  4.47s/it] 80%|████████  | 16/20 [01:00<00:14,  3.72s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.45s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.52s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.57s/it] 80%|████████  | 16/20 [01:04<00:16,  4.12s/it] 90%|█████████ | 18/20 [01:05<00:06,  3.40s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.39s/it] 85%|████████▌ | 17/20 [01:07<00:11,  3.80s/it] 90%|█████████ | 18/20 [01:07<00:06,  3.44s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.40s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.49s/it] 90%|█████████ | 18/20 [01:10<00:07,  3.65s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.47s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.60s/it]100%|██████████| 20/20 [01:13<00:00,  3.74s/it]100%|██████████| 20/20 [01:13<00:00,  3.68s/it]
100%|██████████| 20/20 [01:14<00:00,  3.69s/it]100%|██████████| 20/20 [01:14<00:00,  3.72s/it]
100%|██████████| 20/20 [01:14<00:00,  3.68s/it]100%|██████████| 20/20 [01:14<00:00,  3.74s/it]
100%|██████████| 20/20 [01:18<00:00,  3.89s/it]100%|██████████| 20/20 [01:18<00:00,  3.93s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 05:07:18,564 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_77/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 05:07:18,564 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 05:07:18,565 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 05:07:18,565 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 05:09:38,918 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 05:09:38,919 - RUN - INFO - Evaluation Results:
2025-02-23 05:09:38,924 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.6
10  3        cross   0.42
11  3        single  0.78
12  Average  all     0.61
13  Average  cross   0.4475
14  Average  single  0.7725
--  -------  ------  ------
[rank0]:[W223 05:09:39.018120779 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 05:09:45.703229 2629656 site-packages/torch/distributed/run.py:793] 
W0223 05:09:45.703229 2629656 site-packages/torch/distributed/run.py:793] *****************************************
W0223 05:09:45.703229 2629656 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 05:09:45.703229 2629656 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank2]:[W223 05:10:11.607008361 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 05:10:11.607010230 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 05:10:11.607971544 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W223 05:10:11.615098050 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.30s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.40s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]
Max step: 79, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Max step: 79, bias_value: 0.2
Max step: 79, bias_value: 0.2
Max step: 79, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
  0%|          | 0/5 [00:00<?, ?it/s]Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [04:08<16:35, 248.89s/it] 20%|██        | 1/5 [05:11<20:45, 311.45s/it] 20%|██        | 1/5 [05:29<21:56, 329.25s/it] 20%|██        | 1/5 [05:52<23:28, 352.16s/it] 40%|████      | 2/5 [08:16<12:24, 248.11s/it] 40%|████      | 2/5 [09:08<13:01, 260.61s/it] 40%|████      | 2/5 [10:04<14:53, 297.75s/it] 40%|████      | 2/5 [10:15<15:21, 307.22s/it] 60%|██████    | 3/5 [12:22<08:14, 247.35s/it] 60%|██████    | 3/5 [15:28<10:30, 315.24s/it] 60%|██████    | 3/5 [15:20<10:12, 306.03s/it] 60%|██████    | 3/5 [15:44<10:33, 316.99s/it] 80%|████████  | 4/5 [16:42<04:12, 252.01s/it] 80%|████████  | 4/5 [20:46<05:16, 316.26s/it] 80%|████████  | 4/5 [21:16<05:25, 325.76s/it] 80%|████████  | 4/5 [21:57<05:39, 339.05s/it]100%|██████████| 5/5 [22:20<00:00, 283.34s/it]100%|██████████| 5/5 [22:20<00:00, 268.19s/it]
100%|██████████| 5/5 [24:49<00:00, 289.63s/it]100%|██████████| 5/5 [24:49<00:00, 297.83s/it]
100%|██████████| 5/5 [25:59<00:00, 310.21s/it]100%|██████████| 5/5 [25:59<00:00, 311.90s/it]
100%|██████████| 5/5 [26:46<00:00, 320.95s/it]100%|██████████| 5/5 [26:46<00:00, 321.21s/it]
[rank0]:[W223 05:38:21.972857800 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 05:38:28.515921 2672687 site-packages/torch/distributed/run.py:793] 
W0223 05:38:28.515921 2672687 site-packages/torch/distributed/run.py:793] *****************************************
W0223 05:38:28.515921 2672687 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 05:38:28.515921 2672687 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W223 05:38:56.662502395 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 05:38:56.679652139 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 05:38:56.681063698 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 05:39:27.708156504 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.24s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.40s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.24it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_79/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_79/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_79/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_79/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json


Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:07<02:25,  7.68s/it]  5%|▌         | 1/20 [00:09<02:57,  9.36s/it]  5%|▌         | 1/20 [00:09<03:00,  9.48s/it]  5%|▌         | 1/20 [00:09<03:02,  9.60s/it] 10%|█         | 2/20 [00:12<01:45,  5.84s/it] 10%|█         | 2/20 [00:11<01:36,  5.38s/it] 10%|█         | 2/20 [00:12<01:45,  5.89s/it] 10%|█         | 2/20 [00:13<01:49,  6.06s/it] 15%|█▌        | 3/20 [00:15<01:13,  4.31s/it] 15%|█▌        | 3/20 [00:15<01:14,  4.36s/it] 15%|█▌        | 3/20 [00:13<01:09,  4.07s/it] 15%|█▌        | 3/20 [00:15<01:16,  4.48s/it] 20%|██        | 4/20 [00:18<01:04,  4.04s/it] 20%|██        | 4/20 [00:17<01:03,  4.00s/it] 20%|██        | 4/20 [00:19<01:07,  4.20s/it] 20%|██        | 4/20 [00:20<01:10,  4.39s/it] 25%|██▌       | 5/20 [00:22<00:56,  3.75s/it] 25%|██▌       | 5/20 [00:22<00:57,  3.82s/it] 25%|██▌       | 5/20 [00:21<00:56,  3.78s/it] 25%|██▌       | 5/20 [00:23<01:00,  4.04s/it] 30%|███       | 6/20 [00:25<00:50,  3.60s/it] 30%|███       | 6/20 [00:25<00:51,  3.66s/it] 30%|███       | 6/20 [00:24<00:49,  3.57s/it] 30%|███       | 6/20 [00:26<00:53,  3.81s/it] 35%|███▌      | 7/20 [00:28<00:42,  3.27s/it] 35%|███▌      | 7/20 [00:28<00:43,  3.34s/it] 35%|███▌      | 7/20 [00:27<00:43,  3.34s/it] 35%|███▌      | 7/20 [00:29<00:44,  3.42s/it] 40%|████      | 8/20 [00:31<00:40,  3.34s/it] 40%|████      | 8/20 [00:32<00:42,  3.52s/it] 40%|████      | 8/20 [00:31<00:41,  3.49s/it] 40%|████      | 8/20 [00:33<00:42,  3.52s/it] 45%|████▌     | 9/20 [00:34<00:35,  3.23s/it] 45%|████▌     | 9/20 [00:35<00:37,  3.37s/it] 45%|████▌     | 9/20 [00:34<00:37,  3.39s/it] 45%|████▌     | 9/20 [00:36<00:37,  3.38s/it] 50%|█████     | 10/20 [00:38<00:34,  3.41s/it] 50%|█████     | 10/20 [00:39<00:36,  3.68s/it] 50%|█████     | 10/20 [00:39<00:34,  3.49s/it] 50%|█████     | 10/20 [00:39<00:38,  3.83s/it] 55%|█████▌    | 11/20 [00:43<00:32,  3.64s/it] 55%|█████▌    | 11/20 [00:43<00:34,  3.81s/it] 55%|█████▌    | 11/20 [00:44<00:34,  3.79s/it] 55%|█████▌    | 11/20 [00:43<00:35,  3.97s/it] 60%|██████    | 12/20 [00:48<00:32,  4.03s/it] 60%|██████    | 12/20 [00:48<00:32,  4.11s/it] 60%|██████    | 12/20 [00:49<00:33,  4.18s/it] 60%|██████    | 12/20 [00:48<00:33,  4.24s/it] 65%|██████▌   | 13/20 [00:52<00:29,  4.22s/it] 65%|██████▌   | 13/20 [00:52<00:29,  4.20s/it] 65%|██████▌   | 13/20 [00:54<00:30,  4.30s/it] 65%|██████▌   | 13/20 [00:52<00:30,  4.40s/it] 70%|███████   | 14/20 [00:56<00:24,  4.08s/it] 70%|███████   | 14/20 [00:56<00:24,  4.16s/it] 70%|███████   | 14/20 [00:58<00:25,  4.22s/it] 70%|███████   | 14/20 [00:57<00:26,  4.42s/it] 75%|███████▌  | 15/20 [00:59<00:19,  3.83s/it] 75%|███████▌  | 15/20 [01:00<00:20,  4.06s/it] 75%|███████▌  | 15/20 [01:01<00:20,  4.09s/it] 80%|████████  | 16/20 [01:02<00:14,  3.66s/it] 75%|███████▌  | 15/20 [01:01<00:21,  4.38s/it] 80%|████████  | 16/20 [01:03<00:14,  3.63s/it] 80%|████████  | 16/20 [01:05<00:15,  3.86s/it] 85%|████████▌ | 17/20 [01:06<00:10,  3.56s/it] 80%|████████  | 16/20 [01:04<00:16,  4.02s/it] 85%|████████▌ | 17/20 [01:06<00:10,  3.56s/it] 85%|████████▌ | 17/20 [01:08<00:11,  3.68s/it] 90%|█████████ | 18/20 [01:09<00:06,  3.43s/it] 85%|████████▌ | 17/20 [01:08<00:11,  3.75s/it] 90%|█████████ | 18/20 [01:09<00:06,  3.48s/it] 90%|█████████ | 18/20 [01:11<00:07,  3.55s/it] 90%|█████████ | 18/20 [01:11<00:07,  3.61s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.48s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.46s/it] 95%|█████████▌| 19/20 [01:15<00:03,  3.57s/it] 95%|█████████▌| 19/20 [01:14<00:03,  3.54s/it]100%|██████████| 20/20 [01:17<00:00,  3.69s/it]100%|██████████| 20/20 [01:17<00:00,  3.85s/it]
100%|██████████| 20/20 [01:17<00:00,  3.82s/it]100%|██████████| 20/20 [01:17<00:00,  3.90s/it]
100%|██████████| 20/20 [01:19<00:00,  3.76s/it]100%|██████████| 20/20 [01:19<00:00,  3.98s/it]
100%|██████████| 20/20 [01:19<00:00,  3.82s/it]100%|██████████| 20/20 [01:19<00:00,  3.96s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 05:42:10,061 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_79/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 05:42:10,061 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 05:42:10,061 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 05:42:10,061 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 05:44:39,675 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 05:44:39,675 - RUN - INFO - Evaluation Results:
2025-02-23 05:44:39,678 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.655
 1  0        single  0.76
 2  0        cross   0.55
 3  1        all     0.605
 4  1        single  0.79
 5  1        cross   0.42
 6  2        all     0.58
 7  2        single  0.76
 8  2        cross   0.4
 9  3        all     0.6
10  3        single  0.78
11  3        cross   0.42
12  Average  all     0.61
13  Average  single  0.7725
14  Average  cross   0.4475
--  -------  ------  ------
[rank0]:[W223 05:44:40.594118202 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 05:44:46.259075 2681771 site-packages/torch/distributed/run.py:793] 
W0223 05:44:46.259075 2681771 site-packages/torch/distributed/run.py:793] *****************************************
W0223 05:44:46.259075 2681771 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 05:44:46.259075 2681771 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank0]:[W223 05:45:13.578717465 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 05:45:13.582448625 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 05:45:13.583512060 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 05:45:13.583948361 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.34it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s]
Max step: 81, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Max step: 81, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]Max step: 81, bias_value: 0.2
Max step: 81, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [04:09<16:39, 249.76s/it] 20%|██        | 1/5 [05:11<20:44, 311.15s/it] 20%|██        | 1/5 [05:32<22:08, 332.03s/it] 20%|██        | 1/5 [05:55<23:41, 355.48s/it] 40%|████      | 2/5 [08:16<12:24, 248.10s/it] 40%|████      | 2/5 [09:13<13:08, 262.73s/it] 40%|████      | 2/5 [10:18<15:27, 309.16s/it] 40%|████      | 2/5 [10:07<14:56, 298.88s/it] 60%|██████    | 3/5 [12:25<08:17, 248.59s/it] 60%|██████    | 3/5 [15:42<10:41, 320.69s/it] 60%|██████    | 3/5 [15:29<10:18, 309.48s/it] 60%|██████    | 3/5 [15:52<10:40, 320.09s/it] 80%|████████  | 4/5 [16:47<04:13, 253.78s/it] 80%|████████  | 4/5 [21:03<05:20, 320.55s/it] 80%|████████  | 4/5 [21:30<05:29, 329.58s/it] 80%|████████  | 4/5 [22:08<05:42, 342.22s/it]100%|██████████| 5/5 [22:30<00:00, 286.03s/it]100%|██████████| 5/5 [22:30<00:00, 270.16s/it]
100%|██████████| 5/5 [25:08<00:00, 293.42s/it]100%|██████████| 5/5 [25:08<00:00, 301.72s/it]
100%|██████████| 5/5 [26:15<00:00, 313.50s/it]100%|██████████| 5/5 [26:15<00:00, 315.04s/it]
100%|██████████| 5/5 [27:00<00:00, 324.39s/it]100%|██████████| 5/5 [27:00<00:00, 324.19s/it]
[rank0]:[W223 06:13:21.435309918 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 06:13:28.149869 2727610 site-packages/torch/distributed/run.py:793] 
W0223 06:13:28.149869 2727610 site-packages/torch/distributed/run.py:793] *****************************************
W0223 06:13:28.149869 2727610 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 06:13:28.149869 2727610 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank3]:[W223 06:13:57.910583713 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 06:13:57.911226681 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 06:13:57.911589637 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 06:14:28.625926810 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_81/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_81/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Rank 0:  Model Class: LlavaQwenForCausalLM
Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.32s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]
  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:41,  5.34s/it]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
 10%|█         | 2/20 [00:09<01:19,  4.42s/it]  5%|▌         | 1/20 [00:06<02:01,  6.41s/it]Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_81/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_81/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
 15%|█▌        | 3/20 [00:11<01:00,  3.56s/it] 10%|█         | 2/20 [00:09<01:22,  4.58s/it] 20%|██        | 4/20 [00:15<00:58,  3.67s/it] 15%|█▌        | 3/20 [00:12<01:01,  3.62s/it] 25%|██▌       | 5/20 [00:18<00:53,  3.54s/it] 20%|██        | 4/20 [00:16<01:00,  3.77s/it] 30%|███       | 6/20 [00:21<00:46,  3.35s/it]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s] 25%|██▌       | 5/20 [00:19<00:55,  3.71s/it] 35%|███▌      | 7/20 [00:26<00:49,  3.79s/it] 30%|███       | 6/20 [00:26<01:05,  4.66s/it]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
 40%|████      | 8/20 [00:32<00:52,  4.41s/it] 35%|███▌      | 7/20 [00:28<00:51,  3.97s/it]  5%|▌         | 1/20 [00:10<03:26, 10.89s/it]  5%|▌         | 1/20 [00:10<03:26, 10.87s/it] 45%|████▌     | 9/20 [00:35<00:43,  3.98s/it] 40%|████      | 8/20 [00:32<00:46,  3.86s/it] 10%|█         | 2/20 [00:14<01:55,  6.40s/it] 10%|█         | 2/20 [00:14<01:55,  6.44s/it] 15%|█▌        | 3/20 [00:16<01:17,  4.56s/it] 45%|████▌     | 9/20 [00:35<00:39,  3.59s/it] 15%|█▌        | 3/20 [00:16<01:19,  4.68s/it] 50%|█████     | 10/20 [00:40<00:42,  4.26s/it] 20%|██        | 4/20 [00:20<01:08,  4.26s/it] 50%|█████     | 10/20 [00:39<00:35,  3.59s/it] 20%|██        | 4/20 [00:20<01:08,  4.27s/it] 55%|█████▌    | 11/20 [00:44<00:37,  4.20s/it] 25%|██▌       | 5/20 [00:23<00:57,  3.84s/it] 25%|██▌       | 5/20 [00:23<00:58,  3.88s/it] 55%|█████▌    | 11/20 [00:43<00:33,  3.70s/it] 60%|██████    | 12/20 [00:49<00:35,  4.39s/it] 30%|███       | 6/20 [00:26<00:51,  3.65s/it] 30%|███       | 6/20 [00:26<00:51,  3.69s/it] 60%|██████    | 12/20 [00:47<00:32,  4.02s/it] 35%|███▌      | 7/20 [00:29<00:42,  3.28s/it] 35%|███▌      | 7/20 [00:29<00:43,  3.33s/it] 65%|██████▌   | 13/20 [00:53<00:31,  4.50s/it] 40%|████      | 8/20 [00:32<00:40,  3.38s/it] 40%|████      | 8/20 [00:33<00:40,  3.39s/it] 65%|██████▌   | 13/20 [00:52<00:28,  4.14s/it] 70%|███████   | 14/20 [00:58<00:26,  4.47s/it] 45%|████▌     | 9/20 [00:35<00:35,  3.24s/it] 45%|████▌     | 9/20 [00:36<00:35,  3.26s/it] 70%|███████   | 14/20 [00:56<00:24,  4.10s/it] 50%|█████     | 10/20 [00:39<00:33,  3.33s/it] 75%|███████▌  | 15/20 [01:02<00:22,  4.45s/it] 50%|█████     | 10/20 [00:40<00:37,  3.71s/it] 75%|███████▌  | 15/20 [00:59<00:19,  3.92s/it] 80%|████████  | 16/20 [01:05<00:16,  4.05s/it] 55%|█████▌    | 11/20 [00:43<00:31,  3.54s/it] 80%|████████  | 16/20 [01:02<00:14,  3.73s/it] 55%|█████▌    | 11/20 [00:44<00:34,  3.81s/it] 85%|████████▌ | 17/20 [01:08<00:11,  3.75s/it] 85%|████████▌ | 17/20 [01:06<00:10,  3.58s/it] 60%|██████    | 12/20 [00:48<00:31,  3.91s/it] 90%|█████████ | 18/20 [01:11<00:07,  3.58s/it] 60%|██████    | 12/20 [00:49<00:32,  4.07s/it] 90%|█████████ | 18/20 [01:09<00:06,  3.45s/it] 65%|██████▌   | 13/20 [00:52<00:29,  4.16s/it] 95%|█████████▌| 19/20 [01:15<00:03,  3.56s/it] 65%|██████▌   | 13/20 [00:53<00:29,  4.16s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.49s/it] 70%|███████   | 14/20 [00:56<00:24,  4.07s/it]100%|██████████| 20/20 [01:19<00:00,  3.79s/it]100%|██████████| 20/20 [01:19<00:00,  3.99s/it]
 70%|███████   | 14/20 [00:57<00:24,  4.03s/it]100%|██████████| 20/20 [01:17<00:00,  3.71s/it]100%|██████████| 20/20 [01:17<00:00,  3.86s/it]
 75%|███████▌  | 15/20 [01:00<00:19,  3.99s/it] 75%|███████▌  | 15/20 [01:00<00:19,  3.84s/it] 80%|████████  | 16/20 [01:03<00:14,  3.59s/it] 80%|████████  | 16/20 [01:04<00:14,  3.74s/it] 85%|████████▌ | 17/20 [01:06<00:10,  3.47s/it] 85%|████████▌ | 17/20 [01:07<00:10,  3.59s/it] 90%|█████████ | 18/20 [01:09<00:06,  3.41s/it] 90%|█████████ | 18/20 [01:10<00:06,  3.46s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.40s/it] 95%|█████████▌| 19/20 [01:14<00:03,  3.51s/it]100%|██████████| 20/20 [01:17<00:00,  3.74s/it]100%|██████████| 20/20 [01:17<00:00,  3.87s/it]
100%|██████████| 20/20 [01:18<00:00,  3.75s/it]100%|██████████| 20/20 [01:18<00:00,  3.94s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 06:17:13,569 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_81/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 06:17:13,569 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 06:17:13,569 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 06:17:13,569 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 06:19:35,316 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 06:19:35,317 - RUN - INFO - Evaluation Results:
2025-02-23 06:19:35,319 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.655
 1  0        single  0.76
 2  0        cross   0.55
 3  1        all     0.605
 4  1        single  0.79
 5  1        cross   0.42
 6  2        all     0.58
 7  2        single  0.76
 8  2        cross   0.4
 9  3        all     0.6
10  3        single  0.78
11  3        cross   0.42
12  Average  all     0.61
13  Average  single  0.7725
14  Average  cross   0.4475
--  -------  ------  ------
[rank0]:[W223 06:19:36.178044585 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 06:19:42.721061 2737187 site-packages/torch/distributed/run.py:793] 
W0223 06:19:42.721061 2737187 site-packages/torch/distributed/run.py:793] *****************************************
W0223 06:19:42.721061 2737187 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 06:19:42.721061 2737187 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank1]:[W223 06:20:11.347238273 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 06:20:11.347239530 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W223 06:20:11.364728421 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 06:20:11.366139378 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.37s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.39s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]
Max step: 83, bias_value: 0.2
Max step: 83, bias_value: 0.2Max step: 83, bias_value: 0.2Max step: 83, bias_value: 0.2


Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [04:12<16:51, 252.89s/it] 20%|██        | 1/5 [05:17<21:09, 317.46s/it] 20%|██        | 1/5 [05:36<22:26, 336.65s/it] 20%|██        | 1/5 [06:01<24:04, 361.11s/it] 40%|████      | 2/5 [08:25<12:38, 252.82s/it] 40%|████      | 2/5 [09:20<13:18, 266.20s/it] 40%|████      | 2/5 [10:15<15:08, 302.70s/it] 40%|████      | 2/5 [10:33<15:49, 316.45s/it] 60%|██████    | 3/5 [12:43<08:29, 254.91s/it] 60%|██████    | 3/5 [15:41<10:26, 313.23s/it] 60%|██████    | 3/5 [15:54<10:48, 324.39s/it] 60%|██████    | 3/5 [16:12<10:54, 327.05s/it] 80%|████████  | 4/5 [17:06<04:18, 258.29s/it] 80%|████████  | 4/5 [21:19<05:24, 324.63s/it] 80%|████████  | 4/5 [21:41<05:31, 331.78s/it] 80%|████████  | 4/5 [22:34<05:48, 348.47s/it]100%|██████████| 5/5 [22:52<00:00, 290.06s/it]100%|██████████| 5/5 [22:52<00:00, 274.58s/it]
100%|██████████| 5/5 [25:28<00:00, 297.23s/it]100%|██████████| 5/5 [25:28<00:00, 305.63s/it]
100%|██████████| 5/5 [26:31<00:00, 316.52s/it]100%|██████████| 5/5 [26:31<00:00, 318.20s/it]
100%|██████████| 5/5 [27:30<00:00, 329.66s/it]100%|██████████| 5/5 [27:30<00:00, 330.10s/it]
[rank0]:[W223 06:49:10.916428924 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 06:49:17.655813 2785731 site-packages/torch/distributed/run.py:793] 
W0223 06:49:17.655813 2785731 site-packages/torch/distributed/run.py:793] *****************************************
W0223 06:49:17.655813 2785731 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 06:49:17.655813 2785731 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W223 06:49:46.717432317 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 06:49:46.719648998 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 06:49:46.731200060 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 06:50:16.875495790 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.61s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.58s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.48s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_83/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_83/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_83/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonMax step: 50, bias_value: 0.2

load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_83/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:05<01:49,  5.76s/it]  5%|▌         | 1/20 [00:06<02:10,  6.85s/it]  5%|▌         | 1/20 [00:07<02:16,  7.16s/it]  5%|▌         | 1/20 [00:07<02:14,  7.06s/it] 10%|█         | 2/20 [00:10<01:25,  4.77s/it] 10%|█         | 2/20 [00:10<01:26,  4.82s/it] 10%|█         | 2/20 [00:10<01:29,  4.99s/it] 10%|█         | 2/20 [00:09<01:23,  4.62s/it] 15%|█▌        | 3/20 [00:12<01:01,  3.64s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.75s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.84s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.66s/it] 20%|██        | 4/20 [00:16<00:58,  3.67s/it] 20%|██        | 4/20 [00:16<00:59,  3.70s/it] 20%|██        | 4/20 [00:15<00:59,  3.73s/it] 20%|██        | 4/20 [00:17<01:03,  3.94s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.47s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.50s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.62s/it] 25%|██▌       | 5/20 [00:20<00:56,  3.76s/it] 30%|███       | 6/20 [00:22<00:47,  3.41s/it] 30%|███       | 6/20 [00:22<00:47,  3.42s/it] 30%|███       | 6/20 [00:22<00:47,  3.41s/it] 30%|███       | 6/20 [00:24<00:50,  3.64s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.13s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.13s/it] 35%|███▌      | 7/20 [00:25<00:42,  3.27s/it] 35%|███▌      | 7/20 [00:26<00:43,  3.32s/it] 40%|████      | 8/20 [00:28<00:39,  3.29s/it] 40%|████      | 8/20 [00:28<00:39,  3.31s/it] 40%|████      | 8/20 [00:30<00:41,  3.43s/it] 40%|████      | 8/20 [00:29<00:41,  3.48s/it] 45%|████▌     | 9/20 [00:31<00:35,  3.19s/it] 45%|████▌     | 9/20 [00:31<00:35,  3.19s/it] 45%|████▌     | 9/20 [00:33<00:36,  3.30s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.36s/it] 50%|█████     | 10/20 [00:35<00:32,  3.30s/it] 50%|█████     | 10/20 [00:36<00:36,  3.69s/it] 50%|█████     | 10/20 [00:37<00:34,  3.48s/it] 50%|█████     | 10/20 [00:37<00:38,  3.86s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.54s/it] 55%|█████▌    | 11/20 [00:40<00:34,  3.80s/it] 55%|█████▌    | 11/20 [00:41<00:33,  3.70s/it] 55%|█████▌    | 11/20 [00:41<00:35,  4.00s/it] 60%|██████    | 12/20 [00:44<00:31,  3.92s/it] 60%|██████    | 12/20 [00:45<00:32,  4.07s/it] 60%|██████    | 12/20 [00:46<00:32,  4.06s/it] 60%|██████    | 12/20 [00:46<00:33,  4.24s/it] 65%|██████▌   | 13/20 [00:48<00:28,  4.12s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.16s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.16s/it] 65%|██████▌   | 13/20 [00:51<00:30,  4.39s/it] 70%|███████   | 14/20 [00:52<00:24,  4.02s/it] 70%|███████   | 14/20 [00:53<00:23,  3.99s/it] 70%|███████   | 14/20 [00:54<00:24,  4.10s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.93s/it] 75%|███████▌  | 15/20 [00:56<00:18,  3.74s/it] 70%|███████   | 14/20 [00:55<00:26,  4.44s/it] 75%|███████▌  | 15/20 [00:58<00:19,  3.98s/it] 80%|████████  | 16/20 [00:58<00:14,  3.52s/it] 80%|████████  | 16/20 [01:00<00:14,  3.68s/it] 75%|███████▌  | 15/20 [00:59<00:21,  4.36s/it] 80%|████████  | 16/20 [01:01<00:15,  3.85s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.44s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.54s/it] 80%|████████  | 16/20 [01:03<00:16,  4.01s/it] 85%|████████▌ | 17/20 [01:05<00:11,  3.67s/it] 90%|█████████ | 18/20 [01:05<00:06,  3.35s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.42s/it] 85%|████████▌ | 17/20 [01:06<00:11,  3.75s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.52s/it] 95%|█████████▌| 19/20 [01:08<00:03,  3.34s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.46s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.60s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.55s/it]100%|██████████| 20/20 [01:13<00:00,  3.72s/it]100%|██████████| 20/20 [01:13<00:00,  3.66s/it]
 95%|█████████▌| 19/20 [01:12<00:03,  3.53s/it]100%|██████████| 20/20 [01:14<00:00,  3.67s/it]100%|██████████| 20/20 [01:14<00:00,  3.71s/it]
100%|██████████| 20/20 [01:16<00:00,  3.77s/it]100%|██████████| 20/20 [01:16<00:00,  3.81s/it]
100%|██████████| 20/20 [01:17<00:00,  3.79s/it]100%|██████████| 20/20 [01:17<00:00,  3.86s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 06:53:09,201 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_83/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 06:53:09,201 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 06:53:09,201 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 06:53:09,201 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 06:55:30,558 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 06:55:30,558 - RUN - INFO - Evaluation Results:
2025-02-23 06:55:30,561 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.6
10  3        cross   0.42
11  3        single  0.78
12  Average  all     0.61
13  Average  cross   0.4475
14  Average  single  0.7725
--  -------  ------  ------
[rank0]:[W223 06:55:31.535694580 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 06:55:37.285929 2795462 site-packages/torch/distributed/run.py:793] 
W0223 06:55:37.285929 2795462 site-packages/torch/distributed/run.py:793] *****************************************
W0223 06:55:37.285929 2795462 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 06:55:37.285929 2795462 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank0]:[W223 06:56:05.321556922 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 06:56:05.323257012 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 06:56:05.323472475 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 06:56:05.324491671 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.05s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.15s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.24it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Max step: 85, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 85, bias_value: 0.2
Max step: 85, bias_value: 0.2
Max step: 85, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [04:18<17:12, 258.17s/it] 20%|██        | 1/5 [05:18<21:12, 318.04s/it] 20%|██        | 1/5 [05:42<22:48, 342.09s/it] 20%|██        | 1/5 [06:03<24:13, 363.27s/it] 40%|████      | 2/5 [08:27<12:38, 252.97s/it] 40%|████      | 2/5 [09:25<13:24, 268.26s/it] 40%|████      | 2/5 [10:21<15:16, 305.40s/it] 40%|████      | 2/5 [10:35<15:53, 317.70s/it] 60%|██████    | 3/5 [12:37<08:23, 251.78s/it] 60%|██████    | 3/5 [15:50<10:31, 315.86s/it] 60%|██████    | 3/5 [16:01<10:53, 326.69s/it] 60%|██████    | 3/5 [16:19<10:59, 329.78s/it] 80%|████████  | 4/5 [17:06<04:18, 258.55s/it] 80%|████████  | 4/5 [21:26<05:26, 326.19s/it] 80%|████████  | 4/5 [21:52<05:34, 334.11s/it] 80%|████████  | 4/5 [22:46<05:52, 352.11s/it]100%|██████████| 5/5 [22:57<00:00, 291.83s/it]100%|██████████| 5/5 [22:57<00:00, 275.53s/it]
100%|██████████| 5/5 [25:36<00:00, 298.62s/it]100%|██████████| 5/5 [25:36<00:00, 307.28s/it]
100%|██████████| 5/5 [26:42<00:00, 318.44s/it]100%|██████████| 5/5 [26:42<00:00, 320.57s/it]
100%|██████████| 5/5 [27:44<00:00, 332.87s/it]100%|██████████| 5/5 [27:44<00:00, 332.95s/it]
[rank0]:[W223 07:25:23.356163953 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 07:25:29.994907 2845659 site-packages/torch/distributed/run.py:793] 
W0223 07:25:29.994907 2845659 site-packages/torch/distributed/run.py:793] *****************************************
W0223 07:25:29.994907 2845659 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 07:25:29.994907 2845659 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W223 07:25:57.688395220 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 07:25:57.688395173 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 07:25:57.688729850 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 07:26:28.894791426 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.37s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.42s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.25s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_85/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_85/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_85/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_85/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:03,  6.48s/it]  5%|▌         | 1/20 [00:07<02:15,  7.11s/it]  5%|▌         | 1/20 [00:07<02:15,  7.15s/it]  5%|▌         | 1/20 [00:07<02:17,  7.22s/it] 10%|█         | 2/20 [00:10<01:27,  4.87s/it] 10%|█         | 2/20 [00:10<01:28,  4.89s/it] 10%|█         | 2/20 [00:10<01:28,  4.91s/it] 10%|█         | 2/20 [00:10<01:29,  4.95s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.80s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.80s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.83s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.85s/it] 20%|██        | 4/20 [00:16<01:01,  3.81s/it] 20%|██        | 4/20 [00:16<00:59,  3.73s/it] 20%|██        | 4/20 [00:17<01:02,  3.90s/it] 20%|██        | 4/20 [00:17<01:02,  3.88s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.48s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.68s/it] 25%|██▌       | 5/20 [00:20<00:54,  3.65s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.69s/it] 30%|███       | 6/20 [00:22<00:47,  3.40s/it] 30%|███       | 6/20 [00:23<00:49,  3.51s/it] 30%|███       | 6/20 [00:23<00:49,  3.52s/it] 30%|███       | 6/20 [00:23<00:49,  3.56s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.13s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.19s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.31s/it] 35%|███▌      | 7/20 [00:26<00:41,  3.23s/it] 40%|████      | 8/20 [00:28<00:38,  3.23s/it] 40%|████      | 8/20 [00:29<00:40,  3.38s/it] 40%|████      | 8/20 [00:29<00:40,  3.36s/it] 40%|████      | 8/20 [00:29<00:41,  3.47s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.17s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.24s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.24s/it] 45%|████▌     | 9/20 [00:33<00:36,  3.35s/it] 50%|█████     | 10/20 [00:36<00:33,  3.36s/it] 50%|█████     | 10/20 [00:36<00:33,  3.35s/it] 50%|█████     | 10/20 [00:36<00:36,  3.69s/it] 50%|█████     | 10/20 [00:37<00:38,  3.83s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.58s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.57s/it] 55%|█████▌    | 11/20 [00:40<00:34,  3.83s/it] 55%|█████▌    | 11/20 [00:42<00:35,  3.92s/it] 60%|██████    | 12/20 [00:45<00:31,  3.91s/it] 60%|██████    | 12/20 [00:45<00:31,  3.97s/it] 60%|██████    | 12/20 [00:45<00:32,  4.10s/it] 60%|██████    | 12/20 [00:46<00:33,  4.23s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.05s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.17s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.17s/it] 65%|██████▌   | 13/20 [00:51<00:30,  4.39s/it] 70%|███████   | 14/20 [00:53<00:24,  4.02s/it] 70%|███████   | 14/20 [00:53<00:24,  4.01s/it] 70%|███████   | 14/20 [00:53<00:24,  4.09s/it] 70%|███████   | 14/20 [00:56<00:26,  4.42s/it] 75%|███████▌  | 15/20 [00:56<00:18,  3.77s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.86s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.98s/it] 80%|████████  | 16/20 [01:00<00:14,  3.57s/it] 80%|████████  | 16/20 [01:00<00:14,  3.64s/it] 80%|████████  | 16/20 [01:00<00:14,  3.67s/it] 75%|███████▌  | 15/20 [01:00<00:22,  4.42s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.55s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.55s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.51s/it] 80%|████████  | 16/20 [01:03<00:16,  4.03s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.42s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.45s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.45s/it] 85%|████████▌ | 17/20 [01:06<00:11,  3.76s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.44s/it] 90%|█████████ | 18/20 [01:10<00:07,  3.63s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.43s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.47s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.61s/it]100%|██████████| 20/20 [01:14<00:00,  3.68s/it]100%|██████████| 20/20 [01:14<00:00,  3.72s/it]
100%|██████████| 20/20 [01:14<00:00,  3.71s/it]100%|██████████| 20/20 [01:14<00:00,  3.73s/it]
100%|██████████| 20/20 [01:14<00:00,  3.81s/it]100%|██████████| 20/20 [01:14<00:00,  3.75s/it]
100%|██████████| 20/20 [01:18<00:00,  3.87s/it]100%|██████████| 20/20 [01:18<00:00,  3.91s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 07:29:20,230 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_85/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 07:29:20,230 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 07:29:20,230 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 07:29:20,230 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 07:31:34,737 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 07:31:34,737 - RUN - INFO - Evaluation Results:
2025-02-23 07:31:34,739 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.6
10  3        cross   0.42
11  3        single  0.78
12  Average  all     0.61
13  Average  cross   0.4475
14  Average  single  0.7725
--  -------  ------  ------
[rank0]:[W223 07:31:35.620928252 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 07:31:42.113656 2854886 site-packages/torch/distributed/run.py:793] 
W0223 07:31:42.113656 2854886 site-packages/torch/distributed/run.py:793] *****************************************
W0223 07:31:42.113656 2854886 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 07:31:42.113656 2854886 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank0]:[W223 07:32:11.560056778 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 07:32:11.561607933 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 07:32:11.561970539 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 07:32:11.563459090 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.59s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.58s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.58s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]
Max step: 87, bias_value: 0.2
Max step: 87, bias_value: 0.2
Max step: 87, bias_value: 0.2
Max step: 87, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [04:14<16:59, 254.95s/it] 20%|██        | 1/5 [05:19<21:16, 319.03s/it] 20%|██        | 1/5 [05:45<23:03, 345.92s/it] 20%|██        | 1/5 [06:01<24:05, 361.26s/it] 40%|████      | 2/5 [08:26<12:38, 252.89s/it] 40%|████      | 2/5 [09:19<13:16, 265.53s/it] 40%|████      | 2/5 [10:27<15:25, 308.36s/it] 40%|████      | 2/5 [10:37<15:56, 318.85s/it] 60%|██████    | 3/5 [12:37<08:24, 252.20s/it] 60%|██████    | 3/5 [15:59<10:38, 319.15s/it] 60%|██████    | 3/5 [16:01<10:55, 327.75s/it] 60%|██████    | 3/5 [16:25<11:03, 331.83s/it] 80%|████████  | 4/5 [17:08<04:19, 259.50s/it] 80%|████████  | 4/5 [21:28<05:27, 327.30s/it] 80%|████████  | 4/5 [22:07<05:38, 338.20s/it] 80%|████████  | 4/5 [22:54<05:54, 354.70s/it]100%|██████████| 5/5 [23:05<00:00, 294.56s/it]100%|██████████| 5/5 [23:05<00:00, 277.04s/it]
100%|██████████| 5/5 [25:37<00:00, 299.09s/it]100%|██████████| 5/5 [25:37<00:00, 307.45s/it]
100%|██████████| 5/5 [27:04<00:00, 323.21s/it]100%|██████████| 5/5 [27:04<00:00, 324.81s/it]
100%|██████████| 5/5 [27:55<00:00, 335.33s/it]100%|██████████| 5/5 [27:55<00:00, 335.16s/it]
[rank0]:[W223 08:01:38.435900153 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 08:01:45.418377 2905074 site-packages/torch/distributed/run.py:793] 
W0223 08:01:45.418377 2905074 site-packages/torch/distributed/run.py:793] *****************************************
W0223 08:01:45.418377 2905074 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 08:01:45.418377 2905074 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W223 08:02:13.931587391 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 08:02:13.936805516 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 08:02:13.954532927 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 08:02:44.892277949 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.44s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.46s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.46s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_87/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_87/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_87/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_87/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:03,  6.50s/it]  5%|▌         | 1/20 [00:07<02:16,  7.18s/it]  5%|▌         | 1/20 [00:07<02:17,  7.25s/it]  5%|▌         | 1/20 [00:07<02:18,  7.30s/it] 10%|█         | 2/20 [00:10<01:26,  4.81s/it] 10%|█         | 2/20 [00:10<01:28,  4.93s/it] 10%|█         | 2/20 [00:10<01:27,  4.89s/it] 10%|█         | 2/20 [00:10<01:30,  5.04s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.77s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.83s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.83s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.86s/it] 20%|██        | 4/20 [00:16<01:01,  3.82s/it] 20%|██        | 4/20 [00:16<00:59,  3.72s/it] 20%|██        | 4/20 [00:16<01:01,  3.87s/it] 20%|██        | 4/20 [00:17<01:02,  3.93s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.55s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.66s/it] 25%|██▌       | 5/20 [00:20<00:53,  3.59s/it] 25%|██▌       | 5/20 [00:20<00:56,  3.76s/it] 30%|███       | 6/20 [00:22<00:48,  3.45s/it] 30%|███       | 6/20 [00:23<00:48,  3.45s/it] 30%|███       | 6/20 [00:23<00:48,  3.48s/it] 30%|███       | 6/20 [00:24<00:50,  3.62s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.19s/it] 35%|███▌      | 7/20 [00:25<00:42,  3.26s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.19s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.30s/it] 40%|████      | 8/20 [00:29<00:39,  3.31s/it] 40%|████      | 8/20 [00:29<00:39,  3.33s/it] 40%|████      | 8/20 [00:29<00:41,  3.42s/it] 40%|████      | 8/20 [00:30<00:41,  3.48s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.21s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.23s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.30s/it] 45%|████▌     | 9/20 [00:33<00:36,  3.34s/it] 50%|█████     | 10/20 [00:35<00:32,  3.28s/it] 50%|█████     | 10/20 [00:37<00:36,  3.70s/it] 50%|█████     | 10/20 [00:37<00:34,  3.45s/it] 50%|█████     | 10/20 [00:37<00:37,  3.78s/it] 55%|█████▌    | 11/20 [00:40<00:31,  3.54s/it] 55%|█████▌    | 11/20 [00:41<00:33,  3.77s/it] 55%|█████▌    | 11/20 [00:41<00:33,  3.69s/it] 55%|█████▌    | 11/20 [00:41<00:35,  3.90s/it] 60%|██████    | 12/20 [00:44<00:31,  3.91s/it] 60%|██████    | 12/20 [00:45<00:32,  4.04s/it] 60%|██████    | 12/20 [00:46<00:32,  4.05s/it] 60%|██████    | 12/20 [00:46<00:33,  4.18s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.12s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.17s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.18s/it] 65%|██████▌   | 13/20 [00:51<00:30,  4.38s/it] 70%|███████   | 14/20 [00:53<00:24,  4.03s/it] 70%|███████   | 14/20 [00:53<00:24,  4.02s/it] 70%|███████   | 14/20 [00:55<00:25,  4.20s/it] 70%|███████   | 14/20 [00:55<00:26,  4.39s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.93s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.80s/it] 75%|███████▌  | 15/20 [00:58<00:20,  4.03s/it] 80%|████████  | 16/20 [00:59<00:14,  3.52s/it] 75%|███████▌  | 15/20 [01:00<00:21,  4.36s/it] 80%|████████  | 16/20 [01:00<00:14,  3.68s/it] 80%|████████  | 16/20 [01:02<00:15,  3.84s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.45s/it] 80%|████████  | 16/20 [01:03<00:15,  3.99s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.55s/it] 85%|████████▌ | 17/20 [01:05<00:10,  3.65s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.37s/it] 85%|████████▌ | 17/20 [01:06<00:11,  3.72s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.41s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.54s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.34s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.56s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.47s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.60s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.56s/it]100%|██████████| 20/20 [01:13<00:00,  3.71s/it]100%|██████████| 20/20 [01:13<00:00,  3.69s/it]
100%|██████████| 20/20 [01:14<00:00,  3.66s/it]100%|██████████| 20/20 [01:14<00:00,  3.73s/it]
100%|██████████| 20/20 [01:16<00:00,  3.78s/it]100%|██████████| 20/20 [01:16<00:00,  3.83s/it]
100%|██████████| 20/20 [01:17<00:00,  3.76s/it]100%|██████████| 20/20 [01:17<00:00,  3.86s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 08:05:37,108 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_87/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 08:05:37,108 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 08:05:37,108 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 08:05:37,109 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 08:07:39,279 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 08:07:39,280 - RUN - INFO - Evaluation Results:
2025-02-23 08:07:39,282 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.6
10  3        cross   0.42
11  3        single  0.78
12  Average  all     0.61
13  Average  cross   0.4475
14  Average  single  0.7725
--  -------  ------  ------
[rank0]:[W223 08:07:40.149079516 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 08:07:45.887126 2920050 site-packages/torch/distributed/run.py:793] 
W0223 08:07:45.887126 2920050 site-packages/torch/distributed/run.py:793] *****************************************
W0223 08:07:45.887126 2920050 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 08:07:45.887126 2920050 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank2]:[W223 08:08:12.938934513 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W223 08:08:12.948158622 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 08:08:12.976609302 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 08:08:12.993565476 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.32s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.39it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]
Max step: 89, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 89, bias_value: 0.2
Max step: 89, bias_value: 0.2
Max step: 89, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [04:16<17:07, 256.85s/it] 20%|██        | 1/5 [05:23<21:34, 323.53s/it] 20%|██        | 1/5 [05:49<23:16, 349.03s/it] 20%|██        | 1/5 [06:02<24:10, 362.72s/it] 40%|████      | 2/5 [08:30<12:45, 255.05s/it] 40%|████      | 2/5 [09:24<13:24, 268.30s/it] 40%|████      | 2/5 [10:34<15:34, 311.36s/it] 40%|████      | 2/5 [10:48<16:12, 324.20s/it] 60%|██████    | 3/5 [12:45<08:29, 254.94s/it] 60%|██████    | 3/5 [16:08<11:00, 330.25s/it] 60%|██████    | 3/5 [16:12<10:47, 323.73s/it] 60%|██████    | 3/5 [16:39<11:13, 336.56s/it] 80%|████████  | 4/5 [17:15<04:21, 261.08s/it] 80%|████████  | 4/5 [21:42<05:31, 331.61s/it] 80%|████████  | 4/5 [22:23<05:42, 342.43s/it] 80%|████████  | 4/5 [23:12<05:58, 358.92s/it]100%|██████████| 5/5 [23:16<00:00, 296.80s/it]100%|██████████| 5/5 [23:16<00:00, 279.21s/it]
100%|██████████| 5/5 [25:53<00:00, 302.45s/it]100%|██████████| 5/5 [25:53<00:00, 310.67s/it]
100%|██████████| 5/5 [27:17<00:00, 325.03s/it]100%|██████████| 5/5 [27:17<00:00, 327.55s/it]
100%|██████████| 5/5 [28:15<00:00, 338.67s/it]100%|██████████| 5/5 [28:15<00:00, 339.09s/it]
[rank0]:[W223 08:37:59.229417080 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 08:38:06.199184 2995786 site-packages/torch/distributed/run.py:793] 
W0223 08:38:06.199184 2995786 site-packages/torch/distributed/run.py:793] *****************************************
W0223 08:38:06.199184 2995786 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 08:38:06.199184 2995786 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W223 08:38:32.039144045 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 08:38:32.039320043 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 08:38:32.054560694 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 08:39:03.847728569 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.27s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.20s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.22s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_89/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2

load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_89/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_89/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_89/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json


You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:05<01:53,  6.00s/it]  5%|▌         | 1/20 [00:07<02:13,  7.00s/it]  5%|▌         | 1/20 [00:07<02:16,  7.21s/it]  5%|▌         | 1/20 [00:07<02:18,  7.29s/it] 10%|█         | 2/20 [00:09<01:22,  4.61s/it] 10%|█         | 2/20 [00:10<01:26,  4.83s/it] 10%|█         | 2/20 [00:10<01:28,  4.93s/it] 10%|█         | 2/20 [00:10<01:31,  5.08s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.68s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.76s/it] 15%|█▌        | 3/20 [00:13<01:06,  3.92s/it] 15%|█▌        | 3/20 [00:13<01:06,  3.91s/it] 20%|██        | 4/20 [00:16<00:59,  3.74s/it] 20%|██        | 4/20 [00:16<01:00,  3.77s/it] 20%|██        | 4/20 [00:17<01:03,  3.95s/it] 20%|██        | 4/20 [00:19<01:16,  4.76s/it] 25%|██▌       | 5/20 [00:21<01:04,  4.33s/it] 25%|██▌       | 5/20 [00:22<01:05,  4.34s/it] 25%|██▌       | 5/20 [00:22<01:07,  4.50s/it] 25%|██▌       | 5/20 [00:22<01:04,  4.29s/it] 30%|███       | 6/20 [00:24<00:54,  3.89s/it] 30%|███       | 6/20 [00:25<00:56,  4.01s/it] 30%|███       | 6/20 [00:25<00:57,  4.10s/it] 30%|███       | 6/20 [00:26<00:55,  3.98s/it] 35%|███▌      | 7/20 [00:27<00:46,  3.57s/it] 35%|███▌      | 7/20 [00:28<00:46,  3.54s/it] 35%|███▌      | 7/20 [00:28<00:46,  3.60s/it] 35%|███▌      | 7/20 [00:28<00:46,  3.57s/it] 40%|████      | 8/20 [00:31<00:42,  3.53s/it] 40%|████      | 8/20 [00:31<00:43,  3.64s/it] 40%|████      | 8/20 [00:32<00:44,  3.69s/it] 40%|████      | 8/20 [00:32<00:43,  3.62s/it] 45%|████▌     | 9/20 [00:34<00:36,  3.36s/it] 45%|████▌     | 9/20 [00:34<00:38,  3.46s/it] 45%|████▌     | 9/20 [00:35<00:38,  3.46s/it] 45%|████▌     | 9/20 [00:35<00:38,  3.47s/it] 50%|█████     | 10/20 [00:38<00:34,  3.46s/it] 50%|█████     | 10/20 [00:38<00:38,  3.84s/it] 50%|█████     | 10/20 [00:39<00:38,  3.88s/it] 50%|█████     | 10/20 [00:39<00:36,  3.66s/it] 55%|█████▌    | 11/20 [00:42<00:32,  3.66s/it] 55%|█████▌    | 11/20 [00:43<00:35,  3.94s/it] 55%|█████▌    | 11/20 [00:43<00:35,  3.93s/it] 55%|█████▌    | 11/20 [00:44<00:34,  3.84s/it] 60%|██████    | 12/20 [00:47<00:32,  4.04s/it] 60%|██████    | 12/20 [00:47<00:33,  4.16s/it] 60%|██████    | 12/20 [00:48<00:33,  4.23s/it] 60%|██████    | 12/20 [00:49<00:33,  4.15s/it] 65%|██████▌   | 13/20 [00:52<00:29,  4.21s/it] 65%|██████▌   | 13/20 [00:52<00:29,  4.28s/it] 65%|██████▌   | 13/20 [00:52<00:30,  4.35s/it] 65%|██████▌   | 13/20 [00:53<00:29,  4.26s/it] 70%|███████   | 14/20 [00:56<00:24,  4.12s/it] 70%|███████   | 14/20 [00:56<00:24,  4.11s/it] 70%|███████   | 14/20 [00:56<00:26,  4.36s/it] 70%|███████   | 14/20 [00:57<00:25,  4.19s/it] 75%|███████▌  | 15/20 [00:59<00:19,  3.82s/it] 75%|███████▌  | 15/20 [01:00<00:20,  4.05s/it] 75%|███████▌  | 15/20 [01:01<00:20,  4.08s/it] 75%|███████▌  | 15/20 [01:01<00:21,  4.29s/it] 80%|████████  | 16/20 [01:02<00:14,  3.62s/it] 80%|████████  | 16/20 [01:03<00:14,  3.70s/it] 80%|████████  | 16/20 [01:04<00:15,  3.93s/it] 80%|████████  | 16/20 [01:04<00:15,  3.88s/it] 85%|████████▌ | 17/20 [01:06<00:10,  3.54s/it] 85%|████████▌ | 17/20 [01:06<00:10,  3.56s/it] 85%|████████▌ | 17/20 [01:07<00:10,  3.65s/it] 85%|████████▌ | 17/20 [01:08<00:11,  3.71s/it] 90%|█████████ | 18/20 [01:09<00:06,  3.47s/it] 90%|█████████ | 18/20 [01:09<00:06,  3.45s/it] 90%|█████████ | 18/20 [01:10<00:07,  3.55s/it] 90%|█████████ | 18/20 [01:11<00:07,  3.57s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.44s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.53s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.51s/it] 95%|█████████▌| 19/20 [01:15<00:03,  3.60s/it]100%|██████████| 20/20 [01:17<00:00,  3.82s/it]100%|██████████| 20/20 [01:17<00:00,  3.87s/it]
100%|██████████| 20/20 [01:17<00:00,  3.72s/it]100%|██████████| 20/20 [01:17<00:00,  3.88s/it]
100%|██████████| 20/20 [01:18<00:00,  3.73s/it]100%|██████████| 20/20 [01:18<00:00,  3.91s/it]
100%|██████████| 20/20 [01:19<00:00,  3.78s/it]100%|██████████| 20/20 [01:19<00:00,  3.96s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 08:41:57,498 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_89/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 08:41:57,498 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 08:41:57,498 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 08:41:57,498 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 08:44:31,711 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 08:44:31,711 - RUN - INFO - Evaluation Results:
2025-02-23 08:44:31,714 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.655
 1  0        single  0.76
 2  0        cross   0.55
 3  1        all     0.605
 4  1        single  0.79
 5  1        cross   0.42
 6  2        all     0.58
 7  2        single  0.76
 8  2        cross   0.4
 9  3        all     0.6
10  3        single  0.78
11  3        cross   0.42
12  Average  all     0.61
13  Average  single  0.7725
14  Average  cross   0.4475
--  -------  ------  ------
[rank0]:[W223 08:44:32.733175932 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 08:44:38.773210 3011849 site-packages/torch/distributed/run.py:793] 
W0223 08:44:38.773210 3011849 site-packages/torch/distributed/run.py:793] *****************************************
W0223 08:44:38.773210 3011849 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 08:44:38.773210 3011849 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank0]:[W223 08:45:04.213039513 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 08:45:04.213798302 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 08:45:04.218352851 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 08:45:04.223090899 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.10s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.25s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.11s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]
Max step: 91, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 91, bias_value: 0.2
Max step: 91, bias_value: 0.2
Max step: 91, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [04:20<17:21, 260.29s/it] 20%|██        | 1/5 [05:25<21:42, 325.65s/it] 20%|██        | 1/5 [05:55<23:41, 355.46s/it] 20%|██        | 1/5 [06:03<24:12, 363.04s/it] 40%|████      | 2/5 [08:35<12:51, 257.17s/it] 40%|████      | 2/5 [09:22<13:20, 266.78s/it] 40%|████      | 2/5 [10:43<15:47, 315.96s/it] 40%|████      | 2/5 [10:57<16:27, 329.18s/it] 60%|██████    | 3/5 [12:49<08:31, 255.76s/it] 60%|██████    | 3/5 [16:08<11:00, 330.16s/it] 60%|██████    | 3/5 [16:24<10:54, 327.13s/it] 60%|██████    | 3/5 [16:50<11:20, 340.31s/it] 80%|████████  | 4/5 [17:23<04:23, 263.20s/it] 80%|████████  | 4/5 [21:38<05:30, 330.38s/it] 80%|████████  | 4/5 [22:38<05:45, 345.57s/it]100%|██████████| 5/5 [23:27<00:00, 299.53s/it]100%|██████████| 5/5 [23:27<00:00, 281.58s/it]
 80%|████████  | 4/5 [23:30<06:03, 363.75s/it]100%|██████████| 5/5 [25:51<00:00, 302.23s/it]100%|██████████| 5/5 [25:51<00:00, 310.21s/it]
100%|██████████| 5/5 [27:34<00:00, 327.69s/it]100%|██████████| 5/5 [27:34<00:00, 330.80s/it]
100%|██████████| 5/5 [28:36<00:00, 342.78s/it]100%|██████████| 5/5 [28:36<00:00, 343.23s/it]
[rank0]:[W223 09:15:14.114312545 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 09:15:21.916155 3091290 site-packages/torch/distributed/run.py:793] 
W0223 09:15:21.916155 3091290 site-packages/torch/distributed/run.py:793] *****************************************
W0223 09:15:21.916155 3091290 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 09:15:21.916155 3091290 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank2]:[W223 09:15:49.928221363 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 09:15:49.937198333 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 09:15:49.940260347 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 09:16:21.744074158 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.43s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.26s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.37s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_91/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2Max step: 50, bias_value: 0.2

Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_91/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_91/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_91/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:04,  6.58s/it]  5%|▌         | 1/20 [00:07<02:13,  7.04s/it]  5%|▌         | 1/20 [00:07<02:13,  7.04s/it]  5%|▌         | 1/20 [00:06<02:02,  6.45s/it] 10%|█         | 2/20 [00:10<01:27,  4.86s/it] 10%|█         | 2/20 [00:10<01:28,  4.90s/it] 10%|█         | 2/20 [00:10<01:28,  4.91s/it] 10%|█         | 2/20 [00:09<01:23,  4.65s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.80s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.79s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.85s/it] 15%|█▌        | 3/20 [00:12<01:02,  3.67s/it] 20%|██        | 4/20 [00:16<00:59,  3.74s/it] 20%|██        | 4/20 [00:16<01:01,  3.85s/it] 20%|██        | 4/20 [00:16<01:01,  3.82s/it] 20%|██        | 4/20 [00:16<01:00,  3.81s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.53s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.56s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.71s/it] 25%|██▌       | 5/20 [00:19<00:55,  3.67s/it] 30%|███       | 6/20 [00:23<00:48,  3.47s/it] 30%|███       | 6/20 [00:23<00:48,  3.47s/it] 30%|███       | 6/20 [00:23<00:48,  3.47s/it] 30%|███       | 6/20 [00:23<00:49,  3.57s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.17s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.21s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.29s/it] 35%|███▌      | 7/20 [00:25<00:42,  3.24s/it] 40%|████      | 8/20 [00:29<00:39,  3.28s/it] 40%|████      | 8/20 [00:29<00:40,  3.34s/it] 40%|████      | 8/20 [00:29<00:41,  3.48s/it] 40%|████      | 8/20 [00:29<00:40,  3.37s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.18s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.20s/it] 45%|████▌     | 9/20 [00:33<00:37,  3.39s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.28s/it] 50%|█████     | 10/20 [00:35<00:32,  3.28s/it] 50%|█████     | 10/20 [00:36<00:36,  3.67s/it] 50%|█████     | 10/20 [00:36<00:34,  3.47s/it] 50%|█████     | 10/20 [00:38<00:38,  3.84s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.53s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.75s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.68s/it] 55%|█████▌    | 11/20 [00:42<00:35,  3.98s/it] 60%|██████    | 12/20 [00:44<00:31,  3.93s/it] 60%|██████    | 12/20 [00:45<00:32,  4.02s/it] 60%|██████    | 12/20 [00:45<00:32,  4.05s/it] 60%|██████    | 12/20 [00:47<00:33,  4.22s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.14s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.12s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.19s/it] 65%|██████▌   | 13/20 [00:51<00:30,  4.40s/it] 70%|███████   | 14/20 [00:53<00:24,  4.04s/it] 70%|███████   | 14/20 [00:53<00:23,  3.94s/it] 70%|███████   | 14/20 [00:54<00:25,  4.21s/it] 70%|███████   | 14/20 [00:56<00:26,  4.42s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.94s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.81s/it] 75%|███████▌  | 15/20 [00:58<00:20,  4.12s/it] 80%|████████  | 16/20 [00:59<00:14,  3.52s/it] 80%|████████  | 16/20 [01:00<00:14,  3.67s/it] 75%|███████▌  | 15/20 [01:00<00:22,  4.42s/it] 80%|████████  | 16/20 [01:01<00:15,  3.92s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.48s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.53s/it] 80%|████████  | 16/20 [01:03<00:16,  4.04s/it] 85%|████████▌ | 17/20 [01:04<00:11,  3.72s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.41s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.42s/it] 85%|████████▌ | 17/20 [01:07<00:11,  3.76s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.60s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.40s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.47s/it] 90%|█████████ | 18/20 [01:10<00:07,  3.61s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.60s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.56s/it]100%|██████████| 20/20 [01:13<00:00,  3.75s/it]100%|██████████| 20/20 [01:13<00:00,  3.70s/it]
100%|██████████| 20/20 [01:14<00:00,  3.71s/it]100%|██████████| 20/20 [01:14<00:00,  3.72s/it]
100%|██████████| 20/20 [01:15<00:00,  3.79s/it]100%|██████████| 20/20 [01:15<00:00,  3.80s/it]
100%|██████████| 20/20 [01:18<00:00,  3.84s/it]100%|██████████| 20/20 [01:18<00:00,  3.91s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 09:19:15,015 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_91/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 09:19:15,015 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 09:19:15,015 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 09:19:15,015 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 09:21:23,810 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 09:21:23,811 - RUN - INFO - Evaluation Results:
2025-02-23 09:21:23,813 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.6
10  3        cross   0.42
11  3        single  0.78
12  Average  all     0.61
13  Average  cross   0.4475
14  Average  single  0.7725
--  -------  ------  ------
[rank0]:[W223 09:21:24.623783296 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 09:21:30.216893 3106087 site-packages/torch/distributed/run.py:793] 
W0223 09:21:30.216893 3106087 site-packages/torch/distributed/run.py:793] *****************************************
W0223 09:21:30.216893 3106087 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 09:21:30.216893 3106087 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank3]:[W223 09:21:59.451715115 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 09:21:59.451714973 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W223 09:21:59.458695706 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 09:21:59.482646758 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.23s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.19s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]
Max step: 93, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 93, bias_value: 0.2
Max step: 93, bias_value: 0.2Max step: 93, bias_value: 0.2

You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [04:20<17:20, 260.25s/it] 20%|██        | 1/5 [05:29<21:56, 329.16s/it] 20%|██        | 1/5 [05:58<23:54, 358.73s/it] 20%|██        | 1/5 [06:03<24:14, 363.56s/it] 40%|████      | 2/5 [08:36<12:53, 257.92s/it] 40%|████      | 2/5 [09:28<13:30, 270.12s/it] 40%|████      | 2/5 [10:51<16:00, 320.13s/it] 40%|████      | 2/5 [11:02<16:34, 331.44s/it] 60%|██████    | 3/5 [12:51<08:33, 256.65s/it] 60%|██████    | 3/5 [16:18<11:08, 334.05s/it] 60%|██████    | 3/5 [16:40<11:06, 333.04s/it] 60%|██████    | 3/5 [17:00<11:27, 343.75s/it] 80%|████████  | 4/5 [17:29<04:24, 264.96s/it] 80%|████████  | 4/5 [21:50<05:33, 333.39s/it] 80%|████████  | 4/5 [22:56<05:49, 349.94s/it]100%|██████████| 5/5 [23:39<00:00, 302.91s/it]100%|██████████| 5/5 [23:39<00:00, 283.92s/it]
 80%|████████  | 4/5 [23:41<06:06, 366.45s/it]100%|██████████| 5/5 [26:07<00:00, 305.80s/it]100%|██████████| 5/5 [26:07<00:00, 313.53s/it]
100%|██████████| 5/5 [27:52<00:00, 330.78s/it]100%|██████████| 5/5 [27:52<00:00, 334.58s/it]
100%|██████████| 5/5 [28:48<00:00, 344.77s/it]100%|██████████| 5/5 [28:48<00:00, 345.64s/it]
[rank0]:[W223 09:52:17.000844291 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 09:52:25.036543 3184843 site-packages/torch/distributed/run.py:793] 
W0223 09:52:25.036543 3184843 site-packages/torch/distributed/run.py:793] *****************************************
W0223 09:52:25.036543 3184843 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 09:52:25.036543 3184843 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank3]:[W223 09:52:54.514973652 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 09:52:54.515090257 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 09:52:54.516043016 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 09:53:25.958739124 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.46s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.45s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.43s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_93/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_93/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_93/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_93/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:03,  6.49s/it]  5%|▌         | 1/20 [00:07<02:13,  7.02s/it]  5%|▌         | 1/20 [00:07<02:19,  7.32s/it]  5%|▌         | 1/20 [00:07<02:21,  7.45s/it] 10%|█         | 2/20 [00:10<01:28,  4.91s/it] 10%|█         | 2/20 [00:10<01:27,  4.84s/it] 10%|█         | 2/20 [00:10<01:28,  4.90s/it] 10%|█         | 2/20 [00:10<01:31,  5.06s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.81s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.77s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.84s/it] 15%|█▌        | 3/20 [00:13<01:06,  3.89s/it] 20%|██        | 4/20 [00:16<01:01,  3.82s/it] 20%|██        | 4/20 [00:16<00:59,  3.72s/it] 20%|██        | 4/20 [00:16<01:02,  3.88s/it] 20%|██        | 4/20 [00:17<01:02,  3.92s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.53s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.68s/it] 25%|██▌       | 5/20 [00:20<00:54,  3.63s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.67s/it] 30%|███       | 6/20 [00:23<00:48,  3.46s/it] 30%|███       | 6/20 [00:23<00:48,  3.49s/it] 30%|███       | 6/20 [00:23<00:48,  3.49s/it] 30%|███       | 6/20 [00:23<00:49,  3.54s/it] 35%|███▌      | 7/20 [00:25<00:42,  3.25s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.23s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.22s/it] 35%|███▌      | 7/20 [00:26<00:41,  3.22s/it] 40%|████      | 8/20 [00:29<00:39,  3.28s/it] 40%|████      | 8/20 [00:29<00:40,  3.38s/it] 40%|████      | 8/20 [00:29<00:40,  3.39s/it] 40%|████      | 8/20 [00:30<00:40,  3.37s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.20s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.28s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.28s/it] 45%|████▌     | 9/20 [00:33<00:35,  3.27s/it] 50%|█████     | 10/20 [00:36<00:33,  3.35s/it] 50%|█████     | 10/20 [00:36<00:33,  3.40s/it] 50%|█████     | 10/20 [00:37<00:36,  3.67s/it] 50%|█████     | 10/20 [00:37<00:37,  3.76s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.62s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.62s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.82s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.88s/it] 60%|██████    | 12/20 [00:45<00:32,  4.04s/it] 60%|██████    | 12/20 [00:45<00:32,  4.05s/it] 60%|██████    | 12/20 [00:46<00:33,  4.14s/it] 60%|██████    | 12/20 [00:46<00:33,  4.15s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.20s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.15s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.25s/it] 65%|██████▌   | 13/20 [00:51<00:30,  4.34s/it] 70%|███████   | 14/20 [00:57<00:30,  5.15s/it] 70%|███████   | 14/20 [00:57<00:30,  5.09s/it] 70%|███████   | 14/20 [00:57<00:30,  5.08s/it] 70%|███████   | 14/20 [00:58<00:31,  5.30s/it] 75%|███████▌  | 15/20 [01:00<00:22,  4.51s/it] 75%|███████▌  | 15/20 [01:01<00:23,  4.61s/it] 75%|███████▌  | 15/20 [01:01<00:23,  4.73s/it] 75%|███████▌  | 15/20 [01:02<00:24,  4.93s/it] 80%|████████  | 16/20 [01:03<00:16,  4.08s/it] 80%|████████  | 16/20 [01:04<00:16,  4.17s/it] 80%|████████  | 16/20 [01:04<00:16,  4.19s/it] 80%|████████  | 16/20 [01:05<00:17,  4.39s/it] 85%|████████▌ | 17/20 [01:06<00:11,  3.83s/it] 85%|████████▌ | 17/20 [01:07<00:11,  3.87s/it] 85%|████████▌ | 17/20 [01:07<00:11,  3.93s/it] 85%|████████▌ | 17/20 [01:08<00:11,  3.99s/it] 90%|█████████ | 18/20 [01:10<00:07,  3.65s/it] 90%|█████████ | 18/20 [01:10<00:07,  3.68s/it] 90%|█████████ | 18/20 [01:10<00:07,  3.74s/it] 90%|█████████ | 18/20 [01:12<00:07,  3.79s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.58s/it] 95%|█████████▌| 19/20 [01:14<00:03,  3.64s/it] 95%|█████████▌| 19/20 [01:14<00:03,  3.70s/it] 95%|█████████▌| 19/20 [01:15<00:03,  3.68s/it]100%|██████████| 20/20 [01:18<00:00,  3.78s/it]100%|██████████| 20/20 [01:18<00:00,  3.91s/it]
100%|██████████| 20/20 [01:18<00:00,  3.90s/it]100%|██████████| 20/20 [01:18<00:00,  3.91s/it]
100%|██████████| 20/20 [01:18<00:00,  3.84s/it]100%|██████████| 20/20 [01:18<00:00,  3.94s/it]
100%|██████████| 20/20 [01:20<00:00,  3.87s/it]100%|██████████| 20/20 [01:20<00:00,  4.00s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 09:56:24,034 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_93/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 09:56:24,035 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 09:56:24,035 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 09:56:24,035 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 09:58:48,968 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 09:58:48,968 - RUN - INFO - Evaluation Results:
2025-02-23 09:58:48,971 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.6
10  3        cross   0.42
11  3        single  0.78
12  Average  all     0.61
13  Average  cross   0.4475
14  Average  single  0.7725
--  -------  ------  ------
[rank0]:[W223 09:58:49.829959272 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 09:58:55.599310 3200659 site-packages/torch/distributed/run.py:793] 
W0223 09:58:55.599310 3200659 site-packages/torch/distributed/run.py:793] *****************************************
W0223 09:58:55.599310 3200659 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 09:58:55.599310 3200659 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank3]:[W223 09:59:22.972716411 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W223 09:59:22.980039742 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 09:59:22.983878847 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 09:59:22.993281596 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.64s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.63s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.65s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]
Max step: 95, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 95, bias_value: 0.2
Max step: 95, bias_value: 0.2
Max step: 95, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [04:23<17:33, 263.35s/it] 20%|██        | 1/5 [05:32<22:11, 332.76s/it] 20%|██        | 1/5 [06:00<24:03, 360.84s/it] 20%|██        | 1/5 [06:06<24:27, 366.87s/it] 40%|████      | 2/5 [08:39<12:57, 259.10s/it] 40%|████      | 2/5 [09:25<13:23, 267.99s/it] 40%|████      | 2/5 [10:54<16:03, 321.09s/it] 40%|████      | 2/5 [11:11<16:48, 336.02s/it] 60%|██████    | 3/5 [12:54<08:34, 257.32s/it] 60%|██████    | 3/5 [16:19<11:09, 334.69s/it] 60%|██████    | 3/5 [16:43<11:07, 333.95s/it] 60%|██████    | 3/5 [17:17<11:40, 350.02s/it] 80%|████████  | 4/5 [17:32<04:25, 265.56s/it] 80%|████████  | 4/5 [21:55<05:35, 335.13s/it] 80%|████████  | 4/5 [23:02<05:51, 351.62s/it]100%|██████████| 5/5 [23:46<00:00, 304.60s/it]100%|██████████| 5/5 [23:46<00:00, 285.34s/it]
 80%|████████  | 4/5 [24:05<06:12, 372.67s/it]100%|██████████| 5/5 [26:09<00:00, 305.93s/it]100%|██████████| 5/5 [26:09<00:00, 313.93s/it]
100%|██████████| 5/5 [28:02<00:00, 333.24s/it]100%|██████████| 5/5 [28:02<00:00, 336.54s/it]
100%|██████████| 5/5 [29:19<00:00, 351.49s/it]100%|██████████| 5/5 [29:19<00:00, 351.81s/it]
[rank0]:[W223 10:30:14.630807303 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 10:30:21.725279 3280787 site-packages/torch/distributed/run.py:793] 
W0223 10:30:21.725279 3280787 site-packages/torch/distributed/run.py:793] *****************************************
W0223 10:30:21.725279 3280787 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 10:30:21.725279 3280787 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W223 10:30:50.412863205 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 10:30:50.416907520 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 10:30:50.417759570 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 10:31:21.623612901 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.27s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.29s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.30s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_95/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_95/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_95/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_95/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<01:59,  6.29s/it]  5%|▌         | 1/20 [00:07<02:17,  7.22s/it]  5%|▌         | 1/20 [00:07<02:20,  7.40s/it]  5%|▌         | 1/20 [00:07<02:25,  7.63s/it] 10%|█         | 2/20 [00:09<01:25,  4.77s/it] 10%|█         | 2/20 [00:10<01:29,  5.00s/it] 10%|█         | 2/20 [00:10<01:30,  5.02s/it] 10%|█         | 2/20 [00:11<01:32,  5.14s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.77s/it] 15%|█▌        | 3/20 [00:13<01:06,  3.89s/it] 15%|█▌        | 3/20 [00:13<01:07,  3.98s/it] 15%|█▌        | 3/20 [00:13<01:07,  3.96s/it] 20%|██        | 4/20 [00:16<01:01,  3.84s/it] 20%|██        | 4/20 [00:17<01:01,  3.84s/it] 20%|██        | 4/20 [00:17<01:02,  3.89s/it] 20%|██        | 4/20 [00:17<01:04,  4.03s/it] 25%|██▌       | 5/20 [00:20<00:53,  3.59s/it] 25%|██▌       | 5/20 [00:20<00:54,  3.64s/it] 25%|██▌       | 5/20 [00:19<00:55,  3.70s/it] 25%|██▌       | 5/20 [00:21<00:57,  3.85s/it] 30%|███       | 6/20 [00:23<00:49,  3.53s/it] 30%|███       | 6/20 [00:23<00:49,  3.53s/it] 30%|███       | 6/20 [00:23<00:49,  3.52s/it] 30%|███       | 6/20 [00:24<00:51,  3.70s/it] 35%|███▌      | 7/20 [00:26<00:41,  3.23s/it] 35%|███▌      | 7/20 [00:26<00:42,  3.24s/it] 35%|███▌      | 7/20 [00:26<00:43,  3.31s/it] 35%|███▌      | 7/20 [00:27<00:43,  3.37s/it] 40%|████      | 8/20 [00:29<00:39,  3.30s/it] 40%|████      | 8/20 [00:30<00:40,  3.38s/it] 40%|████      | 8/20 [00:29<00:41,  3.49s/it] 40%|████      | 8/20 [00:30<00:41,  3.50s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.21s/it] 45%|████▌     | 9/20 [00:33<00:35,  3.26s/it] 45%|████▌     | 9/20 [00:33<00:37,  3.37s/it] 45%|████▌     | 9/20 [00:34<00:36,  3.36s/it] 50%|█████     | 10/20 [00:36<00:33,  3.37s/it] 50%|█████     | 10/20 [00:37<00:36,  3.69s/it] 50%|█████     | 10/20 [00:37<00:35,  3.52s/it] 50%|█████     | 10/20 [00:38<00:38,  3.89s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.63s/it] 55%|█████▌    | 11/20 [00:41<00:34,  3.82s/it] 55%|█████▌    | 11/20 [00:42<00:33,  3.75s/it] 55%|█████▌    | 11/20 [00:42<00:36,  4.03s/it] 60%|██████    | 12/20 [00:45<00:31,  3.99s/it] 60%|██████    | 12/20 [00:46<00:32,  4.07s/it] 60%|██████    | 12/20 [00:47<00:33,  4.15s/it] 60%|██████    | 12/20 [00:47<00:34,  4.32s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.17s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.17s/it] 65%|██████▌   | 13/20 [00:51<00:29,  4.25s/it] 65%|██████▌   | 13/20 [00:52<00:31,  4.50s/it] 70%|███████   | 14/20 [00:54<00:24,  4.12s/it] 70%|███████   | 14/20 [00:54<00:24,  4.03s/it] 70%|███████   | 14/20 [00:55<00:24,  4.16s/it] 70%|███████   | 14/20 [00:56<00:27,  4.50s/it] 75%|███████▌  | 15/20 [00:57<00:18,  3.78s/it] 75%|███████▌  | 15/20 [00:58<00:20,  4.02s/it] 75%|███████▌  | 15/20 [00:59<00:20,  4.09s/it] 80%|████████  | 16/20 [01:00<00:14,  3.62s/it] 80%|████████  | 16/20 [01:01<00:14,  3.69s/it] 75%|███████▌  | 15/20 [01:01<00:22,  4.52s/it] 80%|████████  | 16/20 [01:03<00:15,  3.90s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.54s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.56s/it] 80%|████████  | 16/20 [01:04<00:16,  4.16s/it] 85%|████████▌ | 17/20 [01:06<00:11,  3.72s/it] 90%|█████████ | 18/20 [01:07<00:07,  3.52s/it] 90%|█████████ | 18/20 [01:07<00:06,  3.50s/it] 85%|████████▌ | 17/20 [01:07<00:11,  3.83s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.55s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.47s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.60s/it] 90%|█████████ | 18/20 [01:11<00:07,  3.71s/it] 95%|█████████▌| 19/20 [01:13<00:03,  3.57s/it] 95%|█████████▌| 19/20 [01:14<00:03,  3.63s/it]100%|██████████| 20/20 [01:15<00:00,  3.85s/it]100%|██████████| 20/20 [01:15<00:00,  3.78s/it]
100%|██████████| 20/20 [01:15<00:00,  3.75s/it]100%|██████████| 20/20 [01:15<00:00,  3.79s/it]
100%|██████████| 20/20 [01:17<00:00,  3.77s/it]100%|██████████| 20/20 [01:17<00:00,  3.87s/it]
100%|██████████| 20/20 [01:19<00:00,  3.87s/it]100%|██████████| 20/20 [01:19<00:00,  3.95s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 10:34:18,356 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_95/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 10:34:18,356 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 10:34:18,356 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 10:34:18,356 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 10:36:36,917 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 10:36:36,917 - RUN - INFO - Evaluation Results:
2025-02-23 10:36:36,919 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.655
 1  0        single  0.76
 2  0        cross   0.55
 3  1        all     0.605
 4  1        single  0.79
 5  1        cross   0.42
 6  2        all     0.58
 7  2        single  0.76
 8  2        cross   0.4
 9  3        all     0.6
10  3        single  0.78
11  3        cross   0.42
12  Average  all     0.61
13  Average  single  0.7725
14  Average  cross   0.4475
--  -------  ------  ------
[rank0]:[W223 10:36:37.740875882 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 10:36:43.715011 3299616 site-packages/torch/distributed/run.py:793] 
W0223 10:36:43.715011 3299616 site-packages/torch/distributed/run.py:793] *****************************************
W0223 10:36:43.715011 3299616 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 10:36:43.715011 3299616 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank2]:[W223 10:37:13.816822986 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 10:37:13.816881997 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W223 10:37:13.822709053 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 10:37:13.824130414 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.12s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.18s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.10s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.93it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.39it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.30it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]
Max step: 97, bias_value: 0.2
Max step: 97, bias_value: 0.2
Max step: 97, bias_value: 0.2
Max step: 97, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [04:24<17:38, 264.69s/it] 20%|██        | 1/5 [05:35<22:21, 335.38s/it] 20%|██        | 1/5 [06:03<24:14, 363.63s/it] 20%|██        | 1/5 [06:10<24:43, 370.82s/it] 40%|████      | 2/5 [08:42<13:01, 260.49s/it] 40%|████      | 2/5 [09:30<13:29, 269.86s/it] 40%|████      | 2/5 [10:58<16:08, 322.94s/it] 40%|████      | 2/5 [11:16<16:55, 338.53s/it] 60%|██████    | 3/5 [13:00<08:39, 259.56s/it] 60%|██████    | 3/5 [16:26<11:13, 336.96s/it] 60%|██████    | 3/5 [16:51<11:13, 336.96s/it] 60%|██████    | 3/5 [17:29<11:49, 354.58s/it] 80%|████████  | 4/5 [17:41<04:27, 267.82s/it] 80%|████████  | 4/5 [22:03<05:36, 336.98s/it] 80%|████████  | 4/5 [23:14<05:54, 354.86s/it]100%|██████████| 5/5 [23:59<00:00, 307.50s/it]100%|██████████| 5/5 [23:59<00:00, 287.81s/it]
 80%|████████  | 4/5 [24:18<06:15, 375.89s/it]100%|██████████| 5/5 [26:22<00:00, 308.59s/it]100%|██████████| 5/5 [26:22<00:00, 316.41s/it]
100%|██████████| 5/5 [28:16<00:00, 335.79s/it]100%|██████████| 5/5 [28:16<00:00, 339.20s/it]
100%|██████████| 5/5 [29:28<00:00, 352.28s/it]100%|██████████| 5/5 [29:28<00:00, 353.76s/it]
[rank0]:[W223 11:08:18.714375894 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 11:08:25.528343 3365049 site-packages/torch/distributed/run.py:793] 
W0223 11:08:25.528343 3365049 site-packages/torch/distributed/run.py:793] *****************************************
W0223 11:08:25.528343 3365049 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 11:08:25.528343 3365049 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank2]:[W223 11:08:54.836565201 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 11:08:54.837258517 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 11:08:54.837548085 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 11:09:24.070317063 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.29s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.43s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.30s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_97/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_97/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_97/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_97/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<01:58,  6.24s/it]  5%|▌         | 1/20 [00:07<02:14,  7.07s/it]  5%|▌         | 1/20 [00:07<02:13,  7.01s/it]  5%|▌         | 1/20 [00:07<02:15,  7.11s/it] 10%|█         | 2/20 [00:09<01:24,  4.68s/it] 10%|█         | 2/20 [00:10<01:26,  4.80s/it] 10%|█         | 2/20 [00:10<01:27,  4.87s/it] 10%|█         | 2/20 [00:10<01:29,  4.98s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.74s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.80s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.75s/it] 15%|█▌        | 3/20 [00:13<01:05,  3.86s/it] 20%|██        | 4/20 [00:16<00:59,  3.70s/it] 20%|██        | 4/20 [00:16<01:00,  3.79s/it] 20%|██        | 4/20 [00:16<01:01,  3.86s/it] 20%|██        | 4/20 [00:17<01:03,  3.96s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.50s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.61s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.64s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.72s/it] 30%|███       | 6/20 [00:22<00:47,  3.41s/it] 30%|███       | 6/20 [00:22<00:48,  3.45s/it] 30%|███       | 6/20 [00:23<00:49,  3.51s/it] 30%|███       | 6/20 [00:23<00:49,  3.56s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.14s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.22s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.19s/it] 35%|███▌      | 7/20 [00:26<00:41,  3.23s/it] 40%|████      | 8/20 [00:28<00:38,  3.21s/it] 40%|████      | 8/20 [00:29<00:40,  3.35s/it] 40%|████      | 8/20 [00:29<00:41,  3.42s/it] 40%|████      | 8/20 [00:30<00:40,  3.38s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.14s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.22s/it] 45%|████▌     | 9/20 [00:32<00:36,  3.33s/it] 45%|████▌     | 9/20 [00:33<00:36,  3.28s/it] 50%|█████     | 10/20 [00:36<00:33,  3.34s/it] 50%|█████     | 10/20 [00:36<00:36,  3.63s/it] 50%|█████     | 10/20 [00:36<00:33,  3.39s/it] 50%|█████     | 10/20 [00:37<00:37,  3.76s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.56s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.74s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.63s/it] 55%|█████▌    | 11/20 [00:41<00:35,  3.91s/it] 60%|██████    | 12/20 [00:45<00:31,  4.00s/it] 60%|██████    | 12/20 [00:44<00:31,  3.95s/it] 60%|██████    | 12/20 [00:45<00:32,  4.05s/it] 60%|██████    | 12/20 [00:46<00:33,  4.20s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.09s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.15s/it] 65%|██████▌   | 13/20 [00:50<00:29,  4.15s/it] 65%|██████▌   | 13/20 [00:51<00:30,  4.39s/it] 70%|███████   | 14/20 [00:52<00:23,  3.93s/it] 70%|███████   | 14/20 [00:53<00:24,  4.04s/it] 70%|███████   | 14/20 [00:54<00:24,  4.09s/it] 70%|███████   | 14/20 [00:55<00:26,  4.37s/it] 75%|███████▌  | 15/20 [00:56<00:18,  3.70s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.98s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.91s/it] 80%|████████  | 16/20 [00:59<00:14,  3.56s/it] 80%|████████  | 16/20 [00:59<00:14,  3.56s/it] 75%|███████▌  | 15/20 [00:59<00:21,  4.29s/it] 80%|████████  | 16/20 [01:01<00:15,  3.78s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.44s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.46s/it] 80%|████████  | 16/20 [01:02<00:15,  3.96s/it] 85%|████████▌ | 17/20 [01:04<00:10,  3.61s/it] 90%|█████████ | 18/20 [01:05<00:06,  3.32s/it] 85%|████████▌ | 17/20 [01:05<00:11,  3.69s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.43s/it] 90%|█████████ | 18/20 [01:07<00:06,  3.45s/it] 95%|█████████▌| 19/20 [01:08<00:03,  3.37s/it] 90%|█████████ | 18/20 [01:09<00:07,  3.57s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.45s/it] 95%|█████████▌| 19/20 [01:11<00:03,  3.51s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.53s/it]100%|██████████| 20/20 [01:13<00:00,  3.59s/it]100%|██████████| 20/20 [01:13<00:00,  3.66s/it]
100%|██████████| 20/20 [01:14<00:00,  3.78s/it]100%|██████████| 20/20 [01:14<00:00,  3.72s/it]
100%|██████████| 20/20 [01:15<00:00,  3.79s/it]100%|██████████| 20/20 [01:15<00:00,  3.78s/it]
100%|██████████| 20/20 [01:17<00:00,  3.81s/it]100%|██████████| 20/20 [01:17<00:00,  3.85s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 11:12:17,907 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_97/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 11:12:17,907 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 11:12:17,907 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 11:12:17,907 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 11:14:46,459 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 11:14:46,459 - RUN - INFO - Evaluation Results:
2025-02-23 11:14:46,462 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.6
10  3        cross   0.42
11  3        single  0.78
12  Average  all     0.61
13  Average  cross   0.4475
14  Average  single  0.7725
--  -------  ------  ------
[rank0]:[W223 11:14:47.324114075 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 11:14:53.243644 3370506 site-packages/torch/distributed/run.py:793] 
W0223 11:14:53.243644 3370506 site-packages/torch/distributed/run.py:793] *****************************************
W0223 11:14:53.243644 3370506 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 11:14:53.243644 3370506 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k_single.tsv
data md5: 8d47e46b373bb4983cc84448749fe532
file md5: 8d47e46b373bb4983cc84448749fe532
[rank3]:[W223 11:15:23.670774210 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 11:15:23.683528967 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W223 11:15:23.702037734 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W223 11:15:23.709709433 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.52s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.58s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.58s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]
Max step: 99, bias_value: 0.2
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 99, bias_value: 0.2
Max step: 99, bias_value: 0.2
Max step: 99, bias_value: 0.2
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [04:24<17:38, 264.72s/it] 20%|██        | 1/5 [05:38<22:34, 338.64s/it] 20%|██        | 1/5 [06:07<24:28, 367.24s/it] 20%|██        | 1/5 [06:07<24:29, 367.49s/it] 40%|████      | 2/5 [08:42<13:01, 260.52s/it] 40%|████      | 2/5 [09:25<13:22, 267.56s/it] 40%|████      | 2/5 [11:03<16:15, 325.18s/it] 40%|████      | 2/5 [11:24<17:08, 342.70s/it] 60%|██████    | 3/5 [13:02<08:41, 260.51s/it] 60%|██████    | 3/5 [16:26<11:15, 337.70s/it] 60%|██████    | 3/5 [17:03<11:22, 341.18s/it] 60%|██████    | 3/5 [17:40<11:56, 358.08s/it] 80%|████████  | 4/5 [17:53<04:32, 272.51s/it] 80%|████████  | 4/5 [22:03<05:37, 337.58s/it] 80%|████████  | 4/5 [23:28<05:58, 358.73s/it]100%|██████████| 5/5 [24:16<00:00, 312.11s/it]100%|██████████| 5/5 [24:16<00:00, 291.21s/it]
 80%|████████  | 4/5 [24:33<06:19, 379.83s/it]100%|██████████| 5/5 [26:21<00:00, 308.83s/it]100%|██████████| 5/5 [26:21<00:00, 316.29s/it]
100%|██████████| 5/5 [28:33<00:00, 339.29s/it]100%|██████████| 5/5 [28:33<00:00, 342.75s/it]
100%|██████████| 5/5 [29:45<00:00, 355.37s/it]100%|██████████| 5/5 [29:45<00:00, 357.15s/it]
[rank0]:[W223 11:46:44.673563031 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0223 11:46:51.562857 3404548 site-packages/torch/distributed/run.py:793] 
W0223 11:46:51.562857 3404548 site-packages/torch/distributed/run.py:793] *****************************************
W0223 11:46:51.562857 3404548 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0223 11:46:51.562857 3404548 site-packages/torch/distributed/run.py:793] *****************************************
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
data path: /mnt/data/users/wenbinwang/datasets/LVLM_Benchmark/LMUData/hr_bench_8k.tsv
[rank1]:[W223 11:47:19.048865368 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W223 11:47:19.053158306 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W223 11:47:19.055334447 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
data md5: 274c9c7f89329b804a4723178a00219c
file md5: 274c9c7f89329b804a4723178a00219c
[rank0]:[W223 11:47:51.498611635 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.52s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.52s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.53s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_99/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
Rank 0:  Loaded LLaVA model: /mnt/data/users/wenbinwang/huggingface/llava-onevision-qwen2-0.5b-ov
Max step: 50, bias_value: 0.2
Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_99/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.jsonload mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_99/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json

Max step: 50, bias_value: 0.2
load mapping: /mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_99/llava_onevision_qwen2_0.5b_ov/HRBench8K/images/mapping.json
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Rank 0:  Loading vision tower: /mnt/data/users/wenbinwang/huggingface/siglip-so400m-patch14-384
Rank 0:  Model Class: LlavaQwenForCausalLM
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  5%|▌         | 1/20 [00:06<02:00,  6.32s/it]  5%|▌         | 1/20 [00:07<02:13,  7.04s/it]  5%|▌         | 1/20 [00:07<02:14,  7.07s/it]  5%|▌         | 1/20 [00:07<02:15,  7.14s/it] 10%|█         | 2/20 [00:09<01:24,  4.71s/it] 10%|█         | 2/20 [00:10<01:26,  4.80s/it] 10%|█         | 2/20 [00:10<01:26,  4.79s/it] 10%|█         | 2/20 [00:10<01:29,  4.94s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.73s/it] 15%|█▌        | 3/20 [00:12<01:03,  3.72s/it] 15%|█▌        | 3/20 [00:12<01:04,  3.77s/it] 15%|█▌        | 3/20 [00:13<01:04,  3.81s/it] 20%|██        | 4/20 [00:16<01:00,  3.78s/it] 20%|██        | 4/20 [00:16<00:59,  3.69s/it] 20%|██        | 4/20 [00:16<01:00,  3.79s/it] 20%|██        | 4/20 [00:16<01:02,  3.88s/it] 25%|██▌       | 5/20 [00:19<00:52,  3.47s/it] 25%|██▌       | 5/20 [00:19<00:53,  3.55s/it] 25%|██▌       | 5/20 [00:19<00:54,  3.65s/it] 25%|██▌       | 5/20 [00:20<00:55,  3.71s/it] 30%|███       | 6/20 [00:22<00:47,  3.40s/it] 30%|███       | 6/20 [00:22<00:48,  3.45s/it] 30%|███       | 6/20 [00:22<00:48,  3.44s/it] 30%|███       | 6/20 [00:23<00:49,  3.54s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.14s/it] 35%|███▌      | 7/20 [00:25<00:40,  3.14s/it] 35%|███▌      | 7/20 [00:25<00:41,  3.21s/it] 35%|███▌      | 7/20 [00:26<00:41,  3.20s/it] 40%|████      | 8/20 [00:28<00:39,  3.25s/it] 40%|████      | 8/20 [00:29<00:39,  3.33s/it] 40%|████      | 8/20 [00:29<00:40,  3.37s/it] 40%|████      | 8/20 [00:29<00:40,  3.37s/it] 45%|████▌     | 9/20 [00:31<00:34,  3.17s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.21s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.26s/it] 45%|████▌     | 9/20 [00:32<00:35,  3.24s/it] 50%|█████     | 10/20 [00:35<00:32,  3.26s/it] 50%|█████     | 10/20 [00:36<00:33,  3.35s/it] 50%|█████     | 10/20 [00:36<00:36,  3.67s/it] 50%|█████     | 10/20 [00:36<00:36,  3.67s/it] 55%|█████▌    | 11/20 [00:39<00:31,  3.53s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.57s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.76s/it] 55%|█████▌    | 11/20 [00:40<00:34,  3.79s/it] 60%|██████    | 12/20 [00:44<00:31,  3.93s/it] 60%|██████    | 12/20 [00:45<00:31,  3.93s/it] 60%|██████    | 12/20 [00:45<00:32,  4.05s/it] 60%|██████    | 12/20 [00:45<00:32,  4.11s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.11s/it] 65%|██████▌   | 13/20 [00:49<00:28,  4.06s/it] 65%|██████▌   | 13/20 [00:49<00:29,  4.17s/it] 65%|██████▌   | 13/20 [00:50<00:30,  4.32s/it] 70%|███████   | 14/20 [00:52<00:24,  4.03s/it] 70%|███████   | 14/20 [00:53<00:23,  3.98s/it] 70%|███████   | 14/20 [00:53<00:24,  4.05s/it] 70%|███████   | 14/20 [00:54<00:26,  4.35s/it] 75%|███████▌  | 15/20 [00:56<00:18,  3.73s/it] 75%|███████▌  | 15/20 [00:56<00:19,  3.92s/it] 75%|███████▌  | 15/20 [00:57<00:19,  3.88s/it] 80%|████████  | 16/20 [00:59<00:14,  3.51s/it] 75%|███████▌  | 15/20 [00:59<00:21,  4.34s/it] 80%|████████  | 16/20 [00:59<00:14,  3.63s/it] 80%|████████  | 16/20 [01:00<00:14,  3.71s/it] 85%|████████▌ | 17/20 [01:02<00:10,  3.43s/it] 80%|████████  | 16/20 [01:02<00:16,  4.00s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.50s/it] 85%|████████▌ | 17/20 [01:03<00:10,  3.56s/it] 85%|████████▌ | 17/20 [01:05<00:11,  3.72s/it] 90%|█████████ | 18/20 [01:05<00:06,  3.38s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.38s/it] 90%|█████████ | 18/20 [01:06<00:06,  3.44s/it] 90%|█████████ | 18/20 [01:08<00:07,  3.58s/it] 95%|█████████▌| 19/20 [01:08<00:03,  3.36s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.41s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.49s/it] 95%|█████████▌| 19/20 [01:12<00:03,  3.53s/it]100%|██████████| 20/20 [01:13<00:00,  3.75s/it]100%|██████████| 20/20 [01:13<00:00,  3.68s/it]
100%|██████████| 20/20 [01:13<00:00,  3.64s/it]100%|██████████| 20/20 [01:13<00:00,  3.69s/it]
100%|██████████| 20/20 [01:14<00:00,  3.80s/it]100%|██████████| 20/20 [01:14<00:00,  3.75s/it]
100%|██████████| 20/20 [01:16<00:00,  3.74s/it]100%|██████████| 20/20 [01:16<00:00,  3.82s/it]
Did not detect the .env file at /mnt/code/users/wangwenbin/LVLM/Evaluation/official/VLMEvalKit/.env, failed to load. 
2025-02-23 11:50:45,135 - ChatAPI - INFO - BaseAPI received the following kwargs: {'processed_image_path_folder': '/mnt/data/users/wenbinwang/LVLM_results/llava_ov_ob5b_vrag_hrbench8k_benchmark_astar_rap_99/llava_onevision_qwen2_0.5b_ov/HRBench8K/images'}
2025-02-23 11:50:45,135 - ChatAPI - INFO - Will try to use them as kwargs for `generate`. 
2025-02-23 11:50:45,135 - ChatAPI - INFO - Environment variable OPENAI_API_BASE is set. Will use it as api_base. 
2025-02-23 11:50:45,135 - ChatAPI - INFO - Using API Base: http://0.0.0.0:23335/v1/chat/completions; API Key: sk-testmyllm
Processing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00 800/800 100%
2025-02-23 11:52:43,857 - RUN - INFO - The evaluation of model llava_onevision_qwen2_0.5b_ov x dataset HRBench8K has finished! 
2025-02-23 11:52:43,857 - RUN - INFO - Evaluation Results:
2025-02-23 11:52:43,859 - RUN - INFO - 
--  -------  ------  ------
 0  0        all     0.655
 1  0        cross   0.55
 2  0        single  0.76
 3  1        all     0.605
 4  1        cross   0.42
 5  1        single  0.79
 6  2        all     0.58
 7  2        cross   0.4
 8  2        single  0.76
 9  3        all     0.6
10  3        cross   0.42
11  3        single  0.78
12  Average  all     0.61
13  Average  cross   0.4475
14  Average  single  0.7725
--  -------  ------  ------
[rank0]:[W223 11:52:44.678469118 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
